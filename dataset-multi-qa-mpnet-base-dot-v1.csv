,Unnamed: 0.1,Unnamed: 0,file path,scan results,old_index,license_relevant_lines,file_comments,response,label,predicted_licenses,license_found,coverage
0,0,10,linux-master/tools/testing/selftests/powerpc/benchmarks/gettimeofday.c,GPL-2.0-only,1316,"Copyright 2015, Anton Blanchard, IBM Corp.
SPDX-License-Identifier: GPL-2.0-only","SPDX-License-Identifier: GPL-2.0-only
Copyright 2015, Anton Blanchard, IBM Corp.",['SPDX-License-Identifier: GPL-2.0-only'],GPL-2.0-only,"IBM PowerPC Initialization and Boot Software
GL2PS License",1.0,100.0
1,1,11,linux-master/tools/testing/selftests/kvm/lib/x86_64/vmx.c,GPL-2.0-only,1946,"For now mark these as accessed and dirty because the only* testcase we have needs that.  Can be reconsidered later.
Initialize the control fields to the most basic settings possible.
Setup shadow VMCS, do not load it yet.
SPDX-License-Identifier: GPL-2.0-only
tools/testing/selftests/kvm/lib/x86_64/vmx.c** Copyright (C) 2018, Google LLC.
Enter VMX root operation.
Load a VMCS.
Initialize the host state fields based on the current host state, with* the exception of HOST_RSP and HOST_RIP, which should be set by vmlaunch* or vmresume.
Initialize the guest state fields essentially as a clone of* the host state fields. Some host state fields have fixed* values, and we set the corresponding guest state fields accordingly.
Configure IA32_FEATURE_CONTROL MSR to allow VMXON:*  Bit 0: Lock bit. If clear, VMXON causes a #GP.*  Bit 2: Enables VMXON outside of SMX operation. If clear, VMXON*    outside of SMX causes a #GP.","SPDX-License-Identifier: GPL-2.0-only
tools/testing/selftests/kvm/lib/x86_64/vmx.c** Copyright (C) 2018, Google LLC.
KVM should return supported EVMCS version range
Allocate memory regions for nested VMX tests.** Input Args:*   vm - The VM to allocate guest-virtual addresses in.** Output Args:*   p_vmx_gva - The guest virtual address for the struct vmx_pages.** Return:*   Pointer to structure with the addresses of the VMX areas.
Setup of a region of guest memory for the vmxon region.
Setup of a region of guest memory for a vmcs.
Setup of a region of guest memory for the MSR bitmap.
Setup of a region of guest memory for the shadow VMCS.
Setup of a region of guest memory for the VMREAD and VMWRITE bitmaps.
Ensure bits in CR0 and CR4 are valid in VMX operation:* - Bit X is 1 in _FIXED0: bit X is fixed to 1 in CRx.* - Bit X is 0 in _FIXED1: bit X is fixed to 0 in CRx.
Enable VMX operation
Configure IA32_FEATURE_CONTROL MSR to allow VMXON:*  Bit 0: Lock bit. If clear, VMXON causes a #GP.*  Bit 2: Enables VMXON outside of SMX operation. If clear, VMXON*    outside of SMX causes a #GP.
Enter VMX root operation.
Load a VMCS.
Setup shadow VMCS, do not load it yet.
Initialize the control fields to the most basic settings possible.
+ 1
Never match
64-bit host
64-bit guest
Initialize the host state fields based on the current host state, with* the exception of HOST_RSP and HOST_RIP, which should be set by vmlaunch* or vmresume.
Initialize the guest state fields essentially as a clone of* the host state fields. Some host state fields have fixed* values, and we set the corresponding guest state fields accordingly.
Entry already present.  Assert that the caller doesn't want* a hugepage at this level, and that there isn't a hugepage at* this level.
For now mark these as accessed and dirty because the only* testcase we have needs that.  Can be reconsidered later.
Map a range of EPT guest physical addresses to the VM's physical address** Input Args:*   vm - Virtual Machine*   nested_paddr - Nested guest physical address to map*   paddr - VM Physical Address*   size - The size of the range to map*   level - The level at which to map the range** Output Args: None** Return: None** Within the VM given by vm, creates a nested guest translation for the* page range starting at nested_paddr to the page range starting at paddr.
Prepare an identity extended page table that maps all the* physical pages in VM.
Identity map a region with 1GiB Pages.",['SPDX-License-Identifier: GPL-2.0-only'],GPL-2.0-only,"Gutmann License
mpi Permissive License
mpi Permissive License
GL2PS License
Michigan/Merit Networks License
mpi Permissive License
eCos license version 2.0
mpi Permissive License
mpi Permissive License
XFree86 License 1.1",1.0,100.0
2,2,12,linux-master/tools/testing/selftests/tc-testing/tdc.sh,GPL-2.0-only,468, !/bin/sh SPDX-License-Identifier: GPL-2.0, !/bin/sh SPDX-License-Identifier: GPL-2.0,['SPDX-License-Identifier: GPL-2.0'],GPL-2.0-only,The Parity Public License 7.0.0,1.0,100.0
3,3,13,linux-master/tools/testing/selftests/pid_namespace/Makefile,GPL-2.0-only,1329,"# SPDX-License-Identifier: GPL-2.0
LOCAL_HDRS += $(selfdir)/pidfd/pidfd.h




include ../lib.mk
TEST_GEN_PROGS = regression_enomem
CFLAGS += -g $(KHDR_INCLUDES)","# SPDX-License-Identifier: GPL-2.0
CFLAGS += -g $(KHDR_INCLUDES)

TEST_GEN_PROGS = regression_enomem

LOCAL_HDRS += $(selfdir)/pidfd/pidfd.h

include ../lib.mk
",['SPDX-License-Identifier: GPL-2.0'],GPL-2.0-only,"GL2PS License
hdparm License
Time::ParseDate License
Time::ParseDate License
Time::ParseDate License
Time::ParseDate License
Symlinks License
eCos license version 2.0
GNU General Public License v3.0 w/GCC Runtime Library exception",1.0,100.0
4,4,14,linux-master/tools/testing/selftests/hid/progs/hid.c,GPL-2.0-only,2098,"Copyright (c) 2022 Red hat
SPDX-License-Identifier: GPL-2.0
USAGE (Z)
-E2BIG
Change Usage Vendor globally
LOGICAL_MAXIMUM (1)
LOGICAL_MAXIMUM (1)
Usage Page (Vendor Defined Page 1)
Usage Page (Vendor Defined Page 1)
we need to be run first","SPDX-License-Identifier: GPL-2.0
Copyright (c) 2022 Red hat
offset */, 3 /* size
EPERM check
offset */, 4 /* size
EPERM check
offset */, 3 /* size
EPERM check
data needs to come at offset 0 so we can use it in calls
-E2BIG
EPERM check
USAGE_PAGE (Generic Desktop)
USAGE (Z)
REPORT_COUNT (1)
INPUT (Data,Var,Rel)
Usage Page (Vendor Defined Page 1)
USAGE_MINIMUM (1)
USAGE_MAXIMUM (3)
LOGICAL_MINIMUM (0)
LOGICAL_MAXIMUM (1)
REPORT_COUNT (3)
REPORT_SIZE (1)
Output (Data,Var,Abs)
REPORT_COUNT (1)
REPORT_SIZE (5)
Output (Cnst,Var,Abs)
Usage Page (Vendor Defined Page 1)
USAGE_MINIMUM (6)
USAGE_MAXIMUM (8)
LOGICAL_MINIMUM (0)
LOGICAL_MAXIMUM (1)
REPORT_COUNT (3)
REPORT_SIZE (1)
Feature (Data,Var,Abs)
REPORT_COUNT (1)
REPORT_SIZE (5)
Output (Cnst,Var,Abs)
END_COLLECTION
END_COLLECTION
offset */, 4096 /* size
EPERM check
insert rdesc at offset 73
Change Usage Vendor globally
offset */, 4 /* size
EPERM check
we need to be run first
offset */, 4 /* size
EPERM check
after insert0 and before insert2
offset */, 4 /* size
EPERM check
at the end",['SPDX-License-Identifier: GPL-2.0'],GPL-2.0-only,"FSF Unlimited License (with License Retention)
GL2PS License
zlib License
bzip2 and libbzip2 License v1.0.6
Systemics BSD variant license
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Hewlett-Packard 1986 License
Hewlett-Packard 1986 License
Creative Commons Attribution Share Alike 2.1 Japan",1.0,100.0
5,5,15,linux-master/tools/testing/selftests/mm/hugepage-mmap.c,GPL-2.0-only,1764,"SPDX-License-Identifier: GPL-2.0
Only ia64 requires this
hugepage-mmap:** Example of using huge page memory in a user application using the mmap* system call.  Before running this application, make sure that the* administrator has mounted the hugetlbfs filesystem (on some directory* like /mnt) using the command mount -t hugetlbfs nodev /mnt. In this* example, the app is requesting memory of size 256MB that is backed by* huge pages.** For the ia64 architecture, the Linux kernel reserves Region number 4 for* huge pages.  That means that if one requires a fixed address, a huge page* aligned address starting with 0x800000... will be required.  If a fixed* address is not required, the kernel will select an address in the proper* range.* Other architectures, such as ppc64, i386 or x86_64 are not so constrained.","SPDX-License-Identifier: GPL-2.0
hugepage-mmap:** Example of using huge page memory in a user application using the mmap* system call.  Before running this application, make sure that the* administrator has mounted the hugetlbfs filesystem (on some directory* like /mnt) using the command mount -t hugetlbfs nodev /mnt. In this* example, the app is requesting memory of size 256MB that is backed by* huge pages.** For the ia64 architecture, the Linux kernel reserves Region number 4 for* huge pages.  That means that if one requires a fixed address, a huge page* aligned address starting with 0x800000... will be required.  If a fixed* address is not required, the kernel will select an address in the proper* range.* Other architectures, such as ppc64, i386 or x86_64 are not so constrained.
Only ia64 requires this",['SPDX-License-Identifier: GPL-2.0'],GPL-2.0-only,"GL2PS License
Intel ACPI Software License Agreement
Caldera License",1.0,100.0
6,6,16,linux-master/tools/testing/selftests/net/forwarding/vxlan_bridge_1q_port_8472.sh,GPL-2.0-only,1564," !/bin/bash SPDX-License-Identifier: GPL-2.0
A wrapper to run VXLAN tests with an unusual port number.","A wrapper to run VXLAN tests with an unusual port number.
 !/bin/bash SPDX-License-Identifier: GPL-2.0",[' !/bin/sh SPDX-License-Identifier: GPL-2.0-only'],GPL-2.0-only,"The Parity Public License 7.0.0
Matrix Template Library License",1.0,100.0
7,7,17,linux-master/tools/testing/selftests/powerpc/eeh/eeh-basic.sh,GPL-2.0-only,1272," !/bin/sh SPDX-License-Identifier: GPL-2.0-only
Skip VFs for now since we don't have a reliable way to break them.
Build up a list of candidate devices.
 record the devices that we break in here. Assuming everything goes to plan we should get them back once the recover process is finished.
Add to this list of device to check
NB: may exit","NB: may exit
Build up a list of candidate devices.
Skip VFs for now since we don't have a reliable way to break them.
Add to this list of device to check
 !/bin/sh SPDX-License-Identifier: GPL-2.0-only
 record the devices that we break in here. Assuming everything goes to plan we should get them back once the recover process is finished.",[' !/bin/sh SPDX-License-Identifier: GPL-2.0-only'],GPL-2.0-only,"The Parity Public License 7.0.0
VOSTROM Public License for Open Source
The Parity Public License 7.0.0
AMD's plpa_map.c License
Creative Commons Attribution Share Alike 2.1 Japan
BSD 2-Clause NetBSD License",1.0,100.0
8,8,18,linux-master/tools/usb/usbip/src/Makefile.am,GPL-2.0-only,186,"# SPDX-License-Identifier: GPL-2.0
usbip_SOURCES := usbip.h utils.h usbip.c utils.c usbip_network.c \
LDADD       = $(top_builddir)/libsrc/libusbip.la
		 usbip_attach.c usbip_detach.c usbip_list.c \




AM_CPPFLAGS = -I$(top_srcdir)/libsrc -DUSBIDS_FILE='""@USBIDS_DIR@/usb.ids""'
sbin_PROGRAMS := usbip usbipd","# SPDX-License-Identifier: GPL-2.0
AM_CPPFLAGS = -I$(top_srcdir)/libsrc -DUSBIDS_FILE='""@USBIDS_DIR@/usb.ids""'
AM_CFLAGS   = @EXTRA_CFLAGS@
LDADD       = $(top_builddir)/libsrc/libusbip.la

sbin_PROGRAMS := usbip usbipd

usbip_SOURCES := usbip.h utils.h usbip.c utils.c usbip_network.c \
		 usbip_attach.c usbip_detach.c usbip_list.c \
		 usbip_bind.c usbip_unbind.c usbip_port.c

usbipd_SOURCES := usbip_network.h usbipd.c usbip_network.c
",['# SPDX-License-Identifier: GPL-2.0'],GPL-2.0-only,"GL2PS License
GNU General Public License v2.0 w/Classpath exception
ssh-keyscan License
Unicode License Agreement - Data Files and Software (2016)
Time::ParseDate License
Time::ParseDate License
Time::ParseDate License
Time::ParseDate License
Linux man-pages Copyleft
IPA Font License",1.0,100.0
9,9,19,linux-master/tools/tracing/latency/Makefile,GPL-2.0-only,244,"# SPDX-License-Identifier: GPL-2.0
#
clean:
install: all
LDFLAGS = -lpthread $(VAR_LDLIBS)
CFLAGS = -Wall -Wextra -g -O2 $(VAR_CFLAGS)



","# SPDX-License-Identifier: GPL-2.0
# Makefile for vm tools
#
VAR_CFLAGS := $(shell pkg-config --cflags libtracefs 2>/dev/null)
VAR_LDLIBS := $(shell pkg-config --libs libtracefs 2>/dev/null)

TARGETS = latency-collector
CFLAGS = -Wall -Wextra -g -O2 $(VAR_CFLAGS)
LDFLAGS = -lpthread $(VAR_LDLIBS)

all: $(TARGETS)

%: %.c
	$(CC) $(CFLAGS) -o $@ $< $(LDFLAGS)

clean:
	$(RM) latency-collector

prefix ?= /usr/local
sbindir ?= ${prefix}/sbin

install: all
	install -d $(DESTDIR)$(sbindir)
	install -m 755 -p $(TARGETS) $(DESTDIR)$(sbindir)
",['# SPDX-License-Identifier: GPL-2.0'],GPL-2.0-only,"GL2PS License
mpi Permissive License
mpi Permissive License
Universal Permissive License v1.0
LaTeX Project Public License v1.1
mpi Permissive License
Time::ParseDate License
Time::ParseDate License
Time::ParseDate License
Time::ParseDate License",1.0,100.0
10,10,20,linux-master/usr/gen_initramfs.sh,GPL,40," !/bin/sh Copyright (C) Martin Schlemmer <azarah@nosferatu.za.org> Copyright (C) 2006 Sam Ravnborg <sam@ravnborg.org>
Released under the terms of the GNU GPL
symlink test must come before file test
-gt 0 ]; do
\n# $1\n"" >> $cpio_list
 Generate a cpio packed initramfs. It uses gen_init_cpio to generate the cpio archive. This script assumes that gen_init_cpio is located in usr/ directory
 If output_file is set we will generate cpio archive we are careful to delete tmp files
 for each file print a line in following format <filetype> <name> <path to file> <octal mode> <uid> <gid> for links, devices etc the format differs. See gen_init_cpio for details
error out on errors
files included in initramfs - used by kbuild","Released under the terms of the GNU GPL
error out on errors
symlink test must come before file test
remap uid/gid to 0 if necessary
\n# $1\n"" >> $cpio_list
process one directory (incl sub-directories)
If $dirlist is only one line, then the directory is empty
If a directory is specified then add all files in it to fs
-gt 0 ]; do
files included in initramfs - used by kbuild
generate cpio image named $1
map $1 to uid=0 (root)
map $1 to gid=0 (root)
input file/dir - process it
 !/bin/sh Copyright (C) Martin Schlemmer <azarah@nosferatu.za.org> Copyright (C) 2006 Sam Ravnborg <sam@ravnborg.org>
 Generate a cpio packed initramfs. It uses gen_init_cpio to generate the cpio archive. This script assumes that gen_init_cpio is located in usr/ directory
 awk style field access $1 - field number; rest is argument string
 for each file print a line in following format <filetype> <name> <path to file> <octal mode> <uid> <gid> for links, devices etc the format differs. See gen_init_cpio for details
 ${srcdir}}"" change '//' into '/'
 If a regular file is specified, assume it is in gen_init_cpio format
 If output_file is set we will generate cpio archive we are careful to delete tmp files",['Released under the terms of the GNU GPL'],GPL,"Kastrup License
GNU General Public License v3.0 only
Symlinks License
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
mpi Permissive License
mpi Permissive License
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
AMD's plpa_map.c License",1.0,100.0
11,11,21,linux-master/tools/testing/selftests/net/reuseport_bpf.c,GPL,1467,"BPF_MOV64_REG(BPF_REG_6, BPF_REG_1)
BPF_ALU64_IMM(BPF_MOD, BPF_REG_0, mod)
BPF_LD_ABS(BPF_W, 0) R0 = (uint32_t)skb[0]
NOTE: UDP socket lookups traverse a different code path when there* are > 10 sockets in a group.  Run the bpf test through both paths.
bit 1: client side; bit-2 server side
BPF_EXIT_INSN()
return A
A = A % mod
TCP fastopen is required for the TCP tests
Test functionality of BPF filters for SO_REUSEPORT.  The tests below will use* a BPF program (both classic and extended) to read the first word from an* incoming packet (expected to be in network byte-order), calculate a modulus* of that number, and then dispatch the packet to the Nth socket using the* result.  These tests are run for each supported address family and protocol.* Additionally, a few edge cases in the implementation are tested.","Test functionality of BPF filters for SO_REUSEPORT.  The tests below will use* a BPF program (both classic and extended) to read the first word from an* incoming packet (expected to be in network byte-order), calculate a modulus* of that number, and then dispatch the packet to the Nth socket using the* result.  These tests are run for each supported address family and protocol.* Additionally, a few edge cases in the implementation are tested.
BPF_MOV64_REG(BPF_REG_6, BPF_REG_1)
BPF_LD_ABS(BPF_W, 0) R0 = (uint32_t)skb[0]
BPF_ALU64_IMM(BPF_MOD, BPF_REG_0, mod)
BPF_EXIT_INSN()
A = (uint32_t)skb[0]
A = A % mod
return A
bit 1: client side; bit-2 server side
NOTE: UDP socket lookups traverse a different code path when there* are > 10 sockets in a group.  Run the bpf test through both paths.
TCP fastopen is required for the TCP tests",[''],No_license_found,"OpenPBS v2.3 Software License
AMD's plpa_map.c License
OpenPBS v2.3 Software License
OpenPBS v2.3 Software License
BitTorrent Open Source License v1.0
Python License 2.0.1
Gutmann License
mpi Permissive License
OpenPBS v2.3 Software License
OpenPBS v2.3 Software License",0.0,0.0
12,12,22,linux-master/CREDITS,GPL,10,"W: https://lwn.net/Articles/691000/
----------
W: https://lwn.net/Articles/789028/
W: http://sunsite.mff.cuni.cz/~jj
W: http://maxim.org.za/at91_26.html
W: http://users.telenet.be/geertu/
W: http://www.qsl.net/dl1bke/
W: http://wand.net.nz/~iam4
D: The XFree86[tm] Project
W: http://www.chello.nl/~j.vreeken/","	This is at least a partial credits-file of people that have
	contributed to the Linux project.  It is sorted by name and
	formatted to allow easy grepping and beautification by
	scripts.  The fields are: name (N), email (E), web-address
	(W), PGP key ID and fingerprint (P), description (D), and
	snail-mail address (S).
	Thanks,

			Linus
----------

N: Matt Mackal
E: mpm@selenic.com
D: SLOB slab allocator

N: Matti Aarnio
E: mea@nic.funet.fi
D: Alpha systems hacking, IPv6 and other network related stuff
D: One of assisting postmasters for vger.kernel.org's lists
S: (ask for current address)
S: Finland

N: Thomas Abraham
E: thomas.ab@samsung.com
D: Samsung pin controller driver

N: Dragos Acostachioaie
E: dragos@iname.com
W: http://www.arbornet.org/~dragos
D: /proc/sysvipc
S: C. Negri 6, bl. D3
S: Iasi 6600
S: Romania

N: Mark Adler
E: madler@alumni.caltech.edu
W: https://alumnus.caltech.edu/~madler/
D: zlib decompression

N: Monalisa Agrawal
E: magrawal@nortelnetworks.com
D: Basic Interphase 5575 driver with UBR and ABR support.
S: 75 Donald St, Apt 42
S: Weymouth, MA 02188
S: USA

N: Dave Airlie
E: airlied@linux.ie
W: http://www.csn.ul.ie/~airlied
D: NFS over TCP patches
D: in-kernel DRM Maintainer
S: Longford, Ireland
S: Sydney, Australia

N: Tigran A. Aivazian
E: tigran@aivazian.fsnet.co.uk
W: http://www.moses.uklinux.net/patches
D: BFS filesystem
D: Intel IA32 CPU microcode update support
D: Various kernel patches
S: United Kingdom

N: Werner Almesberger
E: werner@almesberger.net
W: https://www.almesberger.net/
D: dosfs, LILO, some fd features, ATM, various other hacks here and there
S: Buenos Aires
S: Argentina

N: Tim Alpaerts
E: tim_alpaerts@toyota-motor-europe.com
D: 802.2 class II logical link control layer,
D: the humble start of an opening towards the IBM SNA protocols
S: Klaproosstraat 72 c 10
S: B-2610 Wilrijk-Antwerpen
S: Belgium

N: Anton Altaparmakov
E: aia21@cantab.net
W: http://www-stu.christs.cam.ac.uk/~aia21/
D: Author of new NTFS driver, various other kernel hacks.
S: Christ's College
S: Cambridge CB2 3BU
S: United Kingdom

N: C. Scott Ananian
E: cananian@alumni.princeton.edu
W: http://www.pdos.lcs.mit.edu/~cananian
P: 1024/85AD9EED AD C0 49 08 91 67 DF D7  FA 04 1A EE 09 E8 44 B0
D: Unix98 pty support.
D: APM update to 1.2 spec.
D: /devfs hacking.
S: 7 Kiwi Loop
S: Howell, NJ 07731
S: USA

N: Erik Andersen
E: andersen@codepoet.org
W: https://www.codepoet.org/
P: 1024D/30D39057 1BC4 2742 E885 E4DE 9301  0C82 5F9B 643E 30D3 9057
D: Maintainer of ide-cd and Uniform CD-ROM driver,
D: ATAPI CD-Changer support, Major 2.1.x CD-ROM update.
S: 352 North 525 East
S: Springville, Utah 84663
S: USA

N: Michael Ang
E: mang@subcarrier.org
W: http://www.subcarrier.org/mang
D: Linux/PA-RISC hacker
S: 85 Frank St.
S: Ottawa, Ontario
S: Canada K2P 0X3

N: H. Peter Anvin
E: hpa@zytor.com
W: https://www.zytor.com/~hpa/
P: 2047/2A960705 BA 03 D3 2C 14 A8 A8 BD  1E DF FE 69 EE 35 BD 74
D: Author of the SYSLINUX boot loader, maintainer of the linux.* news
D: hierarchy and the Linux Device List; various kernel hacks
S: 4390 Albany Drive #46
S: San Jose, California 95129
S: USA

N: Andrea Arcangeli
E: andrea@suse.de
W: https://www.kernel.org/pub/linux/kernel/people/andrea/
P: 1024D/68B9CB43 13D9 8355 295F 4823 7C49  C012 DFA1 686E 68B9 CB43
P: 1024R/CB4660B9 CC A0 71 81 F4 A0 63 AC  C0 4B 81 1D 8C 15 C8 E5
D: Parport hacker
D: Implemented a workaround for some interrupt buggy printers
D: Author of pscan that helps to fix lp/parport bugs
D: Author of lil (Linux Interrupt Latency benchmark)
D: Fixed the shm swap deallocation at swapoff time (try_to_unuse message)
D: VM hacker
D: Various other kernel hacks
S: Imola 40026
S: Italy

N: Derek Atkins
E: warlord@MIT.EDU
D: Linux-AFS Port, random kernel hacker,
D: VFS fixes (new notify_change in particular)
D: Moving all VFS access checks into the file systems
S: MIT Room E15-341
S: 20 Ames Street
S: Cambridge, Massachusetts 02139
S: USA

N: Michel Aubry
E: giovanni <giovanni@sudfr.com>
D: Aladdin 1533/1543(C) chipset IDE
D: VIA MVP-3/TX Pro III chipset IDE

N: Jens Axboe
E: axboe@suse.de
D: Linux CD-ROM maintainer, DVD support
D: elevator + block layer rewrites
D: highmem I/O support
D: misc hacking on IDE, SCSI, block drivers, etc
S: Peter Bangs Vej 258, 2TH
S: 2500 Valby
S: Denmark

N: John Aycock
E: aycock@cpsc.ucalgary.ca
D: Adaptec 274x driver
S: Department of Computer Science
S: University of Calgary
S: Calgary, Alberta
S: Canada

N: Miles Bader
E: miles@gnu.org
D: v850 port (uClinux)
S: NEC Corporation
S: 1753 Shimonumabe, Nakahara-ku
S: Kawasaki 211-8666
S: Japan

N: Ralf Baechle
E: ralf@gnu.org
P: 1024/AF7B30C1 CF 97 C2 CC 6D AE A7 FE  C8 BA 9C FC 88 DE 32 C3
D: Linux/MIPS port
D: Linux/68k hacker
S: Hauptstrasse 19
S: 79837 St. Blasien
S: Germany

N: Krishna Balasubramanian
E: balasub@cis.ohio-state.edu
D: Wrote SYS V IPC (part of standard kernel since 0.99.10)

B: Robert Baldyga
E: r.baldyga@hackerion.com
D: Samsung S3FWRN5 NCI NFC Controller

N: Chris Ball
E: chris@printf.net
D: Former maintainer of the MMC/SD/SDIO subsystem.

N: Dario Ballabio
E: ballabio_dario@emc.com
E: dario.ballabio@tiscalinet.it
E: dario.ballabio@inwind.it
D: Author and maintainer of the Ultrastor 14F/34F SCSI driver
D: Author and maintainer of the EATA ISA/EISA/PCI SCSI driver
S: EMC Corporation
S: Milano
S: Italy

N: Paul Bame
E: bame@debian.org
E: bame@puffin.external.hp.com
E: paul_bame@hp.com
W: http://www.parisc-linux.org
D: PA-RISC 32 and 64-bit early boot, firmware interface, interrupts, misc
S: MS42
S: Hewlett-Packard
S: 3404 E Harmony Rd
S: Fort Collins, CO 80525
S: USA

N: Arindam Banerji
E: axb@cse.nd.edu
D: Contributed ESDI driver routines needed to port LINUX to the PS/2 MCA.
S: Department of Computer Science & Eng.
S: University of Notre Dame
S: Notre Dame, Indiana
S: USA

N: Kai Bankett
E: chaosman@ontika.net
D: QNX6 filesystem

N: Greg Banks
E: gnb@alphalink.com.au
D: IDT77105 ATM network driver
D: some SuperH port work
D: some trivial futzing with kconfig

N: James Banks
E: james@sovereign.org
D: TLAN network driver
D: Logitech Busmouse driver

N: Krzysztof G. Baranowski
E: kgb@manjak.knm.org.pl
P: 1024/FA6F16D1 96 D1 1A CF 5F CA 69 EC  F9 4F 36 1F 6D 60 7B DA
D: Maintainer of the System V file system.
D: System V fs update for 2.1.x dcache.
D: Forward ported a couple of SCSI drivers.
D: Various bugfixes.
S: ul. Koscielna 12a
S: 62-300 Wrzesnia
S: Poland

N: Fred Barnes
E: frmb2@ukc.ac.uk
D: Various parport/ppdev hacks and fixes
S: Computing Lab, The University
S: Canterbury, KENT
S: CT2 7NF
S: England

N: Paul Barton-Davis
E: pbd@op.net
D: Driver for WaveFront soundcards (Turtle Beach Maui, Tropez, Tropez+)
D: Various bugfixes and changes to sound drivers
S: USA

N: Carlos Henrique Bauer
E: chbauer@acm.org
E: bauer@atlas.unisinos.br
D: Some new sysctl entries for the parport driver.
D: New sysctl function for handling unsigned longs
S: Universidade do Vale do Rio dos Sinos - UNISINOS
S: DSI/IDASI
S: Av. Unisinos, 950
S: 93022000 Sao Leopoldo RS
S: Brazil

N: Peter Bauer
E: 100136.3530@compuserve.com
D: Driver for depca-ethernet-board
S: 69259 Wilhemsfeld
S: Rainweg 15
S: Germany

N: Fred Baumgarten
E: dc6iq@insl1.etec.uni-karlsruhe.de
E: dc6iq@adacom.org
E: dc6iq@db0ais.#hes.deu.eu (packet radio)
D: NET-2 & netstat(8)
S: Soevener Strasse 11
S: 53773 Hennef
S: Germany

N: Donald Becker
E: becker@cesdis.gsfc.nasa.gov
D: General low-level networking hacker
D: Most of the ethercard drivers
D: Original author of the NFS server
S: USRA Center of Excellence in Space Data and Information Sciences
S: Code 930.5, Goddard Space Flight Center
S: Greenbelt, Maryland 20771
S: USA

N: Adam Belay
E: ambx1@neo.rr.com
D: Linux Plug and Play Support
S: USA

N: Daniele Bellucci
E: bellucda@tiscali.it
D: Various Janitor work.
W: http://web.tiscali.it/bellucda
S: Via Delle Palme, 9
S: Terni 05100
S: Italy

N: Ohad Ben Cohen
E: ohad@wizery.com
D: Remote Processor (remoteproc) subsystem
D: Remote Processor Messaging (rpmsg) subsystem

N: Krzysztof Benedyczak
E: golbi@mat.uni.torun.pl
W: http://www.mat.uni.torun.pl/~golbi
D: POSIX message queues fs (with M. Wronski)
S: ul. Podmiejska 52
S: Radunica
S: 83-000 Pruszcz Gdanski
S: Poland

N: Randolph Bentson
E: bentson@grieg.seaslug.org
W: http://www.aa.net/~bentson/
P: 1024/39ED5729 5C A8 7A F4 B2 7A D1 3E  B5 3B 81 CF 47 30 11 71
D: Author of driver for Cyclom-Y and Cyclades-Z async mux
S: 2322 37th Ave SW
S: Seattle, Washington 98126-2010
S: USA

N: Muli Ben-Yehuda
E: mulix@mulix.org
E: muli@il.ibm.com
W: http://www.mulix.org
D: trident OSS sound driver, x86-64 dma-ops and Calgary IOMMU,
D: KVM and Xen bits and other misc. hackery.
S: Haifa, Israel

N: Johannes Berg
E: johannes@sipsolutions.net
W: https://johannes.sipsolutions.net/
P: 4096R/7BF9099A C0EB C440 F6DA 091C 884D  8532 E0F3 73F3 7BF9 099A
D: powerpc & 802.11 hacker

N: Stephen R. van den Berg (AKA BuGless)
E: berg@pool.informatik.rwth-aachen.de
D: General kernel, gcc, and libc hacker
D: Specialisation: tweaking, ensuring portability, tweaking, cleaning,
D: tweaking and occasionally debugging :-)
S: Bouwensstraat 22
S: 6369 BG Simpelveld
S: The Netherlands

N: Peter Berger
E: pberger@brimson.com
W: http://www.brimson.com
D: Author/maintainer of Digi AccelePort USB driver
S: 1549 Hiironen Rd.
S: Brimson, MN  55602
S: USA

N: Hennus Bergman
P: 1024/77D50909 76 99 FD 31 91 E1 96 1C  90 BB 22 80 62 F6 BD 63
D: Author and maintainer of the QIC-02 tape driver
S: The Netherlands

N: Tomas Berndtsson
E: tomas@nocrew.org
W: http://tomas.nocrew.org/
D: dsp56k device driver

N: Srivatsa S. Bhat
E: srivatsa@csail.mit.edu
D: Maintainer of Generic Paravirt-Ops subsystem
D: Maintainer of VMware hypervisor interface
D: Maintainer of VMware virtual PTP clock driver (ptp_vmw)

N: Ross Biro
E: ross.biro@gmail.com
D: Original author of the Linux networking code

N: Anton Blanchard
E: anton@samba.org
W: https://samba.org/~anton/
P: 1024/8462A731 4C 55 86 34 44 59 A7 99  2B 97 88 4A 88 9A 0D 97
D: sun4 port, Sparc hacker

N: Hugh Blemings
E: hugh@blemings.org
W: http://blemings.org/hugh
D: Original author of the Keyspan USB to serial drivers, random PowerPC hacker
S: PO Box 234
S: Belconnen ACT 2616
S: Australia

N: Philip Blundell
E: philb@gnu.org
D: Linux/ARM hacker
D: Device driver hacker (eexpress, 3c505, c-qcam, ...)
D: m68k port to HP9000/300
D: AUN network protocols
D: Co-architect of the parallel port sharing system
D: IPv6 netfilter
S: FutureTV Labs Ltd
S: Brunswick House, 61-69 Newmarket Rd, Cambridge CB5 8EG
S: United Kingdom

N: Thomas Bogendörfer
E: tsbogend@alpha.franken.de
D: PCnet32 driver, SONIC driver, JAZZ_ESP driver
D: newport abscon driver, g364 framebuffer driver
D: strace for Linux/Alpha
D: Linux/MIPS hacker
S: Schafhofstr. 40
S: 90556 Cadolzburg
S: Germany

N: Bill Bogstad
E: bogstad@pobox.com
D: wrote /proc/self hack, minor samba & dosemu patches

N: Axel Boldt
E: axel@uni-paderborn.de
W: http://math-www.uni-paderborn.de/~axel/
D: Configuration help text support
D: Linux CD and Support Giveaway List

N: Erik Inge Bolsø
E: knan@mo.himolde.no
D: Misc kernel hacks
D: Updated PC speaker driver for 2.3
S: Norway

N: Andreas E. Bombe
E: andreas.bombe@munich.netsurf.de
W: http://home.pages.de/~andreas.bombe/
P: 1024/04880A44 72E5 7031 4414 2EB6 F6B4  4CBD 1181 7032 0488 0A44
D: IEEE 1394 subsystem rewrite and maintainer
D: Texas Instruments PCILynx IEEE 1394 driver

N: Al Borchers
E: alborchers@steinerpoint.com
D: Author/maintainer of Digi AccelePort USB driver
D: work on usbserial and keyspan_pda drivers
S: 4912 Zenith Ave. S.
S: Minneapolis, MN  55410
S: USA

N: Marc Boucher
E: marc@mbsi.ca
P: CA 67 A5 1A 38 CE B6 F2  D5 83 51 03 D2 9C 30 9E  CE D2 DD 65
D: Netfilter core
D: IP policy routing by mark
D: Various fixes (mostly networking)
S: Montreal, Quebec
S: Canada

N: Zoltán Böszörményi
E: zboszor@mail.externet.hu
D: MTRR emulation with Cyrix style ARR registers, Athlon MTRR support

N: John Boyd
E: boyd@cis.ohio-state.edu
D: Co-author of wd7000 SCSI driver
S: 101 Curl Drive #591
S: Columbus, Ohio 43210
S: USA

N: Peter Braam
E: braam@clusterfs.com
W: http://www.clusterfs.com/
D: Coda & InterMezzo filesystems
S: 181 McNeil
S: Canmore, AB
S: Canada, T1W 2R9

N: Ryan Bradetich
E: rbradetich@uswest.net
D: Linux/PA-RISC hacker
S: 1200 Goldenrod Dr.
S: Nampa, Idaho 83686
S: USA

N: Dirk J. Brandewie
E: dirk.j.brandewie@intel.com
E: linux-wimax@intel.com
D: Intel Wireless WiMAX Connection 2400 SDIO driver

N: Derrick J. Brashear
E: shadow@dementia.org
W: http://www.dementia.org/~shadow
P: 512/71EC9367 C5 29 0F BC 83 51 B9 F0  BC 05 89 A0 4F 1F 30 05
D: Author of Sparc CS4231 audio driver, random Sparc work
S: 403 Gilmore Avenue
S: Trafford, Pennsylvania 15085
S: USA

N: Dag Brattli
E: dagb@cs.uit.no
W: http://www.cs.uit.no/~dagb
D: IrDA Subsystem
S: 19. Wellington Road
S: Lancaster, LA1 4DN
S: UK, England

N: Lars Brinkhoff
E: lars@nocrew.org
W: http://lars.nocrew.org/
D: dsp56k device driver
D: ptrace proxy in user mode kernel port
S: Kopmansg 2
S: 411 13  Goteborg
S: Sweden

N: Paul Bristow
E: paul@paulbristow.net
W: https://paulbristow.net/linux/idefloppy.html
D: Maintainer of IDE/ATAPI floppy driver

N: Stefano Brivio
E: stefano.brivio@polimi.it
D: Broadcom B43 driver

N: Dominik Brodowski
E: linux@brodo.de
W: https://www.brodo.de/
P: 1024D/725B37C6  190F 3E77 9C89 3B6D BECD  46EE 67C3 0308 725B 37C6
D: parts of CPUFreq code, ACPI bugfixes, PCMCIA rewrite, cpufrequtils
S: Tuebingen, Germany

N: Andries Brouwer
E: aeb@cwi.nl
D: random Linux hacker
S: Bessemerstraat 21
S: Amsterdam
S: The Netherlands

N: NeilBrown
E: neil@brown.name
P: 4096R/566281B9 1BC6 29EB D390 D870 7B5F  497A 39EC 9EDD 5662 81B9
D: NFSD Maintainer 2000-2007
D: MD Maintainer 2001-2016

N: Zach Brown
E: zab@zabbo.net
D: maestro pci sound

N: David Brownell
D: Kernel engineer, mentor, and friend.  Maintained USB EHCI and
D: gadget layers, SPI subsystem, GPIO subsystem, and more than a few
D: device drivers.  His encouragement also helped many engineers get
D: started working on the Linux kernel.  David passed away in early
D: 2011, and will be greatly missed.
W: https://lore.kernel.org/lkml/20110405034819.GA7872@kroah.com

N: Gary Brubaker
E: xavyer@ix.netcom.com
D: USB Serial Empeg Empeg-car Mark I/II Driver

N: Matthias Bruestle
E: m@mbsks.franken.de
D: REINER SCT cyberJack pinpad/e-com USB chipcard reader driver
S: Germany

N: Adrian Bunk
P: 1024D/4F12B400  B29C E71E FE19 6755 5C8A  84D4 99FC EA98 4F12 B400
D: misc kernel hacking and testing

N: Ray Burr
E: ryb@nightmare.com
D: Original author of Amiga FFS filesystem
S: Orlando, Florida
S: USA

N: Paul Burton
E: paulburton@kernel.org
W: https://pburton.com
D: MIPS maintainer 2018-2020

N: Lennert Buytenhek
E: kernel@wantstofly.org
D: Original (2.4) rewrite of the ethernet bridging code
D: Various ARM bits and pieces
S: Ravenhorst 58
S: 2317 AK Leiden
S: The Netherlands

N: Michael Callahan
E: callahan@maths.ox.ac.uk
D: PPP for Linux
S: The Mathematical Institute
S: 25-29 St Giles
S: Oxford
S: United Kingdom

N: Luiz Fernando N. Capitulino
E: lcapitulino@mandriva.com.br
E: lcapitulino@gmail.com
W: http://www.cpu.eti.br
D: misc kernel hacking
S: Mandriva
S: Brazil

N: Remy Card
E: Remy.Card@masi.ibp.fr
E: Remy.Card@linux.org
D: Extended file system [defunct] designer and developer
D: Second extended file system designer and developer
S: Institut Blaise Pascal
S: 4 Place Jussieu
S: 75252 Paris Cedex 05
S: France

N: Ulf Carlsson
D: SGI Indy audio (HAL2) drivers
E: ulfc@bun.falkenberg.se

N: Ed Carp
E: ecarp@netcom.com
D: uucp, elm, pine, pico port
D: cron, at(1) developer
S: 48287 Sawleaf
S: Fremont, California 94539
S: USA

N: Tomas Cech
E: sleep_walker@suse.com
D: arm/palm treo support

N: Florent Chabaud
E: florent.chabaud@polytechnique.org
D: software suspend
S: SGDN/DCSSI/SDS/LTI
S: 58, Bd Latour-Maubourg
S: 75700 Paris 07 SP
S: France

N: Gordon Chaffee
E: chaffee@cs.berkeley.edu
W: http://bmrc.berkeley.edu/people/chaffee/
D: vfat, fat32, joliet, native language support
S: 3700 Warwick Road
S: Fremont, California 94555
S: USA

N: Chih-Jen Chang
E: chihjenc@scf.usc.edu
E: chihjen@iis.sinica.edu.tw
D: IGMP(Internet Group Management Protocol) version 2
S: 3F, 65 Tajen street
S: Tamsui town, Taipei county,
S: Taiwan 251
S: Republic of China

N: Reinette Chatre
E: reinette.chatre@intel.com
D: WiMedia Link Protocol implementation
D: UWB stack bits and pieces

N: Michael Elizabeth Chastain
E: mec@shout.net
D: Configure, Menuconfig, xconfig

N: Mauro Carvalho Chehab
E: m.chehab@samsung.org
E: mchehab@osg.samsung.com
E: mchehab@infradead.org
D: Media subsystem (V4L/DVB) drivers and core
D: EDAC drivers and EDAC 3.0 core rework
S: Brazil

N: Raymond Chen
E: raymondc@microsoft.com
D: Author of Configure script
S: 14509 NE 39th Street #1096
S: Bellevue, Washington 98007
S: USA

N: Chris Cheney
E: chris.cheney@gmail.com
E: ccheney@redhat.com
P: 1024D/8E384AF2 2D31 1927 87D7 1F24 9FF9  1BC5 D106 5AB3 8E38 4AF2
D: Vista Imaging usb webcam driver
S: 2308 Therrell Way
S: McKinney, TX 75070
S: USA

N: Stuart Cheshire
E: cheshire@cs.stanford.edu
D: Author of Starmode Radio IP (STRIP) driver
D: Originator of design for new combined interrupt handlers
S: William Gates Department
S: Stanford University
S: Stanford, California 94305
S: USA

N: Carlos Chinea
E: carlos.chinea@nokia.com
E: cch.devel@gmail.com
D: Author of HSI Subsystem

N: Randolph Chung
E: tausq@debian.org
D: Linux/PA-RISC hacker
S: Hong Kong

N: Juan Jose Ciarlante
W: http://juanjox.kernelnotes.org/
E: jjciarla@raiz.uncu.edu.ar
E: jjo@mendoza.gov.ar
D: Network driver alias support
D: IP masq hashing and app modules
D: IP masq 2.1 features and bugs
S: Las Cuevas 2385 - Bo Guemes
S: Las Heras, Mendoza CP 5539
S: Argentina

N: Jay Cliburn
E: jcliburn@gmail.com
D: ATLX Ethernet drivers

N: Steven P. Cole
E: scole@lanl.gov
E: elenstev@mesatop.com
D: Various build fixes and kernel documentation.
S: Los Alamos, New Mexico
S: USA

N: Hamish Coleman
E: hamish@zot.apana.org.au
D: SEEQ8005 network driver
S: 98 Paxton Street
S: East Malvern, Victoria, 3145
S: Australia

N: Neil Conway
E: nconway.list@ukaea.org.uk
D: Assorted sched/mm titbits
S: Oxfordshire, UK.

N: Kees Cook
E: kees@outflux.net
E: kees@ubuntu.com
E: keescook@chromium.org
W: http://outflux.net/blog/
P: 4096R/DC6DC026 A5C3 F68F 229D D60F 723E  6E13 8972 F4DF DC6D C026
D: Various security things, bug fixes, and documentation.
S: (ask for current address)
S: Portland, Oregon
S: USA

N: Jason Cooper
D: ARM/Marvell SOC co-maintainer
D: irqchip co-maintainer
D: MVEBU PCI DRIVER co-maintainer

N: Robin Cornelius
E: robincornelius@users.sourceforge.net
D: Ralink rt2x00 WLAN driver
S: Cornwall, U.K.

N: Mark Corner
E: mcorner@umich.edu
W: http://www.eecs.umich.edu/~mcorner/
D: USB Bluetooth Driver
S: University of Michigan
S: Ann Arbor, MI

N: Michael Cornwell
E: cornwell@acm.org
D: Original designer and co-author of ATA Taskfile
D: Kernel module SMART utilities
S: Santa Cruz, California
S: USA

N: Luis Correia
E: luisfcorreia@gmail.com
D: Ralink rt2x00 WLAN driver
S: Belas, Portugal

N: Alan Cox
W: http://www.linux.org.uk/diary/
D: Linux Networking (0.99.10->2.0.29)
D: Original Appletalk, AX.25, and IPX code
D: 3c501 hacker
D: Watchdog timer drivers
D: Linux/SMP x86 (up to 2.0 only)
D: Initial Mac68K port
D: Video4Linux design, bw-qcam and PMS driver ports.
D: IDE modularisation work
D: Z85230 driver
D: Former security contact point (please use vendor-sec@lst.de)
D: ex 2.2 maintainer
D: 2.1.x modular sound
D: Assigned major/minor numbers maintainer at lanana.org
S: c/o Red Hat UK Ltd
S: Alexandra House
S: Alexandra Terrace
S: Guildford, GU1 3DA
S: United Kingdom

N: Cristian Mihail Craciunescu
W: http://www.dnt.ro/~cristi/
E: cristi@dnt.ro
D: Support for Xircom PGSDB9 (firmware and host driver)
S: Bucharest
S: Romania

N: Laurence Culhane
E: loz@holmes.demon.co.uk
D: Wrote the initial alpha SLIP code
S: 81 Hood Street
S: Northampton
S: NN1 3QT
S: United Kingdom

N: Massimo Dal Zotto
E: dz@debian.org
D: i8k Dell laptop SMM driver

N: Uwe Dannowski
E: Uwe.Dannowski@ira.uka.de
W: http://i30www.ira.uka.de/~dannowsk/
D: FORE PCA-200E driver
S: University of Karlsruhe
S: Germany

N: Ray Dassen
E: jdassen@wi.LeidenUniv.nl
W: http://www.wi.leidenuniv.nl/~jdassen/
P: 1024/672D05C1 DD 60 32 60 F7 90 64 80  E7 6F D4 E4 F8 C9 4A 58
D: Debian GNU/Linux: www.debian.org maintainer, FAQ co-maintainer,
D: packages testing, nit-picking & fixing. Enjoying BugFree (TM) kernels.
S: Zuidsingel 10A
S: 2312 SB  Leiden
S: The Netherlands

N: David Davies
E: davies@wanton.lkg.dec.com
D: Network driver author - depca, ewrk3 and de4x5
D: Wrote shared interrupt support
S: Digital Equipment Corporation
S: 550 King Street
S: Littleton, Massachusetts 01460
S: USA

N: Frank Davis
E: fdavis@si.rr.com
E: fdavis112@juno.com
D: Various kernel patches
S: 8 Lakeview Terr.
S: Kerhonkson, NY 12446
S: USA

N: Wayne Davison
E: davison@borland.com
D: Second extended file system co-designer

N: Terry Dawson
E: terry@perf.no.itg.telecom.com.au
E: terry@albert.vk2ktj.ampr.org (Amateur Radio use only)
D: trivial hack to add variable address length routing to Rose.
D: AX25-HOWTO, HAM-HOWTO, IPX-HOWTO, NET-2-HOWTO
D: ax25-utils maintainer.

N: Kamil Debski
E: kamil@wypas.org
D: Samsung S5P 2D graphics acceleration and Multi Format Codec drivers
D: Samsung USB2 phy drivers
D: PWM fan driver

N: Helge Deller
E: deller@gmx.de
W: http://www.parisc-linux.org/
D: PA-RISC Linux architecture maintainer
D: LASI-, ASP-, WAX-, LCD/LED-driver
S: Germany

N: Jean Delvare
E: jdelvare@suse.de
W: http://jdelvare.nerim.net/
D: Several hardware monitoring drivers
S: France

N: Frank ""Jedi/Sector One"" Denis
E: j@pureftpd.org
D: QNX4 filesystem

N: Peter Denison
E: peterd@pnd-pc.demon.co.uk
W: http://www.pnd-pc.demon.co.uk/promise/
D: Promise DC4030VL caching HD controller drivers

N: Todd J. Derr
E: tjd@fore.com
W: https://www.wordsmith.org/~tjd
D: Random console hacks and other miscellaneous stuff
S: 3000 FORE Drive
S: Warrendale, Pennsylvania 15086
S: USA

N: Ludovic Desroches
E: ludovic.desroches@microchip.com
D: Maintainer for ARM/Microchip (AT91) SoC support
D: Author of ADC, pinctrl, XDMA and SDHCI drivers for this platform
S: France

N: Martin Devera
E: devik@cdi.cz
W: http://luxik.cdi.cz/~devik/qos/
D: HTB qdisc and random networking hacks

N: Alex deVries
E: alex@onefishtwo.ca
D: Various SGI parts, bits of HAL2 and Newport, PA-RISC Linux.
S: 41.5 William Street
S: Ottawa, Ontario
S: K1N 6Z9
S: CANADA

N: Vivien Didelot
E: vivien.didelot@gmail.com
D: DSA framework and MV88E6XXX driver
S: Montreal, Quebec, Canada

N: Jeff Dike
E: jdike@karaya.com
W: http://user-mode-linux.sourceforge.net
D: User mode kernel port
S: 375 Tubbs Hill Rd
S: Deering NH 03244
S: USA

N: Matt Domsch
E: Matt_Domsch@dell.com
W: https://www.dell.com/linux
W: https://domsch.com/linux
D: Linux/IA-64
D: Dell PowerEdge server, SCSI layer, misc drivers, and other patches
S: Dell Inc.
S: One Dell Way
S: Round Rock, TX  78682
S: USA

N: Mattia Dongili
E: malattia@gmail.com
D: cpufrequtils (precursor to cpupowerutils)

N: Ben Dooks
E: ben-linux@fluff.org
E: ben@simtec.co.uk
W: http://www.fluff.org/ben/
W: http://www.simtec.co.uk/
D: Samsung S3C2410/S3C2440 support, general ARM support
D: Maintaining Simtec Electronics development boards
S: Simtec Electronics
S: Avondale Drive
S: Tarleton
S: Preston
S: Lancs
S: PR4 6AX
S: United Kingdom

N: Ivo van Doorn
E: IvDoorn@gmail.com
W: http://www.mendiosus.nl
D: Ralink rt2x00 WLAN driver
S: Haarlem, The Netherlands

N: John G Dorsey
E: john+@cs.cmu.edu
D: ARM Linux ports to Assabet/Neponset, Spot
S: Department of Electrical and Computer Engineering
S: Carnegie Mellon University
S: Pittsburgh, PA  15213
S: USA

N: Eddie C. Dost
E: ecd@skynet.be
D: Linux/Sparc kernel hacker
D: Linux/Sparc maintainer
S: Rue de la Chapelle 51
S: 4850 Moresnet
S: Belgium

N: Cort Dougan
E: cort@fsmlabs.com
W: http://www.fsmlabs.com/linuxppcbk.html
D: PowerPC

N: Daniel Drake
E: dsd@gentoo.org
D: USBAT02 CompactFlash support in usb-storage
D: ZD1211RW wireless driver
S: UK

N: Oleg Drokin
E: green@ccssu.crimea.ua
W: http://www.ccssu.crimea.ua/~green
D: Cleaning up sound drivers, SA1100 Watchdog.
S: Skvoznoy per., 14a
S: Evpatoria
S: Crimea
S: UKRAINE, 334320

N: Walt Drummond
E: drummond@valinux.com
D: Linux/IA-64
S: 1382 Bordeaux Drive
S: Sunnyvale, CA 94087
S: USA

N: Bruno Ducrot
E: ducrot@poupinou.org
D: CPUFreq and ACPI bugfixes.
S: Mougin, France

N: Don Dugger
E: n0ano@valinux.com
D: Linux/IA-64
S: 1209 Pearl Street, #12
S: Boulder, CO 80302
S: USA

N: Thomas Dunbar
E: tdunbar@vt.edu
D: TeX & METAFONT hacking/maintenance
S: Virginia Tech Computing Center
S: 1700 Pratt Drive
S: Blacksburg, Virginia 24061
S: USA

N: Randy Dunlap
E: rdunlap@infradead.org
W: https://www.infradead.org/~rdunlap/
D: Linux-USB subsystem, USB core/UHCI/printer/storage drivers
D: x86 SMP, ACPI, bootflag hacking
D: documentation, builds
S: (ask for current address)
S: USA

N: Bob Dunlop
E: rjd@xyzzy.clara.co.uk
E: bob.dunlop@farsite.co.uk
W: www.farsite.co.uk
D: FarSync card device driver
S: FarSite Communications Ltd
S: Tempus Business Centre
S: 60 Kingsclere Road
S: Basingstoke       RG21 6XG
S: UK

N: Cyrus Durgin
E: cider@speakeasy.org
W: http://www.speakeasy.org/~cider/
D: implemented kmod

N: Torsten Duwe
E: Torsten.Duwe@informatik.uni-erlangen.de
D: Part-time kernel hacker
D: The Linux Support Team Erlangen
S: Grevenbroicher Str. 17
S: 47807 Krefeld
S: Germany

N: Tom Dyas
E: tdyas@eden.rutgers.edu
D: minor hacks and some sparc port stuff
S: New Jersey
S: USA

N: Drew Eckhardt
E: drew@PoohSticks.ORG
D: SCSI code
D: Assorted snippets elsewhere
D: Boot sector ""..."" printing
S: 2037 Walnut #6
S: Boulder, Colorado 80302
S: USA

N: Hans-Christian Noren Egtvedt
E: egtvedt@samfundet.no
D: AVR32 architecture maintainer.

N: Heiko Eißfeldt
E: heiko@colossus.escape.de heiko@unifix.de
D: verify_area stuff, generic SCSI fixes
D: SCSI Programming HOWTO
D: POSIX.1 compliance testing
S: Unifix Software GmbH
S: Bueltenweg 27a
S: D-38106 Braunschweig
S: Germany

N: Bjorn Ekwall
E: bj0rn@blox.se
W: http://www.pi.se/blox/
D: Extended support for loadable modules
D: D-Link pocket adapter drivers
S: Brevia 1043
S: S-114 79 Stockholm
S: Sweden

N: Pekka Enberg
E: penberg@cs.helsinki.fi
W: http://www.cs.helsinki.fi/u/penberg/
D: Various kernel hacks, fixes, and cleanups.
D: Slab allocators
S: Finland

N: David Engebretsen
E: engebret@us.ibm.com
D: Linux port to 64-bit PowerPC architecture

N: Michael Engel
E: engel@unix-ag.org
D: DECstation framebuffer drivers
S: Germany

N: Paal-Kristian Engstad
E: engstad@intermetrics.com
D: Kernel smbfs (to mount WfW, NT and OS/2 network drives.)
S: 17101 Springdale Street #225
S: Huntington Beach, California 92649
S: USA

N: Stephane Eranian
E: eranian@hpl.hp.com
D: Linux/ia64
S: 1501 Page Mill Rd, MS 1U17
S: Palo Alto, CA 94304
S: USA

N: Johannes Erdfelt
E: johannes@erdfelt.com
D: Linux/IA-64 bootloader and kernel goop, USB
S: 6350 Stoneridge Mall Road
S: Pleasanton, CA 94588
S: USA

N: Dmitry Eremin-Solenikov
E: dbaryshkov@gmail.com
D: Power Supply Maintainer from v3.14 - v3.15

N: Doug Evans
E: dje@cygnus.com
D: Wrote Xenix FS (part of standard kernel since 0.99.15)

N: Riccardo Facchetti
E: fizban@tin.it
P: 1024/6E657BB5 AF 22 90 33 78 76 04 8B  AF F9 97 1E B5 E2 65 30
D: Audio Excel DSP 16 init driver author
D: libmodem author
D: Yet Another Micro Monitor port and current maintainer
D: First ELF-HOWTO author
D: random kernel hacker
S: Via Paolo VI n.29
S: 23900 - LECCO (Lc)
S: Italy

N: Nils Faerber
E: nils@kernelconcepts.de
D: i810 TCO watchdog driver author
D: Mitsumi LU005 tests and fixes
D: port and fixes of cs46xx sounddriver
S: Dreisbachstrasse 24
S: D-57250 Netphen
S: Germany

N: Rik Faith
E: faith@acm.org
D: Future Domain TMC-16x0 SCSI driver (author)
D: APM driver (early port)
D: DRM drivers (author of several)

N: Veaceslav Falico
E: vfalico@gmail.com
D: Co-maintainer and co-author of the network bonding driver.

N: János Farkas
E: chexum@shadow.banki.hu
D: romfs, various (mostly networking) fixes
P: 1024/F81FB2E1 41 B7 E4 E6 3E D4 A6 71  6D 9C F3 9F F2 BF DF 6E
S: Madarász Viktor utca 25
S: 1131 Budapest
S: Hungary

N: Ben Fennema
E: bfennema@falcon.csc.calpoly.edu
W: http://www.csc.calpoly.edu/~bfennema
D: UDF filesystem
S: (ask for current address)
S: USA

N: Jürgen Fischer
E: fischer@norbit.de
D: Author of Adaptec AHA-152x SCSI driver
S: Schulstraße 18
S: 26506 Norden
S: Germany

N: Jeremy Fitzhardinge
E: jeremy@goop.org
W: https://www.goop.org/~jeremy
D: author of userfs filesystem
D: Improved mmap and munmap handling
D: General mm minor tidyups
D: autofs v4 maintainer
D: Xen subsystem
S: 987 Alabama St
S: San Francisco
S: CA, 94110
S: USA

N: Ralf Flaxa
E: rfflaxa@immd4.informatik.uni-erlangen.de
D: The Linux Support Team Erlangen
D: Creator of LST distribution
D: Author of installation tool LISA
S: Pfitznerweg 6
S: 74523 Schwaebisch Hall
S: Germany

N: Lawrence Foard
E: entropy@world.std.com
D: Floppy track reading, fs code
S: 217 Park Avenue, Suite 108
S: Worcester, Massachusetts 01609
S: USA

N: Karl Fogel
E: kfogel@cs.oberlin.edu
D: Contributor, Linux User's Guide
S: 1123 North Oak Park Avenue
S: Oak Park, Illinois 60302
S: USA

N: Daniel J. Frasnelli
E: dfrasnel@alphalinux.org
W: http://www.alphalinux.org/
P: 1024/3EF87611 B9 F1 44 50 D3 E8 C2 80  DA E5 55 AA 56 7C 42 DA
D: DEC Alpha hacker
D: Miscellaneous bug squisher

N: Jim Freeman
E: jfree@sovereign.org
W: http://www.sovereign.org/
D: Initial GPL'd Frame Relay driver
D: Dynamic PPP devices
D: Sundry modularizations (PPP, IPX, ...) and fixes

N: Bob Frey
E: bobf@advansys.com
D: AdvanSys SCSI driver
S: 1150 Ringwood Court
S: San Jose, California 95131
S: USA

N: Adam Fritzler
E: mid@zigamorph.net

N: Richard ""Scuba"" A. Frowijn
E: scuba@wxs.nl
D: QNX4 filesystem

N: Fernando Fuganti
E: fuganti@conectiva.com.br
E: fuganti@netbank.com.br
D: random kernel hacker, ZF MachZ Watchdog driver
S: Conectiva S.A.
S: R. Tocantins, 89 - Cristo Rei
S: 80050-430 - Curitiba - Paraná
S: Brazil

N: Oded Gabbay
E: ogabbay@kernel.org
D: HabanaLabs maintainer
S: 29 Duchifat St.
S: Ra'anana 4372029
S: Israel

N: Kumar Gala
E: galak@kernel.crashing.org
D: Embedded PowerPC 6xx/7xx/74xx/82xx/83xx/85xx support
S: Austin, Texas 78729
S: USA

N: Nigel Gamble
E: nigel@nrg.org
D: Interrupt-driven printer driver
D: Preemptible kernel
S: 120 Alley Way
S: Mountain View, California 94040
S: USA

N: Jeff Garzik
E: jgarzik@pobox.com

N: Jacques Gelinas
E: jacques@solucorp.qc.ca
D: Author of the Umsdos file system
S: 1326 De Val-Brillant
S: Laval, Quebec
S: Canada H7Y 1V9

N: David Gentzel
E: gentzel@telerama.lm.com
D: Original BusLogic driver and original UltraStor driver
S: Whitfield Software Services
S: 600 North Bell Avenue, Suite 160
S: Carnegie, Pennsylvania 15106-4304
S: USA

N: Kai Germaschewski
E: kai@germaschewski.name
D: Major kbuild rework during the 2.5 cycle
D: ISDN Maintainer
S: USA

N: Gerrit Renker
E: gerrit@erg.abdn.ac.uk
D: DCCP protocol support.

N: Philip Gladstone
E: philip@gladstonefamily.net
D: Kernel / timekeeping stuff
S: Carlisle, MA 01741
S: USA

N: Jan-Benedict Glaw
E: jbglaw@lug-owl.de
D: SRM environment driver (for Alpha systems)
P: 1024D/8399E1BB 250D 3BCF 7127 0D8C A444  A961 1DBD 5E75 8399 E1BB

N: Thomas Gleixner
E: tglx@linutronix.de
D: NAND flash hardware support, JFFS2 on NAND flash

N: Richard E. Gooch
E: rgooch@atnf.csiro.au
D: parent process death signal to children
D: prctl() syscall
D: /proc/mtrr support to manipulate MTRRs on Intel P6 family
D: Device FileSystem (devfs)
S: CSIRO Australia Telescope National Facility
S: P.O. Box 76, Epping
S: New South Wales, 2121
S: Australia

N: Carlos E. Gorges
E: carlos@techlinux.com.br
D: fix smp support on cmpci driver
P: 2048G/EA3C4B19 FF31 33A6 0362 4915 B7EB  E541 17D0 0379 EA3C 4B19
S: Brazil

N: Dmitry S. Gorodchanin
E: pgmdsg@ibi.com
D: RISCom/8 driver, misc kernel fixes.
S: 4 Main Street
S: Woodbridge, Connecticut 06525
S: USA

N: Paul Gortmaker
E: p_gortmaker@yahoo.com
D: Author of RTC driver & several net drivers, Ethernet & BootPrompt Howto.
D: Made support for modules, ramdisk, generic-serial, etc. optional.
D: Transformed old user space bdflush into 1st kernel thread - kflushd.
D: Many other patches, documentation files, mini kernels, utilities, ...

N: Masanori GOTO
E: gotom@debian.or.jp
D: Workbit NinjaSCSI-32Bi/UDE driver
S: Japan

N: John E. Gotts
E: jgotts@linuxsavvy.com
D: kernel hacker
S: 8124 Constitution Apt. 7
S: Sterling Heights, Michigan 48313
S: USA

N: Wolfgang Grandegger
E: wg@grandegger.com
D: Controller Area Network (device drivers)

N: William Greathouse
E: wgreathouse@smva.com
E: wgreathouse@myfavoritei.com
D: Current Belkin USB Serial Adapter F5U103 hacker
D: Kernel hacker, embedded systems
S: 7802 Fitzwater Road
S: Brecksville, OH  44141-1334
S: USA

N: Tristan Greaves
E: tristan@extricate.org
W: http://www.extricate.org/
D: Miscellaneous ipv4 sysctl patches

N: Michael A. Griffith
E: grif@cs.ucr.edu
W: http://www.cs.ucr.edu/~grif
D: Loopback speedup, qlogic SCSI hacking, VT_LOCKSWITCH
S: Department of Computer Science
S: University of California, Riverside
S: Riverside, California 92521-0304
S: USA

N: Hans Grobler
E: grobh@sun.ac.za
D: Various AX.25/ROSE/NETROM + hamradio driver patches
D: Various X.25/LABP + driver patches
D: Misc kernel fixes and updates
S: Department of Electronic Engineering
S: University of Stellenbosch
S: Stellenbosch, Western Cape
S: South Africa

N: Grant Grundler
E: grantgrundler@gmail.com
W: http://obmouse.sourceforge.net/
W: http://www.parisc-linux.org/
D: obmouse - rewrote Olivier Florent's Omnibook 600 ""pop-up"" mouse driver
D: PA-RISC - Interrupt/PCI HBA/IOMMU author and architect
S: Mountain View, California
S: USA

N: Grant Guenther
E: grant@torque.net
W: http://www.torque.net/linux-pp.html
D: original author of ppa driver for parallel port ZIP drive
D: original architect of the parallel-port sharing scheme
D: PARIDE subsystem: drivers for parallel port IDE & ATAPI devices
S: 44 St. Joseph Street, Suite 506
S: Toronto, Ontario, M4Y 2W4
S: Canada

N: Richard Günther
E: rguenth@tat.physik.uni-tuebingen.de
W: http://www.tat.physik.uni-tuebingen.de/~rguenth
P: 2048/2E829319 2F 83 FC 93 E9 E4 19 E2  93 7A 32 42 45 37 23 57
D: binfmt_misc
S: 72074 Tübingen
S: Germany

N: Justin Guyett
E: jguyett@andrew.cmu.edu
D: via-rhine net driver hacking

N: Nitin Gupta
E: ngupta@vflare.org
D: zsmalloc memory allocator and zram block device driver

N: Danny ter Haar
E: dth@cistron.nl
D: /proc/cpuinfo, reboot on panic , kernel pre-patch tester ;)
S: Cistron
S: PO-Box 297
S: 2400 AG, Alphen aan den Rijn
S: The Netherlands

N: Enver Haase
E: ehaase@inf.fu-berlin.de
W: http://www.inf.fu-berlin.de/~ehaase
D: Driver for the Commodore A2232 serial board

N: Bruno Haible
E: haible@ma2s2.mathematik.uni-karlsruhe.de
D: SysV FS, shm swapping, memory management fixes
S: 17 rue Danton
S: F - 94270 Le Kremlin-Bicêtre
S: France

N: Jack Hammer
D: IBM ServeRAID RAID (ips) driver maintenance

N: Greg Hankins
E: gregh@cc.gatech.edu
D: fixed keyboard driver to separate LED and locking status
S: 25360 Georgia Tech Station
S: Atlanta, Georgia 30332
S: USA

N: Brad Hards
E: bradh@frogmouth.net
D: Various USB bits, other minor patches

N: Angelo Haritsis
E: ah@computer.org
D: kernel patches (serial, watchdog)
D: xringd, vuzkern, greekXfonts
S: 77 Clarence Mews
S: London SE16 1GD
S: United Kingdom

N: Jan Harkes
E: jaharkes@cs.cmu.edu
W: http://www.coda.cs.cmu.edu/
D: Coda file system
S: Computer Science Department
S: Carnegie Mellon University
S: 5000 Forbes Avenue
S: Pittsburgh, Pennsylvania 15213
S: USA

N: Kai Harrekilde-Petersen
E: kai.harrekilde@get2net.dk
D: Original author of the ftape-HOWTO, i82078 fdc detection code.

N: Bart Hartgers
E: bart@etpmod.phys.tue.nl
D: MTRR emulation with Centaur MCRs
S: Gen Stedmanstraat 212
S: 5623 HZ Eindhoven
S: The Netherlands

N: Oliver Hartkopp
E: oliver.hartkopp@volkswagen.de
W: https://www.volkswagen.de
D: Controller Area Network (network layer core)
S: Brieffach 1776
S: 38436 Wolfsburg
S: Germany

N: Andrew Haylett
E: ajh@primag.co.uk
D: Selection mechanism

N: Andre Hedrick
E: andre@linux-ide.org
E: andre@linuxdiskcert.org
W: http://www.linux-ide.org/
W: http://www.linuxdiskcert.org/
D: Random SMP kernel hacker...
D: Uniform Multi-Platform E-IDE driver
D: Active-ATA-Chipset maddness..........
D: Ultra DMA 133/100/66/33 w/48-bit Addressing
D: ATA-Disconnect, ATA-TCQ
D: ATA-Smart Kernel Daemon
D: Serial ATA
D: ATA Command Block and Taskfile
S: Linux ATA Development (LAD)
S: Concord, CA

N: Jochen Hein
E: jochen@jochen.org
P: 1024/4A27F015 25 72 FB E3 85 9F DE 3B  CB 0A DA DA 40 77 05 6C
P: 1024D/77D4FC9B F5C5 1C20 1DFC DEC3 3107  54A4 2332 ADFC 77D4 FC9B
D: National Language Support
D: Linux Internationalization Project
D: German Localization for Linux and GNU software
S: Auf der Fittel 18
S: 53347 Alfter
S: Germany

N: Christoph Hellwig
E: hch@infradead.org
D: all kinds of driver, filesystem & core kernel hacking
D: freevxfs driver
D: sysvfs maintainer
D: chief codingstyle nitpicker
S: Ampferstr. 50 / 4
S: 6020 Innsbruck
S: Austria

N: Richard Henderson
E: rth@twiddle.net
E: rth@cygnus.com
D: Alpha hacker, kernel and userland
S: 1668 California St.
S: Mountain View, California 94041
S: USA

N: Benjamin Herrenschmidt
E: benh@kernel.crashing.org
D: Various parts of PPC/PPC64 & PowerMac
S: 312/107 Canberra Avenue
S: Griffith, ACT 2603
S: Australia

N: Andreas Herrmann
E: herrmann.der.user@gmail.com
E: herrmann.der.user@googlemail.com
D: Key developer of x86/AMD64
D: Author of AMD family 15h processor power monitoring driver
D: Maintainer of AMD Athlon 64 and Opteron processor frequency driver
S: Germany

N: Sebastian Hetze
E: she@lunetix.de
D: German Linux Documentation,
D: Organization of German Linux Conferences
S: Danckelmannstr. 48
S: 14059 Berlin
S: Germany

N: David Hinds
E: dahinds@users.sourceforge.net
W: http://tao.stanford.edu/~dhinds
D: PCMCIA and CardBus stuff, PCMCIA-HOWTO, PCMCIA client drivers
S: 2019 W. Middlefield Rd #1
S: Mountain View, CA  94043
S: USA

N: Michael Hipp
E: hippm@informatik.uni-tuebingen.de
D: drivers for the racal ni5210 & ni6510 Ethernet-boards
S: Talstr. 1
S: D - 72072 Tuebingen
S: Germany

N: Richard Hirst
E: richard@sleepie.demon.co.uk
E: rhirst@linuxcare.com
W: http://www.sleepie.demon.co.uk/
D: linux-m68k VME support
D: PA-RISC port, scsi and network drivers
D: 53c700/53c710 driver author, 82596 driver maintainer
S: United Kingdom

N: Jauder Ho
E: jauderho@carumba.com
W: http://www.carumba.com/
D: bug toaster (A1 sauce makes all the difference)
D: Random linux hacker

N: James Hogan
E: jhogan@kernel.org
D: Metag architecture maintainer
D: TZ1090 SoC maintainer

N: Tim Hockin
E: thockin@hockin.org
W: http://www.hockin.org/~thockin
D: Natsemi ethernet
D: Cobalt Networks (x86) support
D: This-and-That

N: Mark M. Hoffman
E: mhoffman@lightlink.com
D: asb100, lm93 and smsc47b397 hardware monitoring drivers
D: hwmon subsystem core
D: hwmon subsystem maintainer
D: i2c-sis96x and i2c-stub SMBus drivers
S: USA

N: Dirk Hohndel
E: hohndel@suse.de
D: The XFree86[tm] Project
D: USB mouse maintainer
S: SuSE Rhein/Main AG
S: Mergenthalerallee 45-47
S: 65760 Eschborn
S: Germany

N: Kenji Hollis
E: kenji@bitgate.com
W: https://www.bitgate.com/
D: Berkshire PC Watchdog Driver
D: Small/Industrial Driver Project

N: Nick Holloway
E: Nick.Holloway@pyrites.org.uk
W: https://www.pyrites.org.uk/
P: 1024/36115A04 F4E1 3384 FCFD C055 15D6  BA4C AB03 FBF8 3611 5A04
D: Occasional Linux hacker...
S: (ask for current address)
S: United Kingdom

N: Ron Holt
E: ron@holt.org
E: rholt@netcom.com
W: http://www.holt.org/
W: http://www.ronholt.com/
D: Kernel development
D: Kernel LDT modifications to support Wabi and Wine
S: Holtron Internetics, Inc.
S: 998 East 900 South, Suite 26
S: Provo, Utah 84606-5607
S: USA

N: Marcel Holtmann
E: marcel@holtmann.org
W: http://www.holtmann.org
D: Maintainer of the Linux Bluetooth Subsystem
D: Author and maintainer of the various Bluetooth HCI drivers
D: Author and maintainer of the CAPI message transport protocol driver
D: Author and maintainer of the Bluetooth HID protocol driver
D: Various other Bluetooth related patches, cleanups and fixes
S: Germany

N: Rob W. W. Hooft
E: hooft@EMBL-Heidelberg.DE
D: Shared libs for graphics-tools and for the f2c compiler
D: Some kernel programming on the floppy and sound drivers in early days
D: Some other hacks to get different kinds of programs to work for linux
S: Panoramastrasse 18
S: D-69126 Heidelberg
S: Germany

N: Neil Horman
M: nhorman@tuxdriver.com
D: SCTP protocol maintainer.

N: Simon Horman
M: horms@verge.net.au
D: Renesas ARM/ARM64 SoC maintainer

N: Christopher Horn
E: chorn@warwick.net
D: Miscellaneous sysctl hacks
S: 36 Mudtown Road
S: Wantage, New Jersey 07461
S: USA

N: Harald Hoyer
E: harald@redhat.com
W: https://www.harald-hoyer.de
D: ip_masq_quake
D: md boot support
S: Am Strand 5
S: D-19063 Schwerin
S: Germany

N: Jan Hubicka
E: hubicka@freesoft.cz
E: hubicka@suse.cz
W: http://www.paru.cas.cz/~hubicka/
D: Random kernel tweaks and fixes.
S: Dukelskych bojovniku 1944
S: Tabor 390 03
S: Czech Republic

N: David Huggins-Daines
E: dhd@debian.org
E: dhd@eradicator.org
E: dhd@cepstral.com
D: PA-RISC port
D: Nubus subsystem
D: Generic 68k Macintosh framebuffer driver
D: STI framebuffer tweaks
D: LTPC driver tweaks
S: 110 S. 12th St., Apt. A
S: Pittsburgh, PA 15203-1250
S: USA

N: Gareth Hughes
E: gareth.hughes@acm.org
D: Pentium III FXSR, SSE support
D: Author/maintainer of most DRM drivers (especially ATI, MGA)
D: Core DRM templates, general DRM and 3D-related hacking
S: No fixed address

N: Kenn Humborg
E: kenn@wombat.ie
D: Mods to loop device to support sparse backing files
S: Ballinagard
S: Roscommon
S: Ireland

N: Michael Hunold
E: michael@mihu.de
W: http://www.mihu.de/linux/
D: Generic saa7146 video4linux-2 driver core,
D: Driver for the ""Multimedia eXtension Board"", ""dpc7146"",
D: ""Hexium Orion"", ""Hexium Gemini""

N: Miguel de Icaza Amozurrutia
E: miguel@nuclecu.unam.mx
D: Linux/SPARC team, Midnight Commander maintainer
S: Avenida Copilco 162, 22-1003
S: Mexico, DF
S: Mexico

N: Ian Jackson
E: iwj10@cus.cam.ac.uk
E: ijackson@nyx.cs.du.edu
D: FAQ maintainer and poster of the daily postings
D: FSSTND group member
D: Debian core team member and maintainer of several Debian packages
S: 2 Lexington Close
S: Cambridge
S: CB3 0DS
S: United Kingdom

N: Andreas Jaeger
E: aj@suse.de
D: Various smaller kernel fixes
D: glibc developer
S: Gottfried-Kinkel-Str. 18
S: D 67659 Kaiserslautern
S: Germany

N: Mike Jagdis
E: jaggy@purplet.demon.co.uk
E: Mike.Jagdis@purplet.demon.co.uk
D: iBCS personalities, socket and X interfaces, x.out loader, syscalls...
D: Purple Distribution maintainer
D: UK FidoNet support
D: ISODE && PP
D: Kernel and device driver hacking
S: 280 Silverdale Road
S: Earley
S: Reading
S: RG6 2NU
S: United Kingdom

N: Dave Jeffery
E: dhjeffery@gmail.com
D: SCSI hacks and IBM ServeRAID RAID driver maintenance

N: Jakub Jelinek
E: jakub@redhat.com
W: http://sunsite.mff.cuni.cz/~jj
P: 1024/0F7623C5 53 95 71 3C EB 73 99 97  02 49 40 47 F9 19 68 20
D: Sparc hacker, SILO, mc
D: Maintain sunsite.mff.cuni.cz
S: K osmidomkum 723
S: 160 00 Praha 6
S: Czech Republic

N: Niels Kristian Bech Jensen
E: nkbj1970@hotmail.com
D: Miscellaneous kernel updates and fixes.

N: Michael K. Johnson
E: johnsonm@redhat.com
W: http://www.redhat.com/~johnsonm
P: 1024/4536A8DD 2A EC 88 08 40 64 CE D8  DD F8 12 2B 61 43 83 15
D: The Linux Documentation Project
D: Kernel Hackers' Guide
D: Procps
D: Proc filesystem
D: Maintain tsx-11.mit.edu
D: LP driver
S: 201 Howell Street, Apartment 1C
S: Chapel Hill, North Carolina 27514-4818
S: USA

N: Dave Jones
E: davej@codemonkey.org.uk
D: Assorted VIA x86 support.
D: 2.5 AGPGART overhaul.
D: CPUFREQ maintenance.
D: Fedora kernel maintenance (2003-2014).
D: 'Trinity' and similar fuzz testing work.
D: Misc/Other.

N: Martin Josfsson
E: gandalf@wlug.westbo.se
P: 1024D/F6B6D3B1 7610 7CED 5C34 4AA6 DBA2  8BE1 5A6D AF95 F6B6 D3B1
D: netfilter: SAME target
D: netfilter: helper target
D: netfilter: various other hacks
S: Ronneby
S: Sweden

N: Ani Joshi
E: ajoshi@shell.unixbox.com
D: fbdev hacking

N: Jesper Juhl
E: jesperjuhl76@gmail.com
D: Various fixes, cleanups and minor features all over the tree.
D: Wrote initial version of the hdaps driver (since passed on to others).
S: Titangade 5G, 2.tv
S: 2200 Copenhagen N.
S: Denmark

N: Jozsef Kadlecsik
E: kadlec@netfilter.org
P: 1024D/470DB964 4CB3 1A05 713E 9BF7 FAC5  5809 DD8C B7B1 470D B964
D: netfilter: TCP window tracking code
D: netfilter: raw table
D: netfilter: iprange match
D: netfilter: new logging interfaces
D: netfilter: various other hacks
S: Tata
S: Hungary

N: Bernhard Kaindl
E: bkaindl@netway.at
E: edv@bartelt.via.at
D: Author of a menu based configuration tool, kmenu, which
D: is the predecessor of 'make menuconfig' and 'make xconfig'.
D: digiboard driver update(modularisation work and 2.1.x upd)
S: Tallak 95
S: 8103 Rein
S: Austria

N: Mitsuru Kanda
E: mk@linux-ipv6.org
E: mk@isl.rdc.toshiba.co.jp
E: mk@karaba.org
W: http://www.karaba.org/~mk/
P: 1024D/2EC7E30D 4DC3 949B 5A6C F0D6 375F  4472 8888 A8E1 2EC7 E30D
D: IPsec, IPv6
D: USAGI/WIDE Project, TOSHIBA CORPORATION
S: 2-47-8, Takinogawa,
S: Kita, Tokyo 114-0023
S: Japan

N: Jan Kara
E: jack@atrey.karlin.mff.cuni.cz
E: jack@suse.cz
D: Quota fixes for 2.2 kernel
D: Quota fixes for 2.3 kernel
D: Few other fixes in filesystem area (buffer cache, isofs, loopback)
W: http://atrey.karlin.mff.cuni.cz/~jack/
S: Krosenska' 543
S: 181 00 Praha 8
S: Czech Republic

N: Murali Karicheri
E: m-karicheri2@ti.com
D: Keystone NetCP driver
D: Keystone PCIe host controller driver

N: Jan ""Yenya"" Kasprzak
E: kas@fi.muni.cz
D: Author of the COSA/SRP sync serial board driver.
D: Port of the syncppp.c from the 2.0 to the 2.1 kernel.
P: 1024/D3498839 0D 99 A7 FB 20 66 05 D7  8B 35 FC DE 05 B1 8A 5E
W: https://www.fi.muni.cz/~kas/
S: c/o Faculty of Informatics, Masaryk University
S: Botanicka' 68a
S: 602 00 Brno
S: Czech Republic

N: Jakob Kemi
E: jakob.kemi@telia.com
D: V4L W9966 Webcam driver
S: Forsbyvägen 33
S: 74143 Knivsta
S: Sweden

N: Fred N. van Kempen
E: waltje@linux.com
D: NET-2
D: Drivers
D: Kernel cleanups
S: Korte Heul 95
S: 1403 ND  BUSSUM
S: The Netherlands

N: Martin Kepplinger
E: martink@posteo.de
E: martin.kepplinger@puri.sm
W: http://www.martinkepplinger.com
P: 4096R/5AB387D3 F208 2B88 0F9E 4239 3468  6E3F 5003 98DF 5AB3 87D3
D: mma8452 accelerators iio driver
D: pegasus_notetaker input driver
D: Kernel fixes and cleanups
S: Garnisonstraße 26
S: 4020 Linz
S: Austria

N: Karl Keyte
E: karl@koft.com
D: Disk usage statistics and modifications to line printer driver
S: 26a Sheen Road
S: Richmond
S: Surrey
S: TW9 1AE
S: United Kingdom

N: Marko Kiiskila
E: marko@iprg.nokia.com
D: Author of ATM Lan Emulation
S: 660 Harvard Ave. #7
S: Santa Clara, CA 95051
S: USA

N: Kukjin Kim
E: kgene@kernel.org
D: Samsung S3C, S5P and Exynos ARM architectures

N: Milo Kim
D: TI LP855x, LP8727 and LP8788 drivers

N: Sangbeom Kim
E: sbkim73@samsung.com
D: Samsung SoC Audio (ASoC) drivers
D: Samsung PMIC (RTC, regulators, MFD) drivers

N: Russell King
E: rmk@arm.linux.org.uk
D: Linux/arm integrator, maintainer & hacker
D: Acornfb, Cyber2000fb author
S: Burgh Heath, Tadworth, Surrey.
S: England

N: Olaf Kirch
E: okir@monad.swb.de
D: Author of the Linux Network Administrators' Guide
S: Kattreinstr 38
S: D-64295
S: Germany

N: Avi Kivity
E: avi.kivity@gmail.com
D: Kernel-based Virtual Machine (KVM)
S: Ra'annana, Israel

N: Andi Kleen
E: andi@firstfloor.org
W: http://www.halobates.de
D: network, x86, NUMA, various hacks
S: Schwalbenstr. 96
S: 85551 Ottobrunn
S: Germany

N: Ian Kluft
E: ikluft@thunder.sbay.org
W: http://www.kluft.com/~ikluft/
D: NET-1 beta testing & minor patches, original Smail binary packages for
D: Slackware and Debian, vote-taker for 2nd comp.os.linux reorganization
S: Post Office Box 611311
S: San Jose, California 95161-1311
S: USA

N: Hartmut Knaack
E: knaack.h@gmx.de
D: IIO subsystem and drivers

N: Thorsten Knabe
E: Thorsten Knabe <tek@rbg.informatik.tu-darmstadt.de>
E: Thorsten Knabe <tek01@hrzpub.tu-darmstadt.de>
W: http://www.student.informatik.tu-darmstadt.de/~tek
W: http://www.tu-darmstadt.de/~tek01
P: 1024/3BC8D885 8C 29 C5 0A C0 D1 D6 F4  20 D4 2D AB 29 F6 D0 60
D: AD1816 sound driver
S: Am Bergfried 10
S: 63225 Langen
S: Germany

N: Alain L. Knaff
E: Alain.Knaff@lll.lu
D: floppy driver
S: 19, rue Jean l'Aveugle
S: L-1148 Luxembourg-City
S: Luxembourg

N: Gerd Knorr
W: http://bytesex.org
E: kraxel@bytesex.org
E: kraxel@suse.de
D: video4linux, bttv, vesafb, some scsi, misc fixes

N: Hans J. Koch
D: USERSPACE I/O, MAX6650
D: Hans passed away in June 2016, and will be greatly missed.
W: https://lwn.net/Articles/691000/

N: Harald Koenig
E: koenig@tat.physik.uni-tuebingen.de
D: XFree86 (S3), DCF77, some kernel hacks and fixes
S: Koenigsberger Str. 90
S: D-72336 Balingen
S: Germany

N: Rudolf Koenig
E: rfkoenig@immd4.informatik.uni-erlangen.de
D: The Linux Support Team Erlangen

N: Andreas Koensgen
E: ajk@comnets.uni-bremen.de
D: 6pack driver for AX.25

N: Harald Koerfgen
E: hkoerfg@web.de
D: Linux/MIPS kernel hacks and fixes,
D: DECstation port, Sharp Mobilon port
S: D-50931 Koeln
S: Germany

N: Willy Konynenberg
E: willy@xos.nl
W: http://www.xos.nl/
D: IP transparent proxy support
S: X/OS Experts in Open Systems BV
S: Kruislaan 419
S: 1098 VA Amsterdam
S: The Netherlands

N: Goran Koruga
E: korugag@siol.net
D: cpufrequtils (precursor to cpupowerutils)
S: Slovenia

N: Jiri Kosina
E: jikos@jikos.cz
E: jkosina@suse.cz
D: Generic HID layer - original code split, fixes
D: Various ACPI fixes, keeping correct battery state through suspend
D: various lockdep annotations, autofs and other random bugfixes
S: Prague, Czech Republic

N: Gene Kozin
E: 74604.152@compuserve.com
W: https://www.sangoma.com
D: WAN Router & Sangoma WAN drivers
S: Sangoma Technologies Inc.
S: 7170 Warden Avenue, Unit 2
S: Markham, Ontario
S: L3R 8B2
S: Canada

N: Maxim Krasnyansky
E: maxk@qualcomm.com
W: http://vtun.sf.net
W: http://bluez.sf.net
D: Author of the Universal TUN/TAP driver
D: Author of the Linux Bluetooth Subsystem (BlueZ)
D: Various other kernel patches, cleanups and fixes
S: 2213 La Terrace Circle
S: San Jose, CA 95123
S: USA

N: Andreas S. Krebs
E: akrebs@altavista.net
D: CYPRESS CY82C693 chipset IDE, Digital's PC-Alpha 164SX boards

N: Greg Kroah-Hartman
E: greg@kroah.com
E: gregkh@suse.de
W: http://www.kroah.com/linux/
D: USB Serial Converter driver framework, USB Handspring Visor driver
D: ConnectTech WHITEHeat USB driver, Generic USB Serial driver
D: USB I/O Edgeport driver, USB Serial IrDA driver
D: USB Bluetooth driver, USB Skeleton driver
D: bits and pieces of USB core code.
D: PCI Hotplug core, PCI Hotplug Compaq driver modifications
D: portions of the Linux Security Module (LSM) framework
D: parts of the driver core, debugfs.

N: Russell Kroll
E: rkroll@exploits.org
W: http://www.exploits.org/
D: V4L radio cards: radio-aztech (new), others (bugfixes/features)
D: Loopback block device: dynamic sizing (""max_loop"" as module)
S: Post Office Box 691886
S: San Antonio, Texas 78269-1886
S: USA

N: Denis O. Kropp
E: dok@directfb.org
D: NeoMagic framebuffer driver
S: Badensche Str. 46
S: 10715 Berlin
S: Germany

N: Andrzej M. Krzysztofowicz
E: ankry@mif.pg.gda.pl
D: Some 8-bit XT disk driver and devfs hacking
D: Aladdin 1533/1543(C) chipset IDE
D: PIIX chipset IDE
S: ul. Matemblewska 1B/10
S: 80-283 Gdansk
S: Poland

N: Gero Kuhlmann
E: gero@gkminix.han.de
D: mounting root via NFS
S: Donarweg 4
S: D-30657 Hannover
S: Germany

N: Markus Kuhn
E: mskuhn@cip.informatik.uni-erlangen.de
W: http://wwwcip.informatik.uni-erlangen.de/user/mskuhn
D: Unicode, real-time, time, standards
S: Schlehenweg 9
S: D-91080 Uttenreuth
S: Germany

N: Jaya Kumar
E: jayalk@intworks.biz
W: http://www.intworks.biz
D: Arc monochrome LCD framebuffer driver, x86 reboot fixups
D: pirq addr, CS5535 alsa audio driver
S: Gurgaon, India
S: Kuala Lumpur, Malaysia

N: Mohit Kumar
D: ST Microelectronics SPEAr13xx PCI host bridge driver
D: Synopsys DesignWare PCI host bridge driver

N: Gabor Kuti
E: seasons@falcon.sch.bme.hu
E: seasons@makosteszta.sote.hu
D: Original author of software suspend

N: Alexey Kuznetsov
E: kuznet@ms2.inr.ac.ru
D: Author and maintainer of large parts of the networking stack

N: Jaroslav Kysela
E: perex@perex.cz
W: https://www.perex.cz
D: Original Author and Maintainer for HP 10/100 Mbit Network Adapters
D: ISA PnP
S: Sindlovy Dvory 117
S: 370 01  Ceske Budejovice
S: Czech Republic

N: Bas Laarhoven
E: sjml@xs4all.nl
D: Loadable modules and ftape driver
S: J. Obrechtstr 23
S: NL-5216 GP 's-Hertogenbosch
S: The Netherlands

N: Ashley Lai
E: ashleydlai@gmail.com
D: IBM VTPM driver

N: Savio Lam
E: lam836@cs.cuhk.hk
D: Author of the dialog utility, foundation
D: for Menuconfig's lxdialog.

N: Christoph Lameter
E: christoph@lameter.com
D: Digiboard PC/Xe and PC/Xi, Digiboard EPCA
D: NUMA support, Slab allocators, Page migration
D: Scalability, Time subsystem

N: Anders Larsen
E: al@alarsen.net
D: QNX4 filesystem

N: Paul Laufer
E: paul@laufernet.com
D: Soundblaster driver fixes, ISAPnP quirk
S: California, USA

N: Jarkko Lavinen
E: jarkko.lavinen@nokia.com
D: OMAP MMC support

N: Jonathan Layes
D: ARPD support

N: Tom Lees
E: tom@lpsg.demon.co.uk
W: http://www.lpsg.demon.co.uk/
P: 1024/87D4D065 2A 66 86 9D 02 4D A6 1E  B8 A2 17 9D 4F 9B 89 D6
D: Original author and current maintainer of
D: PnP code.

N: David van Leeuwen
E: david@tm.tno.nl
D: Philips/LMS cm206 cdrom driver, generic cdrom driver
S: Scheltemalaan 14
S: 3817 KS Amersfoort
S: The Netherlands

N: Volker Lendecke
E: vl@kki.org
D: Kernel smbfs (to mount WfW, NT and OS/2 network drives.)
D: NCP filesystem support (to mount NetWare volumes)
S: Von-Ossietzky-Str. 12
S: 37085 Göttingen
S: Germany

N: Kevin Lentin
E: kevinl@cs.monash.edu.au
D: NCR53C400/T130B SCSI extension to NCR5380 driver.
S: 18 Board Street
S: Doncaster VIC 3108
S: Australia

N: Hans Lermen
E: lermen@elserv.ffm.fgan.de
D: Author of the LOADLIN Linux loader, hacking on boot stuff
D: Coordinator of DOSEMU releases
S: Am Muehlenweg 38
S: D53424 Remagen
S: Germany

N: Colin Leroy
E: colin@colino.net
W: http://www.geekounet.org/
D: PowerMac adt746x fan driver
D: Random fixing of various drivers (macintosh, usb, sound)
S: Toulouse
S: France

N: Achim Leubner
E: achim_leubner@adaptec.com
D: GDT Disk Array Controller/Storage RAID controller driver
S: ICP vortex GmbH
S: Neckarsulm
S: Germany

N: Phil Lewis
E: beans@bucket.ualr.edu
D: Promised to send money if I would put his name in the source tree.
S: Post Office Box 371
S: North Little Rock, Arkansas 72115
S: USA

N: Christopher Li
E: sparse@chrisli.org
D: Sparse maintainer 2009 - 2018

N: Shaohua Li
D: Worked on many parts of the kernel, from core x86, ACPI, PCI, KVM, MM,
D: and much more. He was the maintainer of MD from 2016 to 2018. Shaohua
D: passed away late 2018, he will be greatly missed.
W: https://www.spinics.net/lists/raid/msg61993.html

N: Stephan Linz
E: linz@mazet.de
E: Stephan.Linz@gmx.de
W: http://www.crosswinds.net/~tuxer
D: PCILynx patch to work with 1394a PHY and without local RAM
S: (ask for current address)
S: Germany

N: Christophe Lizzi
E: lizzi@cnam.fr
W: http://cedric.cnam.fr/personne/lizzi
D: FORE Systems 200E-series ATM network driver, sparc64 port of ATM
S: CNAM, Laboratoire CEDRIC
S: 292, rue St-Martin
S: 75141 Paris Cedex 03
S: France

N: Siegfried ""Frieder"" Loeffler (dg1sek)
E: floeff@tunix.mathematik.uni-stuttgart.de, fl@LF.net
W: http://www.mathematik.uni-stuttgart.de/~floeff
D: Busmaster driver for HP 10/100 Mbit Network Adapters
S: University of Stuttgart, Germany and
S: Ecole Nationale Superieure des Telecommunications, Paris
S: France

N: Jamie Lokier
E: jamie@shareable.org
W: http://www.shareable.org/
D: Reboot-through-BIOS for broken 486 motherboards
D: Parport fixes, futex improvements
D: First instruction of x86 sysenter path :)
S: 51 Sunningwell Road
S: Oxford
S: OX1 4SZ
S: United Kingdom

N: Mark Lord
E: mlord@pobox.com
D: EIDE driver, hd.c support
D: EIDE PCI and bus-master DMA support
D: Hard Disk Parameter (hdparm) utility
S: 33 Ridgefield Cr
S: Nepean, Ontario
S: Canada K2H 6S3

N: Warner Losh
E: imp@village.org
D: Linux/MIPS Deskstation support, Provided OI/OB for Linux
S: 8786 Niwot Road
S: Niwot, Colorado 80503
S: USA

N: Robert M. Love
E: rml@tech9.net
E: rml@novell.com
D: misc. kernel hacking and debugging
S: Cambridge, MA 02139
S: USA

N: Martin von Löwis
E: loewis@informatik.hu-berlin.de
D: script binary format
D: NTFS driver

N: H.J. Lu
E: hjl@gnu.ai.mit.edu
D: GCC + libraries hacker

N: Yanir Lubetkin
E: yanirx.lubatkin@intel.com
E: linux-wimax@intel.com
D: Intel Wireless WiMAX Connection 2400 driver

N: Michal Ludvig
E: michal@logix.cz
E: michal.ludvig@asterisk.co.nz
W: http://www.logix.cz/michal
P: 1024D/C45B2218 1162 6471 D391 76E0 9F99  29DA 0C3A 2509 C45B 2218
D: VIA PadLock driver
D: Netfilter pkttype module
S: Asterisk Ltd.
S: Auckland
S: New Zealand

N: Tuomas J. Lukka
E: Tuomas.Lukka@Helsinki.FI
D: Original dual-monitor patches
D: Console-mouse-tracking patches
S: Puistokaari 1 E 18
S: 00200 Helsinki
S: Finland

N: Daniel J. Maas
E: dmaas@dcine.com
W: https://www.maasdigital.com
D: dv1394

N: Hamish Macdonald
E: hamishm@lucent.com
D: Linux/68k port
S: 32 Clydesdale Avenue
S: Kanata, Ontario
S: Canada K2M-2G7

N: Peter MacDonald
D: SLS distribution
D: Initial implementation of VC's, pty's and select()

N: Pavel Machek
E: pavel@ucw.cz
P: 4096R/92DFCE96 4FA7 9EEF FCD4 C44F C585  B8C7 C060 2241 92DF CE96
D: Softcursor for vga, hypertech cdrom support, vcsa bugfix, nbd,
D: sun4/330 port, capabilities for elf, speedup for rm on ext2, USB,
D: work on suspend-to-ram/disk, killing duplicates from ioctl32,
D: Altera SoCFPGA and Nokia N900 support.
S: Czech Republic

N: Paul Mackerras
E: paulus@samba.org
D: PPP driver
D: Linux for PowerPC
D: Linux port for PCI Power Macintosh

N: Pat Mackinlay
E: pat@it.com.au
D: 8 bit XT hard disk driver
D: Miscellaneous ST0x, TMC-8xx and other SCSI hacking
S: 25 McMillan Street
S: Victoria Park 6100
S: Australia

N: James B. MacLean
E: macleajb@ednet.ns.ca
W: http://www.ednet.ns.ca/~macleajb/dosemu.html
D: Former Coordinator of DOSEMU releases
D: Program in DOSEMU
S: PO BOX 220, HFX. CENTRAL
S: Halifax, Nova Scotia
S: Canada B3J 3C8

N: Kai Mäkisara
E: Kai.Makisara@kolumbus.fi
D: SCSI Tape Driver

N: Asit Mallick
E: asit.k.mallick@intel.com
D: Linux/IA-64
S: 2200 Mission College Blvd
S: Santa Clara, CA 95052
S: USA

N: Petko Manolov
E: petkan@users.sourceforge.net
D: USB ethernet pegasus/pegasus-II driver
D: USB ethernet rtl8150 driver
D: optimizing i[45]86 string routines
D: i386 task switching hacks
S: 482 Shadowgraph Dr.
S: San Jose, CA  95110
S: USA

N: Michal Marek
E: michal.lkml@markovi.net
D: Kbuild Maintainer 2009-2017

N: Martin Mares
E: mj@ucw.cz
W: http://www.ucw.cz/~mj/
D: BIOS video mode handling code
D: MOXA C-218 serial board driver
D: Network autoconfiguration
D: PCI subsystem
D: Random kernel hacking
S: Kankovskeho 1241
S: 182 00 Praha 8
S: Czech Republic

N: John A. Martin
E: jam@acm.org
W: http://www.tux.org/~jam/
P: 1024/04456D53 9D A3 6C 6B 88 80 8A 61  D7 06 22 4F 95 40 CE D2
P: 1024/3B986635 5A61 7EE6 9E20 51FB 59FB  2DA5 3E18 DD55 3B98 6635
D: FSSTND contributor
D: Credit file compilator

N: Kevin E. Martin
E: martin@cs.unc.edu
D: Developed original accelerated X servers included in XFree86
D: XF86_Mach64
D: XF86_Mach32
D: XF86_Mach8
D: XF86_8514
D: cfdisk (curses based disk partitioning program)

N: Mat Martineau
E: martineau@kernel.org
D: MPTCP subsystem co-maintainer
D: Keyctl restricted keyring and Diffie-Hellman UAPI
D: Bluetooth L2CAP ERTM mode and AMP
S: USA

N: John S. Marvin
E: jsm@fc.hp.com
D: PA-RISC port
S: Hewlett Packard
S: MS 42
S: 3404 E. Harmony Road
S: Fort Collins, CO 80528
S: USA

N: Torben Mathiasen
E: torben.mathiasen@compaq.com
E: torben@kernel.dk
W: http://tlan.kernel.dk
D: ThunderLAN maintainer
D: ThunderLAN updates and other kernel fixes.
S: Bremensgade 29, st.th
S: 2300 Copenhagen S
S: Denmark

N: Claudio S. Matsuoka
E: cmatsuoka@gmail.com
E: claudio@mandriva.com
W: http://helllabs.org/~claudio
D: V4L, OV511 and HDA-codec hacks
S: Conectiva S.A.
S: Souza Naves 1250
S: 80050-040  Curitiba PR
S: Brazil

N: Heinz Mauelshagen
E: mge@EZ-Darmstadt.Telekom.de
D: Logical Volume Manager
S: Bartningstr. 12
S: 64289 Darmstadt
S: Germany

N: Mark W. McClelland
E: mmcclell@bigfoot.com
E: mark@alpha.dyndns.org
W: http://alpha.dyndns.org/ov511/
P: 1024D/357375CC 317C 58AC 1B39 2AB0 AB96  EB38 0B6F 731F 3573 75CC
D: OV511 driver
S: (address available on request)
S: USA

N: Ian McDonald
E: ian.mcdonald@jandi.co.nz
E: imcdnzl@gmail.com
W: http://wand.net.nz/~iam4
W: http://imcdnzl.blogspot.com
D: DCCP, CCID3
S: Hamilton
S: New Zealand

N: Patrick McHardy
E: kaber@trash.net
P: 1024D/12155E80 B128 7DE6 FF0A C2B2 48BE  AB4C C9D4 964E 1215 5E80
D: netfilter: endless number of bugfixes
D: netfilter: CLASSIFY target
D: netfilter: addrtype match
D: tc: HFSC scheduler
S: Freiburg
S: Germany

N: Paul E. McKenney
E: paulmck@us.ibm.com
W: http://www.rdrop.com/users/paulmck/
D: RCU and variants
D: rcutorture module

N: Bradley McLean
E: brad@bradpc.gaylord.com
D: Device driver hacker
D: General kernel debugger
S: 249 Nichols Avenue
S: Syracuse, New York 13206
S: USA

N: Kyle McMartin
E: kyle@mcmartin.ca
D: Linux/PARISC hacker
D: AD1889 sound driver
S: Ottawa, Canada

N: Peter Meerwald-Stadler
E: pmeerw@pmeerw.net
W: https://pmeerw.net
D: IIO reviewing, drivers
S: Schießstandstr. 3a
S: A-5061 Elsbethen
S: Austria

N: Dirk Melchers
E: dirk@merlin.nbg.sub.org
D: 8 bit XT hard disk driver for OMTI5520
S: Schloessleinsgasse 31
S: D-90453 Nuernberg
S: Germany

N: Arnaldo Carvalho de Melo
E: acme@kernel.org
E: arnaldo.melo@gmail.com
E: acme@redhat.com
P: 1024D/9224DF01 D5DF E3BB E3C8 BCBB F8AD  841A B6AB 4681 9224 DF01
D: tools/, IPX, LLC, DCCP, cyc2x, wl3501_cs, net/ hacks
S: Brazil

N: Karsten Merker
E: merker@linuxtag.org
D: DECstation framebuffer drivers
S: Germany

N: Michael Meskes
E: meskes@debian.org
P: 1024/04B6E8F5 6C 77 33 CA CC D6 22 03  AB AB 15 A3 AE AD 39 7D
D: Kernel hacker. PostgreSQL hacker. Software watchdog daemon.
D: Maintainer of several Debian packages
S: Th.-Heuss-Str. 61
S: D-41812 Erkelenz
S: Germany

N: Nigel Metheringham
E: Nigel.Metheringham@ThePLAnet.net
P: 1024/31455639 B7 99 BD B8 00 17 BD 46  C1 15 B8 AB 87 BC 25 FA
D: IP Masquerading work and minor fixes
S: Planet Online
S: The White House, Melbourne Street, LEEDS
S: LS2 7PS, United Kingdom

N: Craig Metz
E: cmetz@inner.net
D: Some of PAS 16 mixer & PCM support, inet6-apps

N: William (Bill) Metzenthen
E: billm@suburbia.net
D: Author of the FPU emulator.
D: Minor kernel hacker for other lost causes (Hercules mono, etc).
S: 22 Parker Street
S: Ormond
S: Victoria 3163
S: Australia

N: Eric Miao
E: eric.y.miao@gmail.com
D: MMP support

N: Pauline Middelink
E: middelin@polyware.nl
D: General low-level bug fixes, /proc fixes, identd support
D: Author of IP masquerading
D: Zoran ZR36120 Video For Linux driver
S: Boterkorfhoek 34
S: 7546 JA  Enschede
S: Netherlands

N: David S. Miller
E: davem@davemloft.net
D: Sparc and blue box hacker
D: Vger Linux mailing list co-maintainer
D: Linux Emacs elf/qmagic support + other libc/gcc things
D: Yee bore de yee bore! ;-)
S: 575 Harrison St. #103
S: San Francisco, CA 94105
S: USA

N: Rick Miller
E: rdmiller@execpc.com
W: http://www.execpc.com/~rdmiller/
D: Original Linux Device Registrar (Major/minor numbers)
D: au-play, bwBASIC
S: S78 W16203 Woods Road
S: Muskego, Wisconsin 53150
S: USA

N: Harald Milz
E: hm@seneca.linux.de
D: Linux Projects Map, Linux Commercial-HOWTO
D: general Linux publicity in Germany, vacation port
D: UUCP and CNEWS binary packages for LST
S: Editorial Board iX Mag
S: Helstorfer Str. 7
S: D-30625 Hannover
S: Germany

N: Ron Minnich
E: rminnich@sandia.gov
E: rminnich@gmail.com
D: 9p filesystem development

N: Corey Minyard
E: minyard@wf-rch.cirr.com
E: minyard@mvista.com
W: http://home.attbi.com/~minyard
D: Sony CDU31A CDROM Driver
D: IPMI driver
D: Various networking fixes long ago
D: Original ppc_md work
D: Shared zlib
S: 7406 Wheat Field Rd
S: Garland, Texas 75044
S: USA

N: Kazunori Miyazawa
E: miyazawa@linux-ipv6.org
E: Kazunori.Miyazawa@jp.yokogawa.com
E: kazunori@miyazawa.org
W: http://www.miyazawa.org/~kazunori/
D: IPsec, IPv6
D: USAGI/WIDE Project, Yokogawa Electric Corporation
S: 2-20-4-203, Nakacho,
S: Musashino, Tokyo 180-0006
S: Japan

N: Patrick Mochel
E: mochel@osdl.org
E: mochelp@infinity.powertie.org
D: PCI Power Management, ACPI work
S: 12725 SW Millikan Way, Suite 400
S: Beaverton, Oregon 97005
S: USA

N: Eberhard Mönkeberg
E: emoenke@gwdg.de
D: CDROM driver ""sbpcd"" (Matsushita/Panasonic/Soundblaster)
S: Ruhstrathöhe 2 b.
S: D-37085 Göttingen
S: Germany

N: Thomas Molina
E: tmolina@cablespeed.com
D: bug fixes, documentation, minor hackery

N: Paul Moore
E: paul@paul-moore.com
W: https://www.paul-moore.com
D: NetLabel, SELinux, audit

N: James Morris
E: jmorris@namei.org
W: http://namei.org/
D: Netfilter, Linux Security Modules (LSM), SELinux, IPSec,
D: Crypto API, general networking, miscellaneous.
S: PO Box 707
S: Spit Junction NSW 2088
S: Australia

N: David Mosberger-Tang
E: davidm@hpl.hp.com if IA-64 related, else David.Mosberger@acm.org
D: Linux/Alpha and Linux/ia64
S: 35706 Runckel Lane
S: Fremont, California 94536
S: USA

N: Sam Mosel
E: sam.mosel@computer.org
D: Wacom Intuos USB Support
S: 22 Seaview St
S: Fullarton 5063
S: South Australia

N: Wolfgang Muees
E: wolfgang@iksw-muees.de
D: Auerswald USB driver

N: Shrijeet Mukherjee
E: shrijeet@gmail.com
D: Network routing domains (VRF).

N: Paul Mundt
E: paul.mundt@gmail.com
D: SuperH maintainer

N: Ian A. Murdock
E: imurdock@gnu.ai.mit.edu
D: Creator of Debian distribution
S: 30 White Tail Lane
S: Lafayette, Indiana 47905
S: USA

N: Scott Murray
E: scottm@somanetworks.com
E: scott@spiteful.org
D: OPL3-SA2, OPL3-SA3 sound driver
D: CompactPCI hotplug core
D: Ziatech ZT5550 and generic CompactPCI hotplug drivers
S: Toronto, Ontario
S: Canada

N: Zwane Mwaikambo
E: zwanem@gmail.com
D: Various driver hacking
D: Lowlevel x86 kernel hacking
D: General debugging
S: (ask for current address)
S: Tanzania

N: Trond Myklebust
E: trond.myklebust@fys.uio.no
D: current NFS client hacker.
S: Dagaliveien 31e
S: N-0391 Oslo
S: Norway

N: Johan Myreen
E: jem@iki.fi
D: PS/2 mouse driver writer etc.
S: Dragonvagen 1 A 13
S: FIN-00330 Helsingfors
S: Finland

N: Matija Nalis
E: mnalis@jagor.srce.hr
E: mnalis@voyager.hr
D: Maintainer of the Umsdos file system
S: Listopadska 7
S: 10000 Zagreb
S: Croatia

N: Jonathan Naylor
E: g4klx@g4klx.demon.co.uk
E: g4klx@amsat.org
W: http://zone.pspt.fi/~jsn/
D: AX.25, NET/ROM and ROSE amateur radio protocol suites
D: CCITT X.25 PLP and LAPB.
S: 24 Castle View Drive
S: Cromford
S: Matlock
S: Derbyshire DE4 3RL
S: United Kingdom

N: Ian S. Nelson
E: nelsonis@earthlink.net
P: 1024D/00D3D983 3EFD 7B86 B888 D7E2 29B6  9E97 576F 1B97 00D3 D983
D: Minor mmap and ide hacks
S: 1370 Atlantis Ave.
S: Lafayette CO, 80026
S: USA

N: Russell Nelson
E: nelson@crynwr.com
W: http://www.crynwr.com/~nelson
P: 1024/83942741 FF 68 EE 27 A0 5A AA C3  F5 DC 05 62 BD 5B 20 2F
D: Author of cs89x0, maintainer of kernel changelog through 1.3.3
D: Wrote many packet drivers, from which some Ethernet drivers are derived.
S: 521 Pleasant Valley Road
S: Potsdam, New York 13676
S: USA

N: Dave Neuer
E: dave.neuer@pobox.com
D: Helped implement support for Compaq's H31xx series iPAQs
D: Other mostly minor tweaks & bugfixes

N: Michael Neuffer
E: mike@i-Connect.Net
E: neuffer@goofy.zdv.uni-mainz.de
W: http://www.i-Connect.Net/~mike/
D: Developer and maintainer of the EATA-DMA SCSI driver
D: Co-developer EATA-PIO SCSI driver
D: /proc/scsi and assorted other snippets
S: Zum Schiersteiner Grund 2
S: 55127 Mainz
S: Germany

N: Gustavo Niemeyer
E: niemeyer@conectiva.com
W: https://moin.conectiva.com.br/GustavoNiemeyer
D: wl3501 PCMCIA wireless card initial support for wireless extensions in 2.4
S: Conectiva S.A.
S: R. Tocantins 89
S: 80050-430  Curitiba PR
S: Brazil

N: David C. Niemi
E: niemi@tux.org
W: http://www.tux.org/~niemi/
D: Assistant maintainer of Mtools, fdutils, and floppy driver
D: Administrator of Tux.Org Linux Server, https://www.tux.org
S: 2364 Old Trail Drive
S: Reston, Virginia 20191
S: USA

N: Fredrik Noring
E: noring@nocrew.org
W: http://www.lysator.liu.se/~noring/
D: dsp56k device driver

N: Michael O'Reilly
E: michael@iinet.com.au
E: oreillym@tartarus.uwa.edu.au
D: Wrote the original dynamic sized disk cache stuff. I think the only
D: part that remains is the GFP_KERNEL et al #defines. :)
S: 192 Nichsolson Road
S: Subiaco, 6008
S: Perth, Western Australia
S: Australia

N: Miguel Ojeda
E: ojeda@kernel.org
W: https://ojeda.dev
D: Author of the ks0108, cfag12864b and cfag12864bfb auxiliary display drivers.
D: Maintainer of the auxiliary display drivers tree (drivers/auxdisplay/*)
S: Spain

N: Peter Oruba
D: AMD Microcode loader driver
S: Germany

N: Jens Osterkamp
E: jens@de.ibm.com
D: Maintainer of Spidernet network driver for Cell

N: Gadi Oxman
E: gadio@netvision.net.il
D: Original author and maintainer of IDE/ATAPI floppy/tape drivers

N: Greg Page
E: gpage@sovereign.org
D: IPX development and support

N: Venkatesh Pallipadi (Venki)
D: x86/HPET

N: Kyungmin Park
E: kyungmin.park@samsung.com
D: Samsung S5Pv210 and Exynos4210 mobile platforms

N: David Parsons
E: orc@pell.chi.il.us
D: improved memory detection code.

N: Ivan Passos
E: ivan@cyclades.com
D: Author of the Cyclades-PC300 synchronous card driver
D: Maintainer of the Cyclom-Y/Cyclades-Z asynchronous card driver
S: Cyclades Corp
S: 41934 Christy St
S: Fremont, CA 94538
S: USA

N: Mikulas Patocka
E: mikulas@artax.karlin.mff.cuni.cz
W: https://artax.karlin.mff.cuni.cz/~mikulas/
P: 1024/BB11D2D5 A0 F1 28 4A C4 14 1E CF  92 58 7A 8F 69 BC A4 D3
D: Read/write HPFS filesystem
S: Weissova 8
S: 644 00 Brno
S: Czech Republic

N: Vojtech Pavlik
E: vojtech@suse.cz
D: Joystick driver
D: arcnet-hardware readme
D: Minor ARCnet hacking
D: USB (HID, ACM, Printer ...)
S: Ucitelska 1576
S: Prague 8
S: 182 00 Czech Republic

N: Rick Payne
D: RFC2385 Support for TCP

N: Barak A. Pearlmutter
E: bap@cs.unm.edu
W: https://www.cs.unm.edu/~bap/
P: 512/602D785D 9B A1 83 CD EE CB AD 93  20 C6 4C B7 F5 E9 60 D4
D: Author of mark-and-sweep GC integrated by Alan Cox
S: Computer Science Department
S: FEC 313
S: University of New Mexico
S: Albuquerque, New Mexico 87131
S: USA

N: Avery Pennarun
E: apenwarr@worldvisions.ca
W: http://www.worldvisions.ca/~apenwarr/
D: ARCnet driver
D: ""make xconfig"" improvements
D: Various minor hacking
S: RR #5, 497 Pole Line Road
S: Thunder Bay, Ontario
S: CANADA P7C 5M9

N: Inaky Perez-Gonzalez
E: inaky.perez-gonzalez@intel.com
E: linux-wimax@intel.com
E: inakypg@yahoo.com
D: WiMAX stack
D: Intel Wireless WiMAX Connection 2400 driver

N: Yuri Per
E: yuri@pts.mipt.ru
D: Some smbfs fixes
S: Demonstratsii 8-382
S: Tula 300000
S: Russia

N: Inaky Perez-Gonzalez
E: inaky.perez-gonzalez@intel.com
D: UWB stack, HWA-RC driver and HWA-HC drivers
D: Wireless USB additions to the USB stack
D: WiMedia Link Protocol bits and pieces

N: Gordon Peters
E: GordPeters@smarttech.com
D: Isochronous receive for IEEE 1394 driver (OHCI module).
D: Bugfixes for the aforementioned.
S: Calgary, Alberta
S: Canada

N: Johnnie Peters
E: jpeters@phx.mcd.mot.com
D: Motorola PowerPC changes for PReP
S: 2900 S. Diable Way
S: Tempe, Arizona 85282
S: USA

N: Kirk Petersen
E: kirk@speakeasy.org
W: http://www.speakeasy.org/~kirk/
D: implemented kmod
D: modularized BSD Unix domain sockets

N: Martin Kasper Petersen
E: mkp@mkp.net
D: PA-RISC port
D: XFS file system
D: kiobuf based block I/O work
S: 314 Frank St.
S: Ottawa, Ontario
S: Canada K2P 0X8

N: Mikael Pettersson
E: mikpelinux@gmail.com
D: Miscellaneous fixes

N: Reed H. Petty
E: rhp@draper.net
W: http://www.draper.net
D: Loop device driver extensions
D: Encryption transfer modules (no export)
S: Post Office Box 1815
S: Harrison, Arkansas  72602-1815
S: USA

N: Kai Petzke
E: petzke@teltarif.de
W: http://www.teltarif.de/
P: 1024/B42868C1 D9 59 B9 98 BB 93 05 38  2E 3E 31 79 C3 65 5D E1
D: Driver for Laser Magnetic Storage CD-ROM
D: Some kernel bug fixes
D: Port of the database Postgres
D: Book: ""Linux verstehen und anwenden"" (Hanser-Verlag)
S: Triftstra=DFe 55
S: 13353 Berlin
S: Germany

N: Emanuel Pirker
E: epirker@edu.uni-klu.ac.at
D: AIC5800 IEEE 1394, RAW I/O on 1394
D: Starter of Linux1394 effort
S: ask per mail for current address

N: Nicolas Pitre
E: nico@fluxnic.net
D: StrongARM SA1100 support integrator & hacker
D: Xscale PXA architecture
D: unified SMC 91C9x/91C11x ethernet driver (smc91x)
S: Montreal, Quebec, Canada

N: Ken Pizzini
E: ken@halcyon.com
D: CDROM driver ""sonycd535"" (Sony CDU-535/531)

N: Stelian Pop
E: stelian@popies.net
P: 1024D/EDBB6147 7B36 0E07 04BC 11DC A7A0  D3F7 7185 9E7A EDBB 6147
D: random kernel hacks
S: Paimpont, France

N: Pete Popov
E: pete_popov@yahoo.com
D: Linux/MIPS AMD/Alchemy Port and mips hacking and debugging
S: San Jose, CA 95134
S: USA

N: Matt Porter
E: mporter@kernel.crashing.org
D: Motorola PowerPC PReP support
D: cPCI PowerPC support
D: Embedded PowerPC 4xx/6xx/7xx/74xx support
S: Chandler, Arizona 85249
S: USA

N: Frederic Potter
E: fpotter@cirpack.com
D: Some PCI kernel support

N: Rui Prior
E: rprior@inescn.pt
D: ATM device driver for NICStAR based cards

N: Stefan Probst
E: sp@caldera.de
D: The Linux Support Team Erlangen, 1993-97
S: Caldera (Deutschland) GmbH
S: Lazarettstrasse 8
S: 91054 Erlangen
S: Germany

N: Giuliano Procida
E: myxie@debian.org,gprocida@madge.com
D: Madge Ambassador driver (Collage 155 Server ATM adapter)
D: Madge Horizon driver (Collage 25 and 155 Client ATM adapters)
P: 1024/93898735 D3 9E F4 F7 6D 8D 2F 3A  38 BA 06 7C 2B 33 43 7D
S: Madge Networks
S: Framewood Road
S: Wexham SL3 6PJ
S: United Kingdom

N: Richard Purdie
E: rpurdie@rpsys.net
D: Backlight subsystem maintainer
S: United Kingdom

N: Daniel Quinlan
E: quinlan@pathname.com
W: https://www.pathname.com/~quinlan/
D: FSSTND coordinator; FHS editor
D: random Linux documentation, patches, and hacks
S: 4390 Albany Drive #41A
S: San Jose, California 95129
S: USA

N: Juan Quintela
E: quintela@fi.udc.es
D: Memory Management hacking
S: LFCIA
S: Departamento de Computación
S: Universidade da Coruña
S: E-15071
S: A Coruña
S: Spain

N: Augusto Cesar Radtke
E: bishop@sekure.org
W: http://bishop.sekure.org
D: {copy,get,put}_user calls updates
D: Miscellaneous hacks
S: R. Otto Marquardt, 226 - Garcia
S: 89020-350 Blumenau - Santa Catarina
S: Brazil

N: Goutham Rao
E: goutham.rao@intel.com
D: Linux/IA-64
S: 2200 Mission College Blvd
S: Santa Clara, CA 95052
S: USA

N: Anil Ravindranath
E: anil_ravindranath@pmc-sierra.com
D: PMC-Sierra MaxRAID driver

N: Eric S. Raymond
E: esr@thyrsus.com
W: http://www.tuxedo.org/~esr/
D: terminfo master file maintainer
D: Editor: Installation HOWTO, Distributions HOWTO, XFree86 HOWTO
D: Author: fetchmail, Emacs VC mode, Emacs GUD mode
S: 6 Karen Drive
S: Malvern, Pennsylvania 19355
S: USA

N: Stefan Reinauer
E: stepan@linux.de
W: http://www.freiburg.linux.de/~stepan/
D: Modularization of some filesystems
D: /proc/sound, minor fixes
S: Schlossbergring 9
S: 79098 Freiburg
S: Germany

N: Thomas Renninger
E: trenn@suse.de
D: cpupowerutils
S: SUSE Linux GmbH
S: Germany

N: Joerg Reuter
E: jreuter@yaina.de
W: http://yaina.de/jreuter/
W: http://www.qsl.net/dl1bke/
D: Generic Z8530 driver, AX.25 DAMA slave implementation
D: Several AX.25 hacks

N: Ricardo Ribalda
E: ribalda@kernel.org
W: http://ribalda.com
D: PLX USB338x driver
D: PCA9634 driver
D: Option GTM671WFS
D: Fintek F81216A
D: AD5761 iio driver
D: TI DAC7612 driver
D: Sony IMX214 driver
D: Various kernel hacks
S: Qtechnology A/S
S: Valby Langgade 142
S: 2500 Valby
S: Denmark

N: Francois-Rene Rideau
E: fare@tunes.org
W: http://www.tunes.org/~fare
D: petty kernel janitor (byteorder, ufs)
S: 6, rue Augustin Thierry
S: 75019 Paris
S: France

N: Rik van Riel
E: riel@redhat.com
W: https://www.surriel.com/
D: Linux-MM site, Documentation/admin-guide/sysctl/*, swap/mm readaround
D: kswapd fixes, random kernel hacker, rmap VM,
D: nl.linux.org administrator, minor scheduler additions
S: Red Hat Boston
S: 3 Lan Drive
S: Westford, MA 01886
S: USA

N: Pekka Riikonen
E: priikone@poseidon.pspt.fi
E: priikone@ssh.com
D: Random kernel hacking and bug fixes
D: International kernel patch project
S: Kasarmikatu 11 A4
S: 70110 Kuopio
S: Finland

N: Tobias Ringström
E: tori@unhappy.mine.nu
D: Davicom DM9102(A)/DM9132/DM9801 fast ethernet driver

N: Luca Risolia
E: luca.risolia@studio.unibo.it
P: 1024D/FCE635A4 88E8 F32F 7244 68BA 3958  5D40 99DA 5D2A FCE6 35A4
D: V4L driver for W996[87]CF JPEG USB Dual Mode Camera Chips
D: V4L2 driver for SN9C10x PC Camera Controllers
D: V4L2 driver for ET61X151 and ET61X251 PC Camera Controllers
D: V4L2 driver for ZC0301 Image Processor and Control Chip
S: Via Liberta' 41/A
S: Osio Sotto, 24046, Bergamo
S: Italy

N: William E. Roadcap
E: roadcapw@cfw.com
W: http://www.cfw.com/~roadcapw
D: Author of menu based configuration tool, Menuconfig.
S: 1407 Broad Street
S: Waynesboro, Virginia 22980
S: USA

N: Andrew J. Robinson
E: arobinso@nyx.net
W: http://www.nyx.net/~arobinso
D: Hayes ESP serial port driver

N: Florian La Roche
E: rzsfl@rz.uni-sb.de
E: flla@stud.uni-sb.de
D: Net programs and kernel net hacker
S: Gaildorfer Str. 27
S: 7000 Stuttgart 50
S: Germany

N: Christoph Rohland
E: hans-christoph.rohland@sap.com
E: ch.rohland@gmx.net
D: shm fs, SYSV semaphores, af_unix
S: Neue Heimat Str. 8
S: D-68789 St.Leon-Rot
S: Germany

N: Thiago Berlitz Rondon
E: maluco@mileniumnet.com.br
W: http://vivaldi.linuxms.com.br/~maluco
D: Miscellaneous kernel hacker
S: R. Anhanguera, 1487 - Ipiranga
S: 79080-740 - Campo Grande - Mato Grosso do Sul
S: Brazil

N: Stephen Rothwell
E: sfr@canb.auug.org.au
W: http://www.canb.auug.org.au/~sfr
P: 1024/BD8C7805 CD A4 9D 01 10 6E 7E 3B  91 88 FA D9 C8 40 AA 02
D: Boot/setup/build work for setup > 2K
D: Author, APM driver
D: Directory notification
S: 66 Maltby Circuit
S: Wanniassa ACT 2903
S: Australia

N: Gerard Roudier
E: groudier@free.fr
D: Contributed to asynchronous read-ahead improvement
S: 21 Rue Carnot
S: 95170 Deuil La Barre
S: France

N: Sebastien Rougeaux
E: Sebastien.Rougeaux@syseng.anu.edu.au
D: IEEE 1394 OHCI module
S: Research School of Information Science and Engineering
S: The Australian National University, ACT 0200
S: Australia

N: Aristeu Sergio Rozanski Filho
E: aris@cathedrallabs.org
D: Support for EtherExpress 10 ISA (i82595) in eepro driver
D: User level driver support for input
S: R. Jose Serrato, 130 - Santa Candida
S: 82640-320 - Curitiba - Paraná
S: Brazil

N: Alessandro Rubini
E: rubini@ipvvis.unipv.it
D: the gpm mouse server and kernel support for it

N: Philipp Rumpf
E: prumpf@tux.org
D: random bugfixes
S: Drausnickstrasse 29
S: 91052 Erlangen
S: Germany

N: Paul `Rusty' Russell
E: rusty@rustcorp.com.au
W: https://ozlabs.org/~rusty
D: Ruggedly handsome.
D: netfilter, ipchains with Michael Neuling.
S: 52 Moore St
S: Turner ACT 2612
S: Australia

N: Richard Russon (FlatCap)
E: kernel@flatcap.org
W: http://www.flatcap.org
D: NTFS support
D: LDM support (Win2000/XP Logical Disk Manager/Dynamic Disks)
S: 50 Swansea Road
S: Reading
S: United Kingdom

N: Bill Ryder
E: bryder@sgi.com
D: FTDI_SIO usb/serial converter driver
W: http://reality.sgi.com/bryder_wellington/ftdi_sio
S: I/3 Walter St
S: Wellington
S: New Zealand

N: Sampo Saaristo
E: sambo@cs.tut.fi
D: Co-author of Multi-Protocol Over ATM (MPOA)
S: Tampere University of Technology / Telecom lab
S: Hermiankatu 12C
S: FIN-33720 Tampere
S: Finland

N: Thomas Sailer
E: t.sailer@alumni.ethz.ch
E: HB9JNX@HB9W.CHE.EU (packet radio)
D: Baycom driver
S: Markusstrasse 18
S: 8006 Zuerich
S: Switzerland

N: Manuel Estrada Sainz
D: Firmware loader (request_firmware)

N: Wayne Salamon
E: wsalamon@tislabs.com
E: wsalamon@nai.com
D: portions of the Linux Security Module (LSM) framework and security modules

N: Robert Sanders
E: gt8134b@prism.gatech.edu
D: Dosemu

N: Duncan Sands
E: duncan.sands@free.fr
W: http://topo.math.u-psud.fr/~sands
D: Alcatel SpeedTouch USB driver
S: 69 rue Dunois
S: 75013 Paris
S: France

N: Aleksa Sarai
E: cyphar@cyphar.com
W: https://www.cyphar.com/
D: /sys/fs/cgroup/pids
D: openat2(2)
S: Sydney, Australia

N: Dipankar Sarma
E: dipankar@in.ibm.com
D: RCU

N: Hannu Savolainen
E: hannu@opensound.com
D: Maintainer of the sound drivers until 2.1.x days.
D: Original compressed boot image support.
S: Valurink. 4A11
S: 03600 Karkkila
S: Finland

N: Deepak Saxena
E: dsaxena@plexity.net
D: I2O kernel layer (config, block, core, pci, net). I2O disk support for LILO
D: XScale(IOP, IXP) porting and other random ARM bits
S: Portland, OR

N: Eric Schenk
E: Eric.Schenk@dna.lth.se
D: Random kernel debugging.
D: SYSV Semaphore code rewrite.
D: Network layer debugging.
D: Dial on demand facility (diald).
S: Dag Hammerskjolds v. 3E
S: S-226 64 LUND
S: Sweden

N: Henning P. Schmiedehausen
E: hps@tanstaafl.de
D: added PCI support to the serial driver
S: Buckenhof, Germany

N: Michael Schmitz
E:
D: Macintosh IDE Driver

N: Peter De Schrijver
E: stud11@cc4.kuleuven.ac.be
D: Mitsumi CD-ROM driver patches March version
S: Molenbaan 29
S: B2240 Zandhoven
S: Belgium

N: Martin Schulze
E: joey@linux.de
W: http://home.pages.de/~joey/
D: Random Linux Hacker, Linux Promoter
D: CD-List, Books-List, Ex-FAQ
D: Linux-Support, -Mailbox, -Stammtisch
D: several improvements to system programs
S: Oldenburg
S: Germany

N: Mathieu Poirier
E: mathieu.poirier@linaro.org
D: CoreSight kernel subsystem, Maintainer 2014-2022
D: Perf tool support for CoreSight

N: Robert Schwebel
E: robert@schwebel.de
W: https://www.schwebel.de
D: Embedded hacker and book author,
D: AMD Elan support for Linux
S: Pengutronix
S: Braunschweiger Strasse 79
S: 31134 Hildesheim
S: Germany

N: Martin Schwidefsky
D: Martin was the most significant contributor to the initial s390
D: port of the Linux Kernel and later the maintainer of the s390
D: architecture backend for almost two decades.
D: He passed away in 2019, and will be greatly missed.
S: Germany
W: https://lwn.net/Articles/789028/

N: Marcel Selhorst
E: tpmdd@selhorst.net
D: TPM driver

N: Darren Senn
E: sinster@darkwater.com
D: Whatever I notice needs doing (so far: itimers, /proc)
S: Post Office Box 64132
S: Sunnyvale, California 94088-4132
S: USA

N: Stas Sergeev
E: stsp@users.sourceforge.net
D: PCM PC-Speaker driver
D: misc fixes
S: Russia

N: Simon Shapiro
E: shimon@i-Connect.Net
W: http://www.-i-Connect.Net/~shimon
D: SCSI debugging
D: Maintainer of the Debian Kernel packages
S: 14355 SW Allen Blvd., Suite #140
S: Beaverton, Oregon 97008
S: USA

N: Mike Shaver
E: shaver@hungry.org
W: http://www.hungry.org/~shaver/
D: MIPS work, /proc/sys/net, misc net hacking
S: 149 Union St.
S: Kingston, Ontario
S: Canada K7L 2P4

N: John Shifflett
E: john@geolog.com
E: jshiffle@netcom.com
D: Always IN2000 SCSI driver
D: wd33c93 SCSI driver (linux-m68k)
S: San Jose, California
S: USA

N: Joonyoung Shim
E: y0922.shim@samsung.com
D: Samsung Exynos DRM drivers

N: Robert Siemer
E: Robert.Siemer@gmx.de
P: 2048/C99A4289 2F DC 17 2E 56 62 01 C8  3D F2 AC 09 F2 E5 DD EE
D: miroSOUND PCM20 radio RDS driver, ACI rewrite
S: Klosterweg 28 / i309
S: 76131 Karlsruhe
S: Germany

N: James Simmons
E: jsimmons@infradead.org
E: jsimmons@users.sf.net
D: Frame buffer device maintainer
D: input layer development
D: tty/console layer
D: various mipsel devices
S: 115 Carmel Avenue
S: El Cerrito CA 94530
S: USA

N: Jaspreet Singh
E: jaspreet@sangoma.com
W: www.sangoma.com
D: WANPIPE drivers & API Support for Sangoma S508/FT1 cards
S: Sangoma Technologies Inc.,
S: 1001 Denison Street
S: Suite 101
S: Markham, Ontario L3R 2Z6
S: Canada

N: Haavard Skinnemoen
M: Haavard Skinnemoen <hskinnemoen@gmail.com>
D: AVR32 architecture port to Linux and maintainer.

N: Rick Sladkey
E: jrs@world.std.com
D: utility hacker: Emacs, NFS server, mount, kmem-ps, UPS debugger, strace, GDB
D: library hacker: RPC, profil(3), realpath(3), regexp.h
D: kernel hacker: unnamed block devs, NFS client, fast select, precision timer
S: 24 Avon Place
S: Arlington, Massachusetts 02174
S: USA

N: Craig Small
E: csmall@triode.apana.org.au
E: vk2xlz@gonzo.vk2xlz.ampr.org (packet radio)
D: Gracilis PackeTwin device driver
D: RSPF daemon
S: 10 Stockalls Place
S: Minto, NSW, 2566
S: Australia

N: Stephen Smalley
E: sds@tycho.nsa.gov
D: portions of the Linux Security Module (LSM) framework and security modules

N: Chris Smith
E: csmith@convex.com
D: Read only HPFS filesystem
S: Richardson, Texas
S: USA

N: Christopher Smith
E: x@xman.org
D: Tulip net driver hacker

N: Mark Smith
E: mark.smith@comdev.cc
D: Multicast support in bonding driver

N: Miquel van Smoorenburg
E: miquels@cistron.nl
D: Kernel and net hacker. Sysvinit, minicom. doing Debian stuff.
S: Cistron Internet Services
S: PO-Box 297
S: 2400 AG, Alphen aan den Rijn
S: The Netherlands

N: Scott Snyder
E: snyder@fnald0.fnal.gov
D: ATAPI cdrom driver
S: MS 352, Fermilab
S: Post Office Box 500
S: Batavia, Illinois 60510
S: USA

N: Leo Spiekman
E: leo@netlabs.net
W: http://www.netlabs.net/hp/leo/
D: Optics Storage 8000AT cdrom driver
S: Cliffwood, New Jersey 07721
S: USA

N: Manfred Spraul
E: manfred@colorfullife.com
W: http://www.colorfullife.com/~manfred
D: Lots of tiny hacks. Larger improvements to SysV IPC msg,
D: slab, pipe, select.
S: 71701 Schwieberdingen
S: Germany

N: Andrew Stanley-Jones
E: asj@lanmedia.com
D: LanMedia Corp. Device WAN card device driver
S: #102, 686 W. Maude Ave
S: Sunyvale, CA 94086
S: USA

N: Michael Still
E: mikal@stillhq.com
W: http://www.stillhq.com
D: Various janitorial patches
D: mandocs and mandocs_install build targets
S: (Email me and ask)
S: Australia

N: Henrik Storner
E: storner@image.dk
W: http://www.image.dk/~storner/
W: https://www.sslug.dk/
D: Configure script: Invented tristate for module-configuration
D: vfat/msdos integration, kerneld docs, Linux promotion
D: Miscellaneous bug-fixes
S: Chr. Winthersvej 1 B, st.th.
S: DK-1860 Frederiksberg C
S: Denmark

N: Drew Sullivan
E: drew@ss.org
W: http://www.ss.org/
P: 1024/ACFFA969 5A 9C 42 AB E4 24 82 31  99 56 00 BF D3 2B 25 46
D: iBCS2 developer
S: 22 Irvington Cres.
S: Willowdale, Ontario
S: Canada M2N 2Z1

N: Adam Sulmicki
E: adam@cfar.umd.edu
W: http://www.eax.com
D: core networking fixes
D: patch-kernel enhancements
D: misc kernel fixes and updates

N: Adrian Sun
E: asun@cobaltnet.com
D: hfs support
D: alpha rtc port, random appletalk fixes
S: Department of Zoology, University of Washington
S: Seattle, WA  98195-1800
S: USA

N: Eugene Surovegin
E: ebs@ebshome.net
W: https://kernel.ebshome.net/
P: 1024D/AE5467F1 FF22 39F1 6728 89F6 6E6C  2365 7602 F33D AE54 67F1
D: Embedded PowerPC 4xx: EMAC, I2C, PIC and random hacks/fixes
S: Sunnyvale, California 94085
S: USA

N: Corey Thomas
E: corey@world.std.com
W: http://world.std.com/~corey/index.html
D: Raylink/WebGear wireless LAN device driver (ray_cs) author
S: 145 Howard St.
S: Northborough, MA 01532
S: USA

N: Doug Thompson
E: dougthompson@xmission.com
D: EDAC

N: Tommy Thorn
E: Tommy.Thorn@irisa.fr
W: http://www.irisa.fr/prive/thorn/index.html
P: 512/B4AFC909 BC BF 6D B1 52 26 1E D6  E3 2F A3 24 2A 84 FE 21
D: Device driver hacker (aha1542 & plip)
S: IRISA
S: Universit=E9 de Rennes I
S: F-35042 Rennes Cedex
S: France

N: Urs Thuermann
E: urs.thuermann@volkswagen.de
W: https://www.volkswagen.de
D: Controller Area Network (network layer core)
S: Brieffach 1776
S: 38436 Wolfsburg
S: Germany

N: Jon Tombs
E: jon@gte.esi.us.es
W: http://www.esi.us.es/~jon
D: NFS mmap()
D: XF86_S3
D: Kernel modules
D: Parts of various other programs (xfig, open, ...)
S: C/ Federico Garcia Lorca 1 10-A
S: Sevilla 41005
S: Spain

N: Linus Torvalds
E: torvalds@linux-foundation.org
D: Original kernel hacker
S: Portland, Oregon 97005
S: USA

N: Marcelo Tosatti
E: marcelo@kvack.org
D: v2.4 kernel maintainer
S: Brazil

N: Stefan Traby
E: stefan@quant-x.com
D: Minor Alpha kernel hacks
S: Mitterlasznitzstr. 13
S: 8302 Nestelbach
S: Austria

N: Jeff Tranter
E: tranter@pobox.com
D: Enhancements to Joystick driver
D: Author of Sound HOWTO and CD-ROM HOWTO
D: Author of several small utilities
D: (bogomips, scope, eject, statserial)
S: 1 Laurie Court
S: Kanata, Ontario
S: Canada K2L 1S2

N: Andrew Tridgell
E: tridge@samba.org
W: https://samba.org/tridge/
D: dosemu, networking, samba
S: 3 Ballow Crescent
S: MacGregor A.C.T 2615
S: Australia

N: Josh Triplett
E: josh@joshtriplett.org
P: 4096R/8AFF873D 758E 5042 E397 4BA3 3A9C  1E67 0ED9 A3DF 8AFF 873D
D: RCU and rcutorture
D: lock annotations, finding and fixing lock bugs
D: kernel tinification

N: Winfried Trümper
E: winni@xpilot.org
W: http://www.shop.de/~winni/
D: German HOWTO, Crash-Kurs Linux (German, 100 comprehensive pages)
D: CD-Writing HOWTO, various mini-HOWTOs
D: One-week tutorials on Linux twice a year (free of charge)
D: Linux-Workshop Köln (aka LUG Cologne, Germany), Installfests
S: Tacitusstr. 6
S: D-50968 Köln

N: Tsu-Sheng Tsao
E: tsusheng@scf.usc.edu
D: IGMP(Internet Group Management Protocol) version 2
S: 2F 14 ALY 31 LN 166 SEC 1 SHIH-PEI RD
S: Taipei
S: Taiwan 112
S: Republic of China
S: 24335 Delta Drive
S: Diamond Bar, California 91765
S: USA

N: Theodore Ts'o
E: tytso@mit.edu
D: Random Linux hacker
D: Maintainer of tsx-11.mit.edu ftp archive
D: Maintainer of c.o.l.* Usenet<->mail gateway
D: Author of serial driver
D: Author of the new e2fsck
D: Author of job control and system call restart code
D: Author of ramdisk device driver
D: Author of loopback device driver
D: Author of /dev/random driver
S: MIT Room E40-343
S: 1 Amherst Street
S: Cambridge, Massachusetts 02139
S: USA

N: Simmule Turner
E: sturner@tele-tv.com
D: Added swapping to filesystem
S: 4226 Landgreen Street
S: Rockville, Maryland 20853
S: USA

N: Stephen Tweedie
E: sct@redhat.com
P: 1024/E7A417AD E2 FE A4 20 34 EC ED FC 7D 7E 67 8D E0 31 D1 69
P: 1024D/43BE7544 D2A4 8556 08E6 90E7 076C  BA3F 243F 20A4 43BE 7544
D: Second extended file system developer
D: General filesystem hacker
D: kswap vm management code
S: 44 Campbell Park Crescent
S: Edinburgh EH13 0HT
S: United Kingdom

N: Thomas Uhl
E: uhl@sun1.rz.fh-heilbronn.de
D: Application programmer
D: Linux promoter
D: Author of a German book on Linux
S: Obere Heerbergstrasse 17
S: 97078 Wuerzburg
S: Germany

N: Jason Uhlenkott
E: juhlenko@akamai.com
D: I3000 EDAC driver

N: Greg Ungerer
E: gerg@snapgear.com
D: uClinux kernel hacker
D: Port uClinux to the Motorola ColdFire CPU
D: Author of Stallion multiport serial drivers
S: SnapGear Inc.
S: 825 Stanley St
S: Woolloongabba. QLD. 4102
S: Australia

N: Jeffrey A. Uphoff
E: juphoff@transmeta.com
E: jeff.uphoff@linux.org
P: 1024/9ED505C5 D7 BB CA AA 10 45 40 1B  16 19 0A C0 38 A0 3E CB
D: Linux Security/Alert mailing lists' moderator/maintainer.
D: NSM (rpc.statd) developer.
D: PAM S/Key module developer.
D: 'dip' contributor.
D: AIPS port, astronomical community support.
S: Transmeta Corporation
S: 2540 Mission College Blvd.
S: Santa Clara, CA 95054
S: USA

N: Matthias Urlichs
E: smurf@smurf.noris.de
E: smurf@debian.org
E: matthias@urlichs.de
D: Consultant, developer, kernel hacker
D: In a previous life, worked on Streams/ISDN/BSD networking code for Linux
S: Schleiermacherstrasse 12
S: 90491 Nuernberg
S: Germany

N: Geert Uytterhoeven
E: geert@linux-m68k.org
W: http://users.telenet.be/geertu/
P: 4096R/4804B4BC3F55EEFB 750D 82B0 A781 5431 5E25  925B 4804 B4BC 3F55 EEFB
D: m68k/Amiga and PPC/CHRP Longtrail coordinator
D: Frame buffer device and XF68_FBDev maintainer
D: m68k IDE maintainer
D: Amiga Zorro maintainer
D: Amiga Buddha and Catweasel chipset IDE
D: Atari Falcon chipset IDE
D: Amiga Gayle chipset IDE
D: mipsel NEC DDB Vrc-5074
S: Haterbeekstraat 55B
S: B-3200 Aarschot
S: Belgium

N: Chris Vance
E: cvance@tislabs.com
E: cvance@nai.com
D: portions of the Linux Security Module (LSM) framework and security modules

N: Petr Vandrovec
E: petr@vandrovec.name
D: Small contributions to ncpfs
D: Matrox framebuffer driver
S: 21513 Conradia Ct
S: Cupertino, CA 95014
S: USA

N: Thibaut Varène
E: hacks+kernel@slashdirt.org
W: http://hacks.slashdirt.org/
D: PA-RISC port minion, PDC and GSCPS2 drivers, debuglocks and other bits
D: Some ARM at91rm9200 bits, S1D13XXX FB driver, random patches here and there
D: AD1889 sound driver
S: France

N: Heikki Vatiainen
E: hessu@cs.tut.fi
D: Co-author of Multi-Protocol Over ATM (MPOA), some LANE hacks
S: Tampere University of Technology / Telecom lab
S: Hermiankatu 12C
S: FIN-33720 Tampere
S: Finland

N: Andrew Veliath
E: andrewtv@usa.net
D: Turtle Beach MultiSound sound driver
S: USA

N: Dirk Verworner
D: Co-author of German book ``Linux-Kernel-Programmierung''
D: Co-founder of Berlin Linux User Group

N: Andrew Victor
E: linux@maxim.org.za
W: http://maxim.org.za/at91_26.html
D: First maintainer of Atmel ARM-based SoC, aka AT91
D: Introduced support for at91rm9200, the first chip of AT91 family
S: South Africa

N: Riku Voipio
E: riku.voipio@iki.fi
D: Author of PCA9532 LED and Fintek f75375s hwmon driver
D: Some random ARM board patches
S: Finland

N: Patrick Volkerding
E: volkerdi@ftp.cdrom.com
D: Produced the Slackware distribution, updated the SVGAlib
D: patches for ghostscript, worked on color 'ls', etc.
S: 301 15th Street S.
S: Moorhead, Minnesota 56560
S: USA

N: Jos Vos
E: jos@xos.nl
W: http://www.xos.nl/
D: Various IP firewall updates, ipfwadm
S: X/OS Experts in Open Systems BV
S: Kruislaan 419
S: 1098 VA Amsterdam
S: The Netherlands

N: Jeroen Vreeken
E: pe1rxq@amsat.org
W: http://www.chello.nl/~j.vreeken/
D: SE401 usb webcam driver
D: ZD1201 usb wireless lan driver
S: Maastrichterweg 63
S: 5554 GG Valkenswaard
S: The Netherlands

N: Mark Wallis
E: mwallis@serialmonkey.com
W: http://mark.serialmonkey.com
D: Ralink rt2x00 WLAN driver
S: Newcastle, Australia

N: Peter Shaobo Wang
E: pwang@mmdcorp.com
W: http://www.mmdcorp.com/pw/linux
D: Driver for Interphase ATM (i)Chip SAR adapter card family (x575, x525, x531).
S: 1513 Brewster Dr.
S: Carrollton, TX 75010
S: USA

N: Tim Waugh
E: tim@cyberelk.net
D: Co-architect of the parallel-port sharing system
S: 17 Curling Vale
S: GUILDFORD
S: Surrey
S: GU2 7PJ
S: United Kingdom

N: Juergen Weigert
E: jnweiger@immd4.informatik.uni-erlangen.de
D: The Linux Support Team Erlangen

N: David Weinehall
E: tao@acc.umu.se
P: 1024D/DC47CA16 7ACE 0FB0 7A74 F994 9B36  E1D1 D14E 8526 DC47 CA16
W: https://www.acc.umu.se/~tao/
D: v2.0 kernel maintainer
D: Fixes for the NE/2-driver
D: Miscellaneous MCA-support
D: Cleanup of the Config-files

N: Matt Welsh
E: mdw@metalab.unc.edu
W: http://www.cs.berkeley.edu/~mdw
D: Original Linux Documentation Project coordinator
D: Author, ""Running Linux"" (O'Reilly)
D: Author, ""Linux Installation and Getting Started"" (LDP) and several HOWTOs
D: Linuxdoc-SGML formatting system (now SGML-Tools)
D: Device drivers for various high-speed network interfaces (Myrinet, ATM)
D: Keithley DAS1200 device driver
D: Original maintainer of sunsite WWW and FTP sites
D: Original moderator of c.o.l.announce and c.o.l.answers
S: Computer Science Division
S: UC Berkeley
S: Berkeley, CA 94720-1776
S: USA

N: Harald Welte
E: laforge@netfilter.org
P: 1024D/30F48BFF DBDE 6912 8831 9A53 879B  9190 5DA5 C655 30F4 8BFF
W: https://gnumonks.org/users/laforge
D: netfilter: new nat helper infrastructure
D: netfilter: ULOG, ECN, DSCP target
D: netfilter: TTL match
D: netfilter: IPv6 mangle table
D: netfilter: various other hacks
S: Berlin
S: Germany

N: Bill Wendling
E: wendling@ganymede.isdn.uiuc.edu
W: http://www.ncsa.uiuc.edu/~wendling/
D: Various random hacks. Mostly on poll/select logic.
S: 605 E. Springfield Ave.
S: Champaign, IL 61820
S: USA

N: Mike Westall
D: IBM Turboways 25 ATM Device Driver
E: westall@cs.clemson.edu
S: Department of Computer Science
S: Clemson University
S: Clemson SC 29634 USA

N: Greg Wettstein
E: greg@wind.rmcc.com
D: Filesystem valid flag for MINIX filesystem.
D: Minor kernel debugging.
D: Development and maintenance of sysklogd.
D: Monitoring of development kernels for long-term stability.
D: Early implementations of Linux in a commercial environment.
S: Dr. Greg Wettstein, Ph.D.
S: Oncology Research Division Computing Facility
S: Roger Maris Cancer Center
S: 820 4th St. N.
S: Fargo, North Dakota 58122
S: USA

N: Steven Whitehouse
E: steve@chygwyn.com
W: http://www.chygwyn.com/~steve
D: Linux DECnet project
D: Minor debugging of other networking protocols.
D: Misc bug fixes and GFS2 filesystem development

N: Hans-Joachim Widmaier
E: hjw@zvw.de
D: AFFS rewrite
S: Eichenweg 16
S: 73650 Winterbach
S: Germany

N: Urban Widmark
E: urban@svenskatest.se
D: via-rhine, misc net driver hacking

N: Marco van Wieringen
E: mvw@planets.elm.net
D: Author of process accounting and diskquota
S: Breeburgsingel 12
S: 2135 CN Hoofddorp
S: The Netherlands

N: Matthew Wilcox
E: matthew@wil.cx
W: ftp://ftp.uk.linux.org/pub/linux/people/willy/
D: Linux/PARISC hacker.  Filesystem hacker.  Random other hacking.  Custom
D: PPC port hacking.

N: G\""unter Windau
E: gunter@mbfys.kun.nl
D: Some bug fixes in the polling printer driver (lp.c)
S: University of Nijmegen
S: Geert-Grooteplein Noord 21
S: 6525 EZ Nijmegen
S: The Netherlands

N: Ulrich Windl
E: Ulrich.Windl@rz.uni-regensburg.de
P: 1024/E843660D CF D7 43 A1 5A 49 14 25  7C 04 A0 6E 4C 3A AC 6D
D: Supports NTP on Linux.  Added PPS code.  Fixed bugs in adjtimex().
S: Alte Regensburger Str. 11a
S: 93149 Nittenau
S: Germany

N: Gertjan van Wingerde
E: gwingerde@gmail.com
D: Ralink rt2x00 WLAN driver
D: Minix V2 file-system
D: Misc fixes
S: The Netherlands

N: Lars Wirzenius
E: liw@iki.fi
D: Linux System Administrator's Guide, author, former maintainer
D: comp.os.linux.announce, former moderator
D: Linux Documentation Project, co-founder
D: Original sprintf in kernel
D: Original kernel README (for version 0.97)
D: Linux News (electronic magazine, now dead), founder and former editor
D: Meta-FAQ, originator, former maintainer
D: INFO-SHEET, former maintainer
D: Author of the longest-living linux bug

N: Jonathan Woithe
E: jwoithe@just42.net
W: http:/www.just42.net/jwoithe
D: ALS-007 sound card extensions to Sound Blaster driver
S: 20 Jordan St
S: Valley View, SA 5093
S: Australia

N: Clifford Wolf
E: god@clifford.at
W: http://www.clifford.at/
D: Menuconfig/lxdialog improvement
S: Foehrengasse 16
S: A-2333 Leopoldsdorf b. Wien
S: Austria

N: Roger E. Wolff
E: R.E.Wolff@BitWizard.nl
D: Written kmalloc/kfree
D: Written Specialix IO8+ driver
D: Written Specialix SX driver
S: van Bronckhorststraat 12
S: 2612 XV Delft
S: The Netherlands

N: Thomas Woller
D: CS461x Cirrus Logic sound driver

N: David Woodhouse
E: dwmw2@infradead.org
D: JFFS2 file system, Memory Technology Device subsystem,
D: various other stuff that annoyed me by not working.
S: c/o Intel Corporation
S: Pipers Way
S: Swindon. SN3 1RJ
S: England

N: Chris Wright
E: chrisw@sous-sol.org
D: hacking on LSM framework and security modules.
S: Portland, OR
S: USA

N: Michal Wronski
E: michal.wronski@gmail.com
D: POSIX message queues fs (with K. Benedyczak)
S: Krakow
S: Poland

N: Frank Xia
E: qx@math.columbia.edu
D: Xiafs filesystem [defunct]
S: 542 West 112th Street, 5N
S: New York, New York 10025
S: USA

N: Li Yang
E: leoli@freescale.com
D: Freescale Highspeed USB device driver
D: Freescale QE SoC support and Ethernet driver
S: B-1206 Jingmao Guojigongyu
S: 16 Baliqiao Nanjie, Beijing 101100
S: People's Repulic of China

N: Vlad Yasevich
E: vyasevich@gmail.com
D: SCTP protocol maintainer.

N: Aviad Yehezkel
E: aviadye@nvidia.com
D: Kernel TLS implementation and offload support.

N: Victor Yodaiken
E: yodaiken@fsmlabs.com
D: RTLinux (RealTime Linux)
S: POB 1822
S: Socorro NM, 87801
S: USA

N: Hiroshi YOKOTA
E: yokota@netlab.is.tsukuba.ac.jp
D: Workbit NinjaSCSI-3/32Bi PCMCIA driver
D: Workbit NinjaSCSI-32Bi/UDE driver
S: Japan

N: Hideaki YOSHIFUJI
E: hideaki@yoshifuji.org
E: yoshfuji@linux-ipv6.org
W: http://www.yoshifuji.org/~hideaki/
P: 1024D/E0620EEA 9022 65EB 1ECF 3AD1 0BDF  80D8 4807 F894 E062 0EEA
D: IPv6 and other networking related stuff
D: USAGI/WIDE Project, Keio University
S: Jeunet Palace Kawasaki #1-201, 10-2, Furukawa-cho, Saiwai-ku
S: Kawasaki, Kanagawa 212-0025
S: Japan

N: Eric Youngdale
E: eric@andante.org
W: http://www.andante.org
D: General kernel hacker
D: SCSI iso9660 and ELF
S: 6389 Hawk View Lane
S: Alexandria, Virginia 22312
S: USA

N: Niibe Yutaka
E: gniibe@mri.co.jp
D: PLIP driver
D: Asynchronous socket I/O in the NET code
S: Mitsubishi Research Institute, Inc.
S: ARCO Tower 1-8-1 Shimomeguro Meguro-ku
S: Tokyo 153
S: Japan

N: James R. Van Zandt
E: jrv@vanzandt.mv.com
P: 1024/E298966D F0 37 4F FD E5 7E C5 E6  F1 A0 1E 22 6F 46 DA 0C
D: Author and maintainer of the Double Talk speech synthesizer driver
S: 27 Spencer Drive
S: Nashua, New Hampshire 03062
S: USA

N: Orest Zborowski
E: orestz@eskimo.com
D: XFree86 and kernel development
S: 1507 145th Place SE #B5
S: Bellevue, Washington 98007
S: USA

N: Wensong Zhang
E: wensong@linux-vs.org
D: IP virtual server (IPVS).

N: Haojian Zhuang
E: haojian.zhuang@gmail.com
D: MMP support

N: Richard Zidlicky
E: rz@linux-m68k.org, rdzidlic@geocities.com
W: http://www.geocities.com/rdzidlic
D: Q40 port - see arch/m68k/q40/README
D: various m68k hacks
S: Germany

N: Werner Zimmermann
E: Werner.Zimmermann@fht-esslingen.de
D: CDROM driver ""aztcd"" (Aztech/Okano/Orchid/Wearnes)
S: Flandernstrasse 101
S: D-73732 Esslingen
S: Germany

N: Roman Zippel
E: zippel@linux-m68k.org
D: AFFS and HFS filesystems, m68k maintainer, new kernel configuration in 2.5

N: Leonard N. Zubkoff
W: http://www.dandelion.com/Linux/
D: BusLogic SCSI driver
D: Mylex DAC960 PCI RAID driver
D: Miscellaneous kernel fixes

N: Alessandro Zummo
E: a.zummo@towertech.it
D: CMI8330 support is sb_card.c
D: ISAPnP fixes in sb_card.c
D: ZyXEL omni.net lcd plus driver
D: RTC subsystem
S: Italy

N: Marc Zyngier
E: maz@wild-wind.fr.eu.org
W: http://www.misterjones.org
D: MD driver
D: EISA/sysfs subsystem
S: France

# Don't add your name here, unless you really _are_ after Marc
# alphabetically. Leonard used to be very proud of being the
# last entry, and he'll get positively pissed if he can't even
# be second-to-last.  (and this file really _is_ supposed to be
# in alphabetic order)
",[''],No_license_found,"mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
Time::ParseDate License
mpi Permissive License
mpi Permissive License
mpi Permissive License
XFree86 License 1.1
Time::ParseDate License",0.0,0.0
13,13,23,linux-master/tools/testing/selftests/timers/leapcrash.c,GPL,438,"We have to call adjtime twice here, as kernels* prior to 6b1859dba01c7 (included in 3.5 and* -stable), had an issue with the state machine* and wouldn't clear the STA_INS/DEL flag directly.
clear NTP time_status & time_state
Set the leap second insert flag
Make sure we cleanup on ctrl-c
Demo leapsecond deadlock*              by: John Stultz (john.stultz@linaro.org)*              (C) Copyright IBM 2012*              (C) Copyright 2013, 2015 Linaro Limited*              Licensed under the GPL** This test demonstrates leapsecond deadlock that is possible* on kernels from 2.6.26 to 3.3.** WARNING: THIS WILL LIKELY HARD HANG SYSTEMS AND MAY LOSE DATA* RUN AT YOUR OWN RISK!*  To build:*	$ gcc leapcrash.c -o leapcrash -lrt
Calculate the next possible leap second 23:59:60 GMT
set the time to 2 seconds before the leap
Get the current time
hammer on adjtime w/ STA_INS","Demo leapsecond deadlock*              by: John Stultz (john.stultz@linaro.org)*              (C) Copyright IBM 2012*              (C) Copyright 2013, 2015 Linaro Limited*              Licensed under the GPL** This test demonstrates leapsecond deadlock that is possible* on kernels from 2.6.26 to 3.3.** WARNING: THIS WILL LIKELY HARD HANG SYSTEMS AND MAY LOSE DATA* RUN AT YOUR OWN RISK!*  To build:*	$ gcc leapcrash.c -o leapcrash -lrt
clear NTP time_status & time_state
We have to call adjtime twice here, as kernels* prior to 6b1859dba01c7 (included in 3.5 and* -stable), had an issue with the state machine* and wouldn't clear the STA_INS/DEL flag directly.
Make sure we cleanup on ctrl-c
Get the current time
Calculate the next possible leap second 23:59:60 GMT
set the time to 2 seconds before the leap
hammer on adjtime w/ STA_INS
Set the leap second insert flag","['Demo leapsecond deadlock*              by: John Stultz (john.stultz@linaro.org)*              (C) Copyright IBM 2012*              (C) Copyright 2013, 2015 Linaro Limited*              Licensed under the GPL** This test demonstrates leapsecond deadlock that is possible* on kernels from 2.6.26 to 3.3.** WARNING: THIS WILL LIKELY HARD HANG SYSTEMS AND MAY LOSE DATA* RUN AT YOUR OWN RISK!*  To build:*\\t$ gcc leapcrash.c -o leapcrash -lrt']",GPL,"Creative Commons Attribution Share Alike 2.1 Japan
NTP No Attribution
mpi Permissive License
mpi Permissive License
XFree86 License 1.1
mpi Permissive License
Gutmann License
mpi Permissive License
McPhee Slideshow License",1.0,100.0
14,14,24,linux-master/tools/testing/selftests/vDSO/vdso_test_clock_getres.c,GPL-2.0-only Linux-syscall-note,387,"SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
vdso_clock_getres.c: Sample code to test clock_getres.* Copyright (c) 2019 Arm Ltd.** Compile with:* gcc -std=gnu99 vdso_clock_getres.c** Tested on ARM, ARM64, MIPS32, x86 (32-bit and 64-bit),* Power (32-bit and 64-bit), S390x (32-bit and 64-bit).* Might work on other architectures.
This function calls clock_getres in vdso and by system call* with different values for clock_id.** Example of output:** clock_id: CLOCK_REALTIME [PASS]* clock_id: CLOCK_BOOTTIME [PASS]* clock_id: CLOCK_TAI [PASS]* clock_id: CLOCK_REALTIME_COARSE [PASS]* clock_id: CLOCK_MONOTONIC [PASS]* clock_id: CLOCK_MONOTONIC_RAW [PASS]* clock_id: CLOCK_MONOTONIC_COARSE [PASS]","SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note
vdso_clock_getres.c: Sample code to test clock_getres.* Copyright (c) 2019 Arm Ltd.** Compile with:* gcc -std=gnu99 vdso_clock_getres.c** Tested on ARM, ARM64, MIPS32, x86 (32-bit and 64-bit),* Power (32-bit and 64-bit), S390x (32-bit and 64-bit).* Might work on other architectures.
This function calls clock_getres in vdso and by system call* with different values for clock_id.** Example of output:** clock_id: CLOCK_REALTIME [PASS]* clock_id: CLOCK_BOOTTIME [PASS]* clock_id: CLOCK_TAI [PASS]* clock_id: CLOCK_REALTIME_COARSE [PASS]* clock_id: CLOCK_MONOTONIC [PASS]* clock_id: CLOCK_MONOTONIC_RAW [PASS]* clock_id: CLOCK_MONOTONIC_COARSE [PASS]",['SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note'],GPL-2.0-only Linux-syscall-note,"SGI Free Software License B v2.0
mpi Permissive License
mpi Permissive License",1.0,100.0
15,15,25,linux-master/COPYING,GPL-2.0-only Linux-syscall-note,11,"Being under the terms of the GNU General Public License version 2 only,
In addition, other licenses may also apply. Please see:
All contributions to the Linux Kernel are subject to this COPYING file.
	LICENSES/preferred/GPL-2.0
for more details.
	Documentation/process/license-rules.rst
	LICENSES/exceptions/Linux-syscall-note
according with:
With an explicit syscall exception, as stated at:
	SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note","The Linux Kernel is provided under:

	SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note

Being under the terms of the GNU General Public License version 2 only,
according with:

	LICENSES/preferred/GPL-2.0

With an explicit syscall exception, as stated at:

	LICENSES/exceptions/Linux-syscall-note

In addition, other licenses may also apply. Please see:

	Documentation/process/license-rules.rst

for more details.

All contributions to the Linux Kernel are subject to this COPYING file.
","['SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note', 'Being under the terms of the GNU General Public License version 2 only,according with:', 'LICENSES/preferred/GPL-2.0']",GPL-2.0-only Linux-syscall-note,"GNU General Public License v1.0 or later
Historical Permission Notice and Disclaimer - Markus Kuhn variant
Linux man-pages Copyleft
Open Software License 2.0
Time::ParseDate License
Red Hat eCos Public License v1.1
Historical Permission Notice and Disclaimer with MIT disclaimer
Creative Commons Attribution Share Alike 2.1 Japan
mpi Permissive License
SGI Free Software License B v2.0",1.0,100.0
16,16,26,linux-master/tools/usb/usbip/src/usbip_detach.c,GPL-2.0-or-later,181,"Copyright (C) 2011 matt mooney <mfm@muteddisk.com>*               2005-2007 Takahiro Hirofuchi
SPDX-License-Identifier: GPL-2.0-or-later
remove the port state file
check for invalid port","SPDX-License-Identifier: GPL-2.0-or-later
Copyright (C) 2011 matt mooney <mfm@muteddisk.com>*               2005-2007 Takahiro Hirofuchi
check for invalid port
remove the port state file",['SPDX-License-Identifier: GPL-2.0-or-later'],GPL-2.0-or-later,"Soundex License
GL2PS License
eCos license version 2.0
The Parity Public License 7.0.0",1.0,100.0
17,17,27,linux-master/tools/testing/selftests/powerpc/alignment/copy_first_unaligned.c,GPL-2.0-or-later,1324,"SPDX-License-Identifier: GPL-2.0-or-later
Copyright 2016, Chris Smart, IBM Corporation.** Calls to copy_first which are not 128-byte aligned should be* caught and sent a SIGBUS.
Check that the signal was on the correct instruction, using a* mask because the compiler assigns the register at RB.
We hit the right instruction
Only run this test on a P9 or later
We should not get here
Register our signal handler with SIGBUS
+1 makes buf unaligned","SPDX-License-Identifier: GPL-2.0-or-later
Copyright 2016, Chris Smart, IBM Corporation.** Calls to copy_first which are not 128-byte aligned should be* caught and sent a SIGBUS.
Check that the signal was on the correct instruction, using a* mask because the compiler assigns the register at RB.
We hit the right instruction
Only run this test on a P9 or later
Register our signal handler with SIGBUS
+1 makes buf unaligned
We should not get here",['SPDX-License-Identifier: GPL-2.0-or-later'],GPL-2.0-or-later,"GL2PS License
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
SGI Free Software License B v2.0
Creative Commons Attribution Share Alike 2.1 Japan",1.0,100.0
18,18,28,linux-master/tools/testing/selftests/powerpc/security/branch_loops.S,GPL-2.0-or-later,1056," * Copyright 2019, Michael Ellerman, IBM Corp.
// SPDX-License-Identifier: GPL-2.0+
	.text
2:	b	1b
	li	r3, 1
	b	.Lloop
	li	r3, 0
	li	r3, 0
	state	4
#include <ppc-asm.h>","// SPDX-License-Identifier: GPL-2.0+

/*
 * Copyright 2019, Michael Ellerman, IBM Corp.
 */

#include <ppc-asm.h>

	.data

jump_table:
	.long	0x0
	.long	(.Lstate_1 - .Lstate_0)
	.long	(.Lstate_2 - .Lstate_0)
	.long	(.Lstate_3 - .Lstate_0)
	.long	(.Lstate_4 - .Lstate_0)
	.long	(.Lstate_5 - .Lstate_0)
	.long	(.Lstate_6 - .Lstate_0)
	.long	(.Lstate_7 - .Lstate_0)

	.text

#define ITER_SHIFT	31

.macro state number
	.balign	32
.Lstate_\number:
	.if	\number==7
	li	r3, 0
	.else
	li	r3, \number+1
	.endif
	b	.Lloop
.endm

FUNC_START(pattern_cache_loop)
	li	r3, 0
	li	r4, 1
	sldi	r4, r4, ITER_SHIFT

.Lloop:	cmpdi	r4, 0
	beqlr

	addi	r4, r4, -1

	ld	r6, jump_table@got(%r2)
	sldi	r5, r3, 2
	lwax	r6, r5, r6
	ld	r7, .Lstate_0@got(%r2)
	add	r6, r6, r7
	mtctr	r6
	bctr

	state	0
	state	1
	state	2
	state	3
	state	4
	state	5
	state	6
	state	7

FUNC_END(pattern_cache_loop)


FUNC_START(indirect_branch_loop)
	li	r3, 1
	sldi	r3, r3, ITER_SHIFT

1:	cmpdi	r3, 0
	beqlr

	addi	r3, r3, -1

	ld	r4, 2f@got(%r2)
	mtctr	r4
	bctr

	.balign 32
2:	b	1b

FUNC_END(indirect_branch_loop)
",['// SPDX-License-Identifier: GPL-2.0+'],GPL-2.0-or-later,"IBM PowerPC Initialization and Boot Software
SGI Free Software License B v2.0
Text-Tabs+Wrap License
diffmark license
LaTeX Project Public License v1.1
bzip2 and libbzip2 License v1.0.5
Open Software License 3.0
Open Software License 3.0
BSD-4-Clause (University of California-Specific)
snprintf License",1.0,100.0
19,19,29,linux-master/tools/testing/selftests/hid/tests/test_hid_core.py,GPL-2.0-or-later,2090," This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.
 Copyright (c) 2017 Benjamin Tissoires <benjamin.tissoires@gmail.com> Copyright (c) 2017 Red Hat, Inc.
 This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
 You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>.
 !/bin/env python3 SPDX-License-Identifier: GPL-2.0 -*- coding: utf-8 -*-
 .Usage Page (Generic Desktop) .Usage (Mouse) .Collection (Application) ..Usage (Mouse) ..Collection (Logical) ...Usage (Pointer) ...Collection (Physical) ....Usage Page (Button) ....Usage Minimum (1) ....Usage Maximum (3) ....Logical Minimum (0) ....Logical Maximum (1) ....Report Size (1) ....Report Count (3) ....Input (Data,Var,Abs) ....Report Size (5) ....Report Count (1) ....Input (Cnst,Var,Abs) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) .....Usage Page (Generic Desktop) .....Usage (X) .....Usage (Y) .....Logical Minimum (-127) .....Logical Maximum (127) .....Report Size (8) .....Report Count (2) .....Input (Data,Var,Rel) ...Collection (Logical) ....Report ID (18) ....Usage (Resolution Multiplier) ....Report Count (1) ....Report Size (2) ....Logical Minimum (0) ....Logical Maximum (1) ....Physical Minimum (1) ....Physical Maximum (12) ....Feature (Data,Var,Abs) ....Report ID (26) ....Usage (Wheel) ....Physical Minimum (0) ....Physical Maximum (0) ....Report Count (1) ....Report Size (16) ....Logical Minimum (-32767) ....Logical Maximum (32767) ....Input (Data,Var,Rel) ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ..End Collection .End Collection
This is for generic devices
This test can only check for negatives. If the kernel crashes, you         know why. If this test passes, either the bug isn't present or just         didn't get triggered. No way to know.          For an explanation, see kernel patch             HID: core: replace the collection tree pointers with indices
fmt: off
fmt: on","This is for generic devices
fmt: off
fmt: on
 !/bin/env python3 SPDX-License-Identifier: GPL-2.0 -*- coding: utf-8 -*-
 Copyright (c) 2017 Benjamin Tissoires <benjamin.tissoires@gmail.com> Copyright (c) 2017 Red Hat, Inc.
 This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.
 This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.
 You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>.
 .Usage Page (Generic Desktop) .Usage (Mouse) .Collection (Application) ..Usage (Mouse) ..Collection (Logical) ...Usage (Pointer) ...Collection (Physical) ....Usage Page (Button) ....Usage Minimum (1) ....Usage Maximum (3) ....Logical Minimum (0) ....Logical Maximum (1) ....Report Size (1) ....Report Count (3) ....Input (Data,Var,Abs) ....Report Size (5) ....Report Count (1) ....Input (Cnst,Var,Abs) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) ....Collection (Logical) .....Usage (Pointer) .....Usage Page (Generic Desktop) .....Usage (X) .....Usage (Y) .....Logical Minimum (-127) .....Logical Maximum (127) .....Report Size (8) .....Report Count (2) .....Input (Data,Var,Rel) ...Collection (Logical) ....Report ID (18) ....Usage (Resolution Multiplier) ....Report Count (1) ....Report Size (2) ....Logical Minimum (0) ....Logical Maximum (1) ....Physical Minimum (1) ....Physical Maximum (12) ....Feature (Data,Var,Abs) ....Report ID (26) ....Usage (Wheel) ....Physical Minimum (0) ....Physical Maximum (0) ....Report Count (1) ....Report Size (16) ....Logical Minimum (-32767) ....Logical Maximum (32767) ....Input (Data,Var,Rel) ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ...End Collection ..End Collection .End Collection
Test class to test re-allocation of the HID collection stack in     hid-core.c.
This test can only check for negatives. If the kernel crashes, you         know why. If this test passes, either the bug isn't present or just         didn't get triggered. No way to know.          For an explanation, see kernel patch             HID: core: replace the collection tree pointers with indices","['This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.', 'You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>.', '!/bin/env python3 SPDX-License-Identifier: GPL-2.0 -*- coding: utf-8 -*-', 'his program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.']",GPL-2.0-or-later,"XSkat License
Time::ParseDate License
Jam License
Checkmk License
Python License 2.0.1
mpi Permissive License
AMD's plpa_map.c License
Gutmann License
McPhee Slideshow License
McPhee Slideshow License",1.0,100.0
20,20,30,linux-master/tools/testing/selftests/powerpc/nx-gzip/include/nxu.h,GPL-2.0-or-later,1208,"SPDX-License-Identifier: GPL-2.0-or-later
Hardware interface of the NX-GZIP compression accelerator** Copyright (C) IBM Corporation, 2020** Author: Bulent Abali <abali@us.ibm.com>
Valid bit. v must be set to 0 by the program* before submitting the coprocessor command.* Software can poll for the v bit
Completion extension ce(0) ce(1) ce(2).  Bits ce(3-7)* unused.  Section 6.6 Figure 6.7.
byte[16:31]
byte[16:31]
append 10 bits 0000001b 00...... ;* assumes appending starts on a byte boundary; b is the final bit.
ce completion extension
byte[48:63]
byte[48:63]","SPDX-License-Identifier: GPL-2.0-or-later
Hardware interface of the NX-GZIP compression accelerator** Copyright (C) IBM Corporation, 2020** Author: Bulent Abali <abali@us.ibm.com>
deflate
nx
util
Definitions of acronyms used here. See* P9 NX Gzip Accelerator User's Manual for details:* https://github.com/libnxz/power-gzip/blob/develop/doc/power_nx_gzip_um.pdf** adler/crc: 32 bit checksums appended to stream tail* ce:       completion extension* cpb:      coprocessor parameter block (metadata)* crb:      coprocessor request block (command)* csb:      coprocessor status block (status)* dht:      dynamic huffman table* dde:      data descriptor element (address, length)* ddl:      list of ddes* dh/fh:    dynamic and fixed huffman types* fc:       coprocessor function code* histlen:  history/dictionary length* history:  sliding window of up to 32KB of data* lzcount:  Deflate LZ symbol counts* rembytecnt: remaining byte count* sfbt:     source final block type; last block's type during decomp* spbc:     source processed byte count* subc:     source unprocessed bit count* tebc:     target ending bit count; valid bits in the last byte* tpbc:     target processed byte count* vas:      virtual accelerator switch; the user mode interface
Note: NX registers with fewer than 32 bits are declared by* convention as uint32_t variables in unions. If *_offset and *_mask* are defined for a variable, then use get_ put_ macros to* conveniently access the register fields for endian conversions.
Data Descriptor Element, Section 6.4
When dde_count == 0 ddead is a pointer to a data buffer;* ddebc is the buffer length bytes.* When dde_count > 0 dde is an indirect dde; ddead is a* pointer to a contiguous list of direct ddes; ddebc is the* total length of all data pointed to by the list of direct* ddes. Note that only one level of indirection is permitted.* See Section 6.4 of the user manual for additional details.
dde byte count
dde address
Coprocessor Status Block, Section 6.6
Valid bit. v must be set to 0 by the program* before submitting the coprocessor command.* Software can poll for the v bit
16B CSB size. Written to 0 by DMA when it writes the CPB
cs completion sequence; unused
cc completion code; cc != 0 exception occurred
ce completion extension
target processed byte count TPBC
Section 6.12.1 CSB NonZero error summary.  FSA Failing storage* address.  Address where error occurred. When available, written* to A field of CSB
Coprocessor Completion Block, Section 6.7
When crb.c==0 (no ccb defined) it is reserved;* When crb.c==1 (ccb defined) it is cm
Signal interrupt of crb.c==1 and cm==1
generic access to the 32bit word
CRB operand of the paste coprocessor instruction is stamped* in quadword 4 with the information shown here as its written* in to the receive FIFO of the coprocessor
Verification only vas buffer number which correlates to* the low order bits of the atag in the paste command
Pointer to Send Window Context that provides for NX address* translation information, such as MSR and LPCR bits, job* completion interrupt RA, PSWID, and job utilization counter.
Pointer to Receive Window Context. NX uses this to return* credits to a Receive FIFO as entries are dequeued.
Invalid bit. If this bit is 1 the CRB is discarded by* NX upon fetching from the receive FIFO. If this bit is 0* the CRB is processed normally. The bit is stamped to 0* by VAS and may be written to 1 by hypervisor while* the CRB is in the receive FIFO (in memory).
A CRB that has a translation fault is stamped by NX in quadword 4* and pasted to the Fault Send Window in VAS.
Coprocessor Parameter Block In/Out are used to pass metadata* to/from accelerator.  Tables 6.5 and 6.6 of the user manual.
CPBInput
bits 0:31
bits 32:63
bits 64:75
bits 93:95
bits 108:111
bits 112:127
bits 116:127
qw[1:18]
byte access
qw[19:23]
CPBOutput
bits 0:31  qw[24]
bits 32:63 qw[24]
bits 77:79 qw[24]
bits 80:95 qw[24]
bits 108:111 qw[24]
bits 112:127 qw[24]
bits 116:127 qw[24]
qw[25:103]
qw[25] compress no lzcounts or wrap
qw[25] wrap
qw[25] compress no lzcounts
286 LL and 30 D symbol counts
qw[25:42]
qw[43] decompress
qw[104] compress with lzcounts
byte[0:3]
bits[24-31]
byte[4:7]
byte[8:15]
c==0 no ccb defined
at==0 address type is ignored;* all addrs effective assumed.
byte[16:31]
byte[32:47]
byte[48:63]
byte[64:239] shift csb by 128 bytes out of the crb; csb was* in crb earlier; JReilly says csb written with partial inject
byte[64:79]
NX hardware convention has the msb bit on the left numbered 0.* The defines below has *_offset defined as the right most bit* position of a field.  x of size_mask(x) is the field width in bits.
Offsets and Widths within the containing 32 bits of the various NX* gzip hardware registers.  Use the getnn/putnn macros to access* these regs
CSB
CCB
VAS stamped CRB fields
NX stamped fault CRB fields
CPB input
CPB output
CRB
mask off bottom 4b
Access macros for the registers.  Do not access registers directly* because of the endian conversion.  P9 processor may run either as* Little or Big endian. However the NX coprocessor regs are always* big endian.* Use the 32 and 64b macros to access respective* register sizes.* Use nn forms for the register fields shorter than 32 bits.
get 32bits less the REG field
get 32bits less the REG field
Completion extension ce(0) ce(1) ce(2).  Bits ce(3-7)* unused.  Section 6.6 Figure 6.7.
termination, output buffers may be modified, SPBC/TPBC invalid Fig.6-7
if not terminated then check full or partial completion
TPBC indicates successfully stored data count
most error CEs have CE(0)=0 and CE(1)=1
some CC=3 are partially completed, Table 6-8
Compression: when TPBC>SPBC then CC=64 Table 6-8; target didn't* compress smaller than source.
Decompress SFBT combinations Tables 5-3, 6-4, 6-6
NX gzip function codes. Table 6.2.* Bits 0:4 are the FC. Bit 5 is used by the DMA controller to* select one of the two Byte Count Limits.
CSB.CC Error codes
initial values for non-resume operations
crc32(0L, Z_NULL, 0)
adler32(0L, Z_NULL, 0)  adler is initialized to 1
prototypes
caller supplies a print buffer 4*sizeof(crb)
NX_SIM
Deflate stream manipulation
append 10 bits 0000001b 00...... ;* assumes appending starts on a byte boundary; b is the final bit.
842 Engine
byte[0:3]
bits[29-31]
byte[4:7]
byte[8:15]
c==0 no ccb defined
at==0 address type is ignored;* all addrs effective assumed.
byte[16:31]
byte[32:47]
byte[48:63]
byte[64:96]
842 CRB
NX_842
_NXU_H",['SPDX-License-Identifier: GPL-2.0-or-later'],GPL-2.0-or-later,"GL2PS License
Info-ZIP License
OpenPBS v2.3 Software License
mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
Computer Associates Trusted Open Source License 1.1
mpi Permissive License
mpi Permissive License",1.0,100.0
21,21,31,linux-master/tools/testing/selftests/futex/functional/run.sh,GPL-2.0-or-later,2120,"Copyright © International Business Machines  Corp., 2009
 AUTHOR Darren Hart <dvhart@linux.intel.com>
 !/bin/sh SPDX-License-Identifier: GPL-2.0-or-later
 HISTORY 2009-Nov-9: Initial version by Darren Hart <dvhart@linux.intel.com> 2010-Jan-6: Add futex_wait_uninitialized_heap and futex_wait_private_mapped_file by KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
 DESCRIPTION Run tests in the current directory.
with timeouts
with long timeout
 requeue pi testing without timeouts
Test for a color capable console","Copyright © International Business Machines  Corp., 2009
Test for a color capable console
with timeouts
with long timeout
 !/bin/sh SPDX-License-Identifier: GPL-2.0-or-later
 DESCRIPTION Run tests in the current directory.
 AUTHOR Darren Hart <dvhart@linux.intel.com>
 HISTORY 2009-Nov-9: Initial version by Darren Hart <dvhart@linux.intel.com> 2010-Jan-6: Add futex_wait_uninitialized_heap and futex_wait_private_mapped_file by KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
 requeue pi testing without timeouts",[' !/bin/sh SPDX-License-Identifier: GPL-2.0-or-later'],GPL-2.0-or-later,"Hewlett-Packard 1989 License
Historical Permission Notice and Disclaimer - sell regexpr variant
The Parity Public License 7.0.0
Historical Permission Notice and Disclaimer - Intel variant
Gutmann License
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
Open Group Test Suite License
Creative Commons Attribution Share Alike 2.1 Japan",1.0,100.0
22,22,32,linux-master/tools/testing/selftests/powerpc/benchmarks/exec_target.c,GPL-2.0-or-later,1319,"SPDX-License-Identifier: GPL-2.0+
Part of fork context switch microbenchmark.** Copyright 2018, Anton Blanchard, IBM Corp.","SPDX-License-Identifier: GPL-2.0+
Part of fork context switch microbenchmark.** Copyright 2018, Anton Blanchard, IBM Corp.",['SPDX-License-Identifier: GPL-2.0+'],GPL-2.0-or-later,"SGI Free Software License B v2.0
snprintf License",1.0,100.0
23,23,33,linux-master/tools/testing/selftests/rcutorture/bin/jitterstart.sh,GPL-2.0-or-later,903,"Authors: Paul E. McKenney <paulmck@kernel.org>
Copyright (C) 2021 Facebook, Inc.
 !/bin/bash SPDX-License-Identifier: GPL-2.0+
Start up the specified number of jitter.sh scripts in the background.
Usage: . jitterstart.sh n jittering-dir duration [ sleepmax [ spinmax ] ]
 n: Number of jitter.sh scripts to start up. jittering-dir: Directory in which to put ""jittering"" file. duration: Time to run in seconds. sleepmax: Maximum microseconds to sleep, defaults to one second. spinmax: Maximum microseconds to spin, defaults to one millisecond.","Start up the specified number of jitter.sh scripts in the background.
Usage: . jitterstart.sh n jittering-dir duration [ sleepmax [ spinmax ] ]
Copyright (C) 2021 Facebook, Inc.
Authors: Paul E. McKenney <paulmck@kernel.org>
 !/bin/bash SPDX-License-Identifier: GPL-2.0+
 n: Number of jitter.sh scripts to start up. jittering-dir: Directory in which to put ""jittering"" file. duration: Time to run in seconds. sleepmax: Maximum microseconds to sleep, defaults to one second. spinmax: Maximum microseconds to spin, defaults to one millisecond.",[' !/bin/bash SPDX-License-Identifier: GPL-2.0+'],GPL-2.0-or-later,"Kastrup License
FSF Unlimited License (with License Retention)
The Parity Public License 7.0.0
mpi Permissive License
mpi Permissive License
mpi Permissive License",1.0,100.0
24,24,34,linux-master/tools/testing/selftests/powerpc/include/fpu_asm.h,GPL-2.0-or-later,1264,"Copyright 2016, Cyril Bur, IBM Corp.
SPDX-License-Identifier: GPL-2.0-or-later
Careful calling this, it will 'clobber' fpu (by design)* Don't call this from C
_SELFTESTS_POWERPC_FPU_ASM_H","SPDX-License-Identifier: GPL-2.0-or-later
Copyright 2016, Cyril Bur, IBM Corp.
Careful calling this, it will 'clobber' fpu (by design)* Don't call this from C
_SELFTESTS_POWERPC_FPU_ASM_H",['SPDX-License-Identifier: GPL-2.0-or-later'],GPL-2.0-or-later,"IBM PowerPC Initialization and Boot Software
GL2PS License
Gutmann License
Creative Commons Attribution Share Alike 2.1 Japan",1.0,100.0
25,25,35,linux-master/tools/testing/selftests/powerpc/include/vsx_asm.h,GPL-2.0-or-later,1256,"Copyright 2015, Cyril Bur, IBM Corp.
SPDX-License-Identifier: GPL-2.0-or-later
Careful this will 'clobber' vsx (by design), VSX are always* volatile though so unlike vmx this isn't so much of an issue* Still should avoid calling from C","SPDX-License-Identifier: GPL-2.0-or-later
Copyright 2015, Cyril Bur, IBM Corp.
Careful this will 'clobber' vsx (by design), VSX are always* volatile though so unlike vmx this isn't so much of an issue* Still should avoid calling from C",['SPDX-License-Identifier: GPL-2.0-or-later'],GPL-2.0-or-later,"IBM PowerPC Initialization and Boot Software
GL2PS License
Gutmann License",1.0,100.0
26,26,36,linux-master/tools/usb/usbip/INSTALL,FSFUL,164,"Copyright (C) 1994, 1995, 1996, 1999, 2000, 2001, 2002, 2004, 2005,
================
=================
==================
==================
==================
unlimited permission to copy, distribute and modify it.
=====================
======================
==========================","Installation Instructions
*************************

Copyright (C) 1994, 1995, 1996, 1999, 2000, 2001, 2002, 2004, 2005,
2006, 2007 Free Software Foundation, Inc.

This file is free documentation; the Free Software Foundation gives
unlimited permission to copy, distribute and modify it.

Basic Installation
==================

Briefly, the shell commands `./configure; make; make install' should
configure, build, and install this package.  The following
more-detailed instructions are generic; see the `README' file for
instructions specific to this package.

   The `configure' shell script attempts to guess correct values for
various system-dependent variables used during compilation.  It uses
those values to create a `Makefile' in each directory of the package.
It may also create one or more `.h' files containing system-dependent
definitions.  Finally, it creates a shell script `config.status' that
you can run in the future to recreate the current configuration, and a
file `config.log' containing compiler output (useful mainly for
debugging `configure').

   It can also use an optional file (typically called `config.cache'
and enabled with `--cache-file=config.cache' or simply `-C') that saves
the results of its tests to speed up reconfiguring.  Caching is
disabled by default to prevent problems with accidental use of stale
cache files.

   If you need to do unusual things to compile the package, please try
to figure out how `configure' could check whether to do them, and mail
diffs or instructions to the address given in the `README' so they can
be considered for the next release.  If you are using the cache, and at
some point `config.cache' contains results you don't want to keep, you
may remove or edit it.

   The file `configure.ac' (or `configure.in') is used to create
`configure' by a program called `autoconf'.  You need `configure.ac' if
you want to change it or regenerate `configure' using a newer version
of `autoconf'.

The simplest way to compile this package is:

  1. `cd' to the directory containing the package's source code and type
     `./configure' to configure the package for your system.

     Running `configure' might take a while.  While running, it prints
     some messages telling which features it is checking for.

  2. Type `make' to compile the package.

  3. Optionally, type `make check' to run any self-tests that come with
     the package.

  4. Type `make install' to install the programs and any data files and
     documentation.

  5. You can remove the program binaries and object files from the
     source code directory by typing `make clean'.  To also remove the
     files that `configure' created (so you can compile the package for
     a different kind of computer), type `make distclean'.  There is
     also a `make maintainer-clean' target, but that is intended mainly
     for the package's developers.  If you use it, you may have to get
     all sorts of other programs in order to regenerate files that came
     with the distribution.

  6. Often, you can also type `make uninstall' to remove the installed
     files again.

Compilers and Options
=====================

Some systems require unusual options for compilation or linking that the
`configure' script does not know about.  Run `./configure --help' for
details on some of the pertinent environment variables.

   You can give `configure' initial values for configuration parameters
by setting variables in the command line or in the environment.  Here
is an example:

     ./configure CC=c99 CFLAGS=-g LIBS=-lposix

   *Note Defining Variables::, for more details.

Compiling For Multiple Architectures
====================================

You can compile the package for more than one kind of computer at the
same time, by placing the object files for each architecture in their
own directory.  To do this, you can use GNU `make'.  `cd' to the
directory where you want the object files and executables to go and run
the `configure' script.  `configure' automatically checks for the
source code in the directory that `configure' is in and in `..'.

   With a non-GNU `make', it is safer to compile the package for one
architecture at a time in the source code directory.  After you have
installed the package for one architecture, use `make distclean' before
reconfiguring for another architecture.

Installation Names
==================

By default, `make install' installs the package's commands under
`/usr/local/bin', include files under `/usr/local/include', etc.  You
can specify an installation prefix other than `/usr/local' by giving
`configure' the option `--prefix=PREFIX'.

   You can specify separate installation prefixes for
architecture-specific files and architecture-independent files.  If you
pass the option `--exec-prefix=PREFIX' to `configure', the package uses
PREFIX as the prefix for installing programs and libraries.
Documentation and other data files still use the regular prefix.

   In addition, if you use an unusual directory layout you can give
options like `--bindir=DIR' to specify different values for particular
kinds of files.  Run `configure --help' for a list of the directories
you can set and what kinds of files go in them.

   If the package supports it, you can cause programs to be installed
with an extra prefix or suffix on their names by giving `configure' the
option `--program-prefix=PREFIX' or `--program-suffix=SUFFIX'.

Optional Features
=================

Some packages pay attention to `--enable-FEATURE' options to
`configure', where FEATURE indicates an optional part of the package.
They may also pay attention to `--with-PACKAGE' options, where PACKAGE
is something like `gnu-as' or `x' (for the X Window System).  The
`README' should mention any `--enable-' and `--with-' options that the
package recognizes.

   For packages that use the X Window System, `configure' can usually
find the X include and library files automatically, but if it doesn't,
you can use the `configure' options `--x-includes=DIR' and
`--x-libraries=DIR' to specify their locations.

Specifying the System Type
==========================

There may be some features `configure' cannot figure out automatically,
but needs to determine by the type of machine the package will run on.
Usually, assuming the package is built to be run on the _same_
architectures, `configure' can figure that out, but if it prints a
message saying it cannot guess the machine type, give it the
`--build=TYPE' option.  TYPE can either be a short name for the system
type, such as `sun4', or a canonical name which has the form:

     CPU-COMPANY-SYSTEM

where SYSTEM can have one of these forms:

     OS KERNEL-OS

   See the file `config.sub' for the possible values of each field.  If
`config.sub' isn't included in this package, then this package doesn't
need to know the machine type.

   If you are _building_ compiler tools for cross-compiling, you should
use the option `--target=TYPE' to select the type of system they will
produce code for.

   If you want to _use_ a cross compiler, that generates code for a
platform different from the build platform, you should specify the
""host"" platform (i.e., that on which the generated programs will
eventually be run) with `--host=TYPE'.

Sharing Defaults
================

If you want to set default values for `configure' scripts to share, you
can create a site shell script called `config.site' that gives default
values for variables like `CC', `cache_file', and `prefix'.
`configure' looks for `PREFIX/share/config.site' if it exists, then
`PREFIX/etc/config.site' if it exists.  Or, you can set the
`CONFIG_SITE' environment variable to the location of the site script.
A warning: not all `configure' scripts look for a site script.

Defining Variables
==================

Variables not defined in a site shell script can be set in the
environment passed to `configure'.  However, some packages may run
configure again during the build, and the customized values of these
variables may be lost.  In order to avoid this problem, you should set
them in the `configure' command line, using `VAR=value'.  For example:

     ./configure CC=/usr/local2/bin/gcc

causes the specified `gcc' to be used as the C compiler (unless it is
overridden in the site shell script).

Unfortunately, this technique does not work for `CONFIG_SHELL' due to
an Autoconf bug.  Until the bug is fixed you can use this workaround:

     CONFIG_SHELL=/bin/bash /bin/bash ./configure CONFIG_SHELL=/bin/bash

`configure' Invocation
======================

`configure' recognizes the following options to control how it operates.

`--help'
`-h'
     Print a summary of the options to `configure', and exit.

`--version'
`-V'
     Print the version of Autoconf used to generate the `configure'
     script, and exit.

`--cache-file=FILE'
     Enable the cache: use and save the results of the tests in FILE,
     traditionally `config.cache'.  FILE defaults to `/dev/null' to
     disable caching.

`--config-cache'
`-C'
     Alias for `--cache-file=config.cache'.

`--quiet'
`--silent'
`-q'
     Do not print messages saying which checks are being made.  To
     suppress all normal output, redirect it to `/dev/null' (any error
     messages will still be shown).

`--srcdir=DIR'
     Look for the package's source code in directory DIR.  Usually
     `configure' can determine that directory automatically.

`configure' also accepts some other, not widely useful, options.  Run
`configure --help' for more details.

","['This file is free documentation; the Free Software Foundation gives unlimited permission to copy, distribute and modify it.']",,"FSF Unlimited License (With License Retention and Warranty Disclaimer)
mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
TermReadKey License
mpi Permissive License
mpi Permissive License
mpi Permissive License",0.0,0.0
27,27,37,linux-master/tools/usb/ffs-aio-example/multibuff/device_app/aio_multibuff.c,Unlicense MIT-style,218,"This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>
free resources
we are waiting for function ENABLE
""/ep#"" */ + 1 /* '\0'
en-us
Buffer structure
we read aio events
Descriptors and Strings
if we got events
for endian.h","This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>
for endian.h
Descriptors and Strings
en-us
Buffer structure
Endpoints handling
""/ep#"" */ + 1 /* '\0'
open endpoint files
setup aio context to handle up to AIO_MAX requests
we are waiting for function ENABLE
when we're preparing new data to submit,* second buffer being transmitted
prepare requests
enable eventfd notification
submit table of requests
if event is ready to read
we read aio events
if we got events
if all req's from iocb completed
free resources","['This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>', 'SPDX-License-Identifier: GPL-2.0-only']",Unlicense MIT-style,"The Unlicense
App::s2p License
Creative Commons Attribution Share Alike 2.1 Japan
mpi Permissive License
mpi Permissive License
Text-Tabs+Wrap License
Creative Commons Attribution Share Alike 2.1 Japan
Gutmann License
Creative Commons Attribution Share Alike 2.1 Japan
eCos license version 2.0",1.0,50.0
28,28,38,linux-master/tools/usb/usbip/vudc/vudc_server_example.sh,Unlicense MIT-style,171," Anyone is free to copy, modify, publish, use, compile, sell, or distribute this software, either in source code form or as a compiled binary, for any purpose, commercial or non-commercial, and by any means.
 THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
This is free and unencumbered software released into the public domain.
 In jurisdictions that recognize copyright laws, the author or authors of this software dedicate any and all copyright interest in the software to the public domain. We make this dedication for the benefit of the public at large and to the detriment of our heirs and successors. We intend this dedication to be an overt act of relinquishment in perpetuity of all present and future rights to this software under copyright law.
For more information, please refer to <https://unlicense.org/>
 Create your USB gadget You may use bare ConfigFS interface (as below) or libusbgx or gt toool Instead of ConfigFS gadgets you may use any of legacy gadgets.
 $ modprobe usbip-vhci $ usbip list -r $SERVER_IP Exportable USB devices ====================== usbipd: info: request 0x8005(6): complete - 127.0.0.1 usbip-vudc.0: Linux Foundation : unknown product (1d6b:0104) : /sys/devices/platform/usbip-vudc.0 : (Defined at Interface level) (00/00/00)
To attach this device to your client you may use:
This is a sample script which shows how to use vUDC with ConfigFS gadgets
 Now your USB gadget is available using USB/IP protocol. To prepare your client, you should ensure that usbip-vhci module is inside your kernel. If it's not then you can load it:","!/bin/bash
This is free and unencumbered software released into the public domain.
For more information, please refer to <https://unlicense.org/>
This is a sample script which shows how to use vUDC with ConfigFS gadgets
Stop script on error
Create a new USB gadget
This gadget contains one function - ACM (serial port over USB)
Just one configuration
Set our gadget identity
Let's now run our usbip daemon in a USB device mode
$ modprobe usbip-vhci
To attach this device to your client you may use:
$ usbip attach -r $SERVER_IP -d usbip-vudc.0
 Anyone is free to copy, modify, publish, use, compile, sell, or distribute this software, either in source code form or as a compiled binary, for any purpose, commercial or non-commercial, and by any means.
 In jurisdictions that recognize copyright laws, the author or authors of this software dedicate any and all copyright interest in the software to the public domain. We make this dedication for the benefit of the public at large and to the detriment of our heirs and successors. We intend this dedication to be an overt act of relinquishment in perpetuity of all present and future rights to this software under copyright law.
 THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 Create your USB gadget You may use bare ConfigFS interface (as below) or libusbgx or gt toool Instead of ConfigFS gadgets you may use any of legacy gadgets.
 Load vudc-module if vudc is not available You may change value of num param to get more than one vUDC instance
 Bind gadget to our vUDC By default we bind to first one but you may change this if you would like to use more than one instance
 Now your USB gadget is available using USB/IP protocol. To prepare your client, you should ensure that usbip-vhci module is inside your kernel. If it's not then you can load it:
 To check availability of your gadget you may try to list devices exported on a remote server:
 $ modprobe usbip-vhci $ usbip list -r $SERVER_IP Exportable USB devices ====================== usbipd: info: request 0x8005(6): complete - 127.0.0.1 usbip-vudc.0: Linux Foundation : unknown product (1d6b:0104) : /sys/devices/platform/usbip-vudc.0 : (Defined at Interface level) (00/00/00)","['This is free and unencumbered software released into the public domain.', 'For more information, please refer to <https://unlicense.org/>', ' Anyone is free to copy, modify, publish, use, compile, sell, or distribute this software, either in source code form or as a compiled binary, for any purpose, commercial or non-commercial, and by any means.', ' In jurisdictions that recognize copyright laws, the author or authors of this software dedicate any and all copyright interest in the software to the public domain. We make this dedication for the benefit of the public at large and to the detriment of our heirs and successors. We intend this dedication to be an overt act of relinquishment in perpetuity of all present and future rights to this software under copyright law.', ' THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.']",Unlicense MIT-style,"The Unlicense
Historical Permission Notice and Disclaimer - sell xserver variant with MIT disclaimer
The Unlicense
Creative Commons Public Domain Dedication and Certification
The Unlicense
HPND sell variant with MIT disclaimer
Solderpad Hardware License v0.5
HPND sell variant with MIT disclaimer
mpi Permissive License
Solderpad Hardware License v0.5",1.0,100.0
29,29,39,linux-master/tools/usb/ffs-aio-example/simple/host_app/test.c,MIT-style Unlicense,209,"This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>
struct test_state - describes test program state* @list: list of devices returned by libusb_get_device_list function* @found: pointer to struct describing tested device* @ctx: context, set to NULL* @handle: handle of tested device* @attached: indicates that device was attached to kernel, and has to be*            reattached at the end of test program
test_exit - cleanup test program
test_init - initialize test program","This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>
struct test_state - describes test program state* @list: list of devices returned by libusb_get_device_list function* @found: pointer to struct describing tested device* @ctx: context, set to NULL* @handle: handle of tested device* @attached: indicates that device was attached to kernel, and has to be*            reattached at the end of test program
test_init - initialize test program
test_exit - cleanup test program","['This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>', 'SPDX-License-Identifier: GPL-2.0-only']",MIT-style Unlicense,"The Unlicense
Common Public Attribution License 1.0
OpenPBS v2.3 Software License
Open Group Test Suite License",1.0,50.0
30,30,40,linux-master/tools/usb/ffs-aio-example/simple/device_app/aio_simple.c,MIT-style Unlicense,212,"This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>
free resources
we are waiting for function ENABLE
$(CROSS_COMPILE)cc -g -o aio_simple aio_simple.c -laio
cpu_to_le16/32 are used when initializing structures, a context where a* function call is not allowed. To solve this, we code cpu_to_le16/32 in a way* that allows them to be used when initializing structures.
""/ep#"" */ + 1 /* '\0'
if OUT transfer not requested
prepare write request
en-us
prepare read request","This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>
$(CROSS_COMPILE)cc -g -o aio_simple aio_simple.c -laio
for endian.h
cpu_to_le16/32 are used when initializing structures, a context where a* function call is not allowed. To solve this, we code cpu_to_le16/32 in a way* that allows them to be used when initializing structures.
Descriptors and Strings
en-us
Endpoints handling
""/ep#"" */ + 1 /* '\0'
open endpoint files
setup aio context to handle up to 2 requests
alloc buffers and requests
we are waiting for function ENABLE
if something was submitted we wait for event
we wait for one event
if we got event
if IN transfer not requested
prepare write request
enable eventfd notification
submit table of requests
if ret > 0 request is queued
if OUT transfer not requested
prepare read request
enable eventfs notification
submit table of requests
if ret > 0 request is queued
free resources","['This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** SPDX-License-Identifier: GPL-2.0-only']",MIT-style Unlicense,"The Unlicense
App::s2p License
Creative Commons Attribution Share Alike 2.1 Japan
GNU General Public License v3.0 w/GCC Runtime Library exception
GNU General Public License v3.0 w/GCC Runtime Library exception
mpi Permissive License
check-cvs License
Linux man-pages - 1 paragraph
mpi Permissive License
Linux man-pages - 1 paragraph",0.0,0.0
31,31,41,linux-master/tools/usb/ffs-aio-example/multibuff/host_app/test.c,MIT-style Unlicense,215,"This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>
struct test_state - describes test program state* @list: list of devices returned by libusb_get_device_list function* @found: pointer to struct describing tested device* @ctx: context, set to NULL* @handle: handle of tested device* @attached: indicates that device was attached to kernel, and has to be*            reattached at the end of test program
test_exit - cleanup test program
test_init - initialize test program","This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>
struct test_state - describes test program state* @list: list of devices returned by libusb_get_device_list function* @found: pointer to struct describing tested device* @ctx: context, set to NULL* @handle: handle of tested device* @attached: indicates that device was attached to kernel, and has to be*            reattached at the end of test program
test_init - initialize test program
test_exit - cleanup test program","['This is free and unencumbered software released into the public domain.** Anyone is free to copy, modify, publish, use, compile, sell, or* distribute this software, either in source code form or as a compiled* binary, for any purpose, commercial or non-commercial, and by any* means.** In jurisdictions that recognize copyright laws, the author or authors* of this software dedicate any and all copyright interest in the* software to the public domain. We make this dedication for the benefit* of the public at large and to the detriment of our heirs and* successors. We intend this dedication to be an overt act of* relinquishment in perpetuity of all present and future rights to this* software under copyright law.** THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,* EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF* MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.* IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR* OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,* ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR* OTHER DEALINGS IN THE SOFTWARE.** For more information, please refer to <http://unlicense.org/>', 'SPDX-License-Identifier: GPL-2.0-only']",MIT-style Unlicense,"The Unlicense
Common Public Attribution License 1.0
OpenPBS v2.3 Software License
Open Group Test Suite License",1.0,50.0
32,32,42,linux-master/tools/thermal/lib/uptimeofday.h,LGPL-2.1-or-later,271,"Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>
SPDX-License-Identifier: LGPL-2.1+","SPDX-License-Identifier: LGPL-2.1+
Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>",['SPDX-License-Identifier: LGPL-2.1+'],LGPL-2.1-or-later,"Time::ParseDate License
Transitive Grace Period Public Licence 1.0",1.0,100.0
33,33,43,linux-master/tools/thermal/lib/uptimeofday.c,LGPL-2.1-or-later,272," SPDX-License-Identifier: LGPL-2.1+ Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>"," SPDX-License-Identifier: LGPL-2.1+ Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>","[' SPDX-License-Identifier: LGPL-2.1+ Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>']",LGPL-2.1-or-later,GL2PS License,1.0,100.0
34,34,44,linux-master/tools/thermal/lib/log.h,LGPL-2.1-or-later,277,"Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>
SPDX-License-Identifier: LGPL-2.1+","SPDX-License-Identifier: LGPL-2.1+
Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>",['SPDX-License-Identifier: LGPL-2.1+'],LGPL-2.1-or-later,"Time::ParseDate License
Transitive Grace Period Public Licence 1.0",1.0,100.0
35,35,45,linux-master/tools/thermal/lib/thermal-tools.h,LGPL-2.1-or-later,273,"Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>
SPDX-License-Identifier: LGPL-2.1+","SPDX-License-Identifier: LGPL-2.1+
Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>",['SPDX-License-Identifier: LGPL-2.1+'],LGPL-2.1-or-later,"Time::ParseDate License
Transitive Grace Period Public Licence 1.0",1.0,100.0
36,36,46,linux-master/tools/thermal/lib/mainloop.c,LGPL-2.1-or-later,276," SPDX-License-Identifier: LGPL-2.1+ Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>"," SPDX-License-Identifier: LGPL-2.1+ Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>","[' SPDX-License-Identifier: LGPL-2.1+ Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>']",LGPL-2.1-or-later,GL2PS License,1.0,100.0
37,37,47,linux-master/tools/thermal/lib/mainloop.h,LGPL-2.1-or-later,275,"Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>
SPDX-License-Identifier: LGPL-2.1+","SPDX-License-Identifier: LGPL-2.1+
Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>",['SPDX-License-Identifier: LGPL-2.1+'],LGPL-2.1-or-later,"Time::ParseDate License
Transitive Grace Period Public Licence 1.0",1.0,100.0
38,38,48,linux-master/tools/thermal/lib/log.c,LGPL-2.1-or-later,278," SPDX-License-Identifier: LGPL-2.1+ Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>"," SPDX-License-Identifier: LGPL-2.1+ Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>","[' SPDX-License-Identifier: LGPL-2.1+ Copyright (C) 2022, Linaro Ltd - Daniel Lezcano <daniel.lezcano@linaro.org>']",LGPL-2.1-or-later,GL2PS License,1.0,100.0
39,39,49,linux-master/tools/thermal/lib/Makefile,LGPL-2.1-only Dual-license BSD-2-Clause,274,"# This is useful for building a package. The program will be
# Then the build tool can move it later.
# SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)
# installed in this directory as if it was the root directory.
clean:
# Most of this file is copied from tools/lib/perf/Makefile
		cp -fpR $(LIBTHERMAL_TOOLS_ALL) $(DESTDIR)$(libdir_SQ)
	$(QUIET_LINK)$(CC) --shared -Wl,-soname,libthermal_tools.so $^ -o $@
install_doc:
install_lib: libs","# SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)
# Most of this file is copied from tools/lib/perf/Makefile

LIBTHERMAL_TOOLS_VERSION = 0
LIBTHERMAL_TOOLS_PATCHLEVEL = 0
LIBTHERMAL_TOOLS_EXTRAVERSION = 1

MAKEFLAGS += --no-print-directory

ifeq ($(srctree),)
srctree := $(patsubst %/,%,$(dir $(CURDIR)))
srctree := $(patsubst %/,%,$(dir $(srctree)))
srctree := $(patsubst %/,%,$(dir $(srctree)))
# $(info Determined 'srctree' to be $(srctree))
endif

INSTALL = install

# Use DESTDIR for installing into a different root directory.
# This is useful for building a package. The program will be
# installed in this directory as if it was the root directory.
# Then the build tool can move it later.
DESTDIR ?=
DESTDIR_SQ = '$(subst ','\'',$(DESTDIR))'

include $(srctree)/tools/scripts/Makefile.include
include $(srctree)/tools/scripts/Makefile.arch

ifeq ($(LP64), 1)
  libdir_relative = lib64
else
  libdir_relative = lib
endif

prefix ?=
libdir = $(prefix)/$(libdir_relative)

# Shell quotes
libdir_SQ = $(subst ','\'',$(libdir))
libdir_relative_SQ = $(subst ','\'',$(libdir_relative))

ifeq (""$(origin V)"", ""command line"")
  VERBOSE = $(V)
endif
ifndef VERBOSE
  VERBOSE = 0
endif

ifeq ($(VERBOSE),1)
  Q =
else
  Q = @
endif

# Set compile option CFLAGS
ifdef EXTRA_CFLAGS
  CFLAGS := $(EXTRA_CFLAGS)
else
  CFLAGS := -g -Wall
endif

INCLUDES = \
-I/usr/include/libnl3 \
-I$(srctree)/tools/lib/thermal/include \
-I$(srctree)/tools/lib/ \
-I$(srctree)/tools/include \
-I$(srctree)/tools/arch/$(SRCARCH)/include/ \
-I$(srctree)/tools/arch/$(SRCARCH)/include/uapi \
-I$(srctree)/tools/include/uapi

# Append required CFLAGS
override CFLAGS += $(EXTRA_WARNINGS)
override CFLAGS += -Werror -Wall
override CFLAGS += -fPIC
override CFLAGS += $(INCLUDES)
override CFGLAS += -Wl,-L.
override CFGLAS += -Wl,-lthermal

all:

export srctree OUTPUT CC LD CFLAGS V
export DESTDIR DESTDIR_SQ

include $(srctree)/tools/build/Makefile.include

PATCHLEVEL    = $(LIBTHERMAL_TOOLS_PATCHLEVEL)
EXTRAVERSION  = $(LIBTHERMAL_TOOLS_EXTRAVERSION)
VERSION       = $(LIBTHERMAL_TOOLS_VERSION).$(LIBTHERMAL_TOOLS_PATCHLEVEL).$(LIBTHERMAL_TOOLS_EXTRAVERSION)

LIBTHERMAL_TOOLS_SO := $(OUTPUT)libthermal_tools.so.$(VERSION)
LIBTHERMAL_TOOLS_A  := $(OUTPUT)libthermal_tools.a
LIBTHERMAL_TOOLS_IN := $(OUTPUT)libthermal_tools-in.o
LIBTHERMAL_TOOLS_PC := $(OUTPUT)libthermal_tools.pc

LIBTHERMAL_TOOLS_ALL := $(LIBTHERMAL_TOOLS_A) $(OUTPUT)libthermal_tools.so*

$(LIBTHERMAL_TOOLS_IN): FORCE
	$(Q)$(MAKE) $(build)=libthermal_tools

$(LIBTHERMAL_TOOLS_A): $(LIBTHERMAL_TOOLS_IN)
	$(QUIET_AR)$(RM) $@ && $(AR) rcs $@ $(LIBTHERMAL_TOOLS_IN)

$(LIBTHERMAL_TOOLS_SO): $(LIBTHERMAL_TOOLS_IN)
	$(QUIET_LINK)$(CC) --shared -Wl,-soname,libthermal_tools.so $^ -o $@
	@ln -sf $(@F) $(OUTPUT)libthermal_tools.so
	@ln -sf $(@F) $(OUTPUT)libthermal_tools.so.$(LIBTHERMAL_TOOLS_VERSION)


libs: $(LIBTHERMAL_TOOLS_A) $(LIBTHERMAL_TOOLS_SO) $(LIBTHERMAL_TOOLS_PC)

all: fixdep
	$(Q)$(MAKE) libs

clean:
	$(call QUIET_CLEAN, libthermal_tools) $(RM) $(LIBTHERMAL_TOOLS_A) \
                *.o *~ *.a *.so *.so.$(VERSION) *.so.$(LIBTHERMAL_TOOLS_VERSION) .*.d .*.cmd LIBTHERMAL_TOOLS-CFLAGS $(LIBTHERMAL_TOOLS_PC)

$(LIBTHERMAL_TOOLS_PC):
	$(QUIET_GEN)sed -e ""s|@PREFIX@|$(prefix)|"" \
		-e ""s|@LIBDIR@|$(libdir_SQ)|"" \
		-e ""s|@VERSION@|$(VERSION)|"" \
		< libthermal_tools.pc.template > $@

define do_install_mkdir
	if [ ! -d '$(DESTDIR_SQ)$1' ]; then             \
		$(INSTALL) -d -m 755 '$(DESTDIR_SQ)$1'; \
	fi
endef

define do_install
	if [ ! -d '$(DESTDIR_SQ)$2' ]; then             \
		$(INSTALL) -d -m 755 '$(DESTDIR_SQ)$2'; \
	fi;                                             \
	$(INSTALL) $1 $(if $3,-m $3,) '$(DESTDIR_SQ)$2'
endef

install_lib: libs
	$(call QUIET_INSTALL, $(LIBTHERMAL_TOOLS_ALL)) \
		$(call do_install_mkdir,$(libdir_SQ)); \
		cp -fpR $(LIBTHERMAL_TOOLS_ALL) $(DESTDIR)$(libdir_SQ)

install_headers:
	$(call QUIET_INSTALL, headers) \
		$(call do_install,include/thermal.h,$(prefix)/include/thermal,644); \

install_pkgconfig: $(LIBTHERMAL_TOOLS_PC)
	$(call QUIET_INSTALL, $(LIBTHERMAL_TOOLS_PC)) \
		$(call do_install,$(LIBTHERMAL_TOOLS_PC),$(libdir_SQ)/pkgconfig,644)

install_doc:
	$(Q)$(MAKE) -C Documentation install-man install-html install-examples

#install: install_lib install_headers install_pkgconfig install_doc
install: install_lib install_headers install_pkgconfig

FORCE:

.PHONY: all install clean FORCE
",['# SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)'],LGPL-2.1-only Dual-license BSD-2-Clause,"mpi Permissive License
Gutmann License
SGI Free Software License B v1.0
Systemics BSD variant license
mpi Permissive License
Artistic License 2.0
psutils License
mpi Permissive License
Historical Permission Notice and Disclaimer - documentation variant
GNU General Public License v2.0 w/Classpath exception",1.0,100.0
40,40,50,linux-master/tools/thermal/lib/libthermal_tools.pc.template,Dual-license LGPL-2.1-only BSD-2-Clause,279,"# SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)
Version: @VERSION@
Requires: libnl-3.0 libnl-genl-3.0
Name: libthermal
Description: thermal library
libdir=@LIBDIR@
Libs: -L${libdir} -lnl-genl-3 -lnl-3
prefix=@PREFIX@
Cflags: -I${includedir} -I{include}/libnl3
","# SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)

prefix=@PREFIX@
libdir=@LIBDIR@
includedir=${prefix}/include

Name: libthermal
Description: thermal library
Requires: libnl-3.0 libnl-genl-3.0
Version: @VERSION@
Libs: -L${libdir} -lnl-genl-3 -lnl-3
Cflags: -I${includedir} -I{include}/libnl3
",['# SPDX-License-Identifier: (LGPL-2.1 OR BSD-2-Clause)'],Dual-license LGPL-2.1-only BSD-2-Clause,"SGI Free Software License B v1.0
GNU Lesser General Public License v3.0 or later
GNU General Public License v2.0 w/Classpath exception
Abstyles License
VOSTROM Public License for Open Source
GNU General Public License v2.0 w/Classpath exception
LaTeX Project Public License v1.2
mpi Permissive License
GNU General Public License v2.0 w/Classpath exception
Time::ParseDate License",1.0,100.0
41,41,51,linux-master/tools/testing/selftests/vDSO/parse_vdso.c,CC0-1.0,392,"And here's the code.
Straight from the ELF specification.
Now figure out whether it matches.
That's all we need.
First step: find the version definition
We need two things from the segment table: the load offset* and the dynamic table.
Failed
Failed
parse_vdso.c: Linux reference vDSO parser* Written by Andrew Lutomirski, 2011-2014.** This code is meant to be linked in to various programs that run on Linux.* As such, it is available with as few restrictions as possible.  This file* is licensed under the Creative Commons Zero License, version 1.0,* available at http://creativecommons.org/publicdomain/zero/1.0/legalcode** The vDSO is a regular ELF DSO that the kernel maps into user space when* it starts a program.  It works equally well in statically and dynamically* linked binaries.** This code is tested on x86.  In principle it should work on any* architecture that has a vDSO.
Check symbol version.","parse_vdso.c: Linux reference vDSO parser* Written by Andrew Lutomirski, 2011-2014.** This code is meant to be linked in to various programs that run on Linux.* As such, it is available with as few restrictions as possible.  This file* is licensed under the Creative Commons Zero License, version 1.0,* available at http://creativecommons.org/publicdomain/zero/1.0/legalcode** The vDSO is a regular ELF DSO that the kernel maps into user space when* it starts a program.  It works equally well in statically and dynamically* linked binaries.** This code is tested on x86.  In principle it should work on any* architecture that has a vDSO.
And here's the code.
Load information
load_addr - recorded vaddr
Symbol table
Version table
Straight from the ELF specification.
Wrong ELF class -- check ELF_BITS
We need two things from the segment table: the load offset* and the dynamic table.
Failed
Fish out the useful bits of the dynamic table.
Failed
Parse the hash table header.
That's all we need.
This is a helper function to check if the version indexed by* ver matches name (which hashes to hash).** The version definition table is a mess, and I don't know how* to do this in better than linear time without allocating memory* to build an index.  I also don't know why the table has* variable size entries in the first place.** For added fun, I can't find a comprehensible specification of how* to parse all the weird flags in the table.** So I just parse the whole table every time.
First step: find the version definition
Apparently bit 15 means ""hidden""
No definition.
Now figure out whether it matches.
Check for a defined global or weak function w/ right name.
Check symbol version.","['parse_vdso.c: Linux reference vDSO parser* Written by Andrew Lutomirski, 2011-2014.  This code is meant to be linked in to various programs that run on Linux.* As such, it is available with as few restrictions as possible.  This file* is licensed under the Creative Commons Zero License, version 1.0,* available at http://creativecommons.org/publicdomain/zero/1.0/legalcode** The vDSO is a regular ELF DSO that the kernel maps into user space when* it starts a program.  It works equally well in statically and dynamically* linked binaries.** This code is tested on x86.  In principle it should work on any* architecture that has a vDSO.', 'SPDX-License-Identifier: GPL-2.0-only']",CC0-1.0,"mpi Permissive License
mpi Permissive License
Gutmann License
Time::ParseDate License
Open Public License v1.0
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
VOSTROM Public License for Open Source
eCos license version 2.0",1.0,50.0
42,42,52,linux-master/tools/testing/selftests/tpm2/tpm2.py,Dual-license BSD-3-Clause GPL-2.0-only,412,"SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)
TPMT_PUBLIC
items are TPMS_PCR_SELECTION's
TPMS_SENSITIVE_CREATE
TPMS_AUTH_COMMAND","SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)
items are TPMS_PCR_SELECTION's
TPMS_AUTH_COMMAND
TPMS_SENSITIVE_CREATE
TPMT_PUBLIC",['SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)'],Dual-license BSD-3-Clause GPL-2.0-only,"SGI Free Software License B v2.0
LaTeX Project Public License v1.2
AMD's plpa_map.c License
LaTeX Project Public License v1.1
Linux man-pages Copyleft",1.0,100.0
43,43,53,linux-master/tools/testing/selftests/tpm2/tpm2_tests.py,Dual-license BSD-3-Clause GPL-2.0-only,411,"SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)
 Then, extend a PCR that is part of the policy and try to unseal. This should fail.
read the response
 Extend first a PCR that is not part of the policy and try to unseal. This should succeed.
Read the whole respone
Send a new cmd
Read part of the respone
expect the second one to raise -EBUSY error","SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)
Read part of the respone
Send a new cmd
Read the whole respone
expect the second one to raise -EBUSY error
read the response
 Extend first a PCR that is not part of the policy and try to unseal. This should succeed.
 Then, extend a PCR that is part of the policy and try to unseal. This should fail.",['SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)'],Dual-license BSD-3-Clause GPL-2.0-only,"SGI Free Software License B v2.0
Creative Commons Attribution Share Alike 2.1 Japan
diffmark license
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
XSkat License
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan",1.0,100.0
44,44,54,linux-master/tools/testing/selftests/tpm2/test_async.sh,GPL-2.0-only Dual-license BSD-3-Clause,415," !/bin/sh SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)
Kselftest framework requirement - SKIP code is 4.","Kselftest framework requirement - SKIP code is 4.
 !/bin/sh SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)",[' !/bin/sh SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)'],GPL-2.0-only Dual-license BSD-3-Clause,"BSD 4 Clause Shortened
mpi Permissive License",1.0,100.0
45,45,55,linux-master/tools/testing/selftests/tpm2/test_space.sh,GPL-2.0-only Dual-license BSD-3-Clause,413," !/bin/sh SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)
Kselftest framework requirement - SKIP code is 4.","Kselftest framework requirement - SKIP code is 4.
 !/bin/sh SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)",[' !/bin/sh SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)'],GPL-2.0-only Dual-license BSD-3-Clause,"BSD 4 Clause Shortened
mpi Permissive License",1.0,100.0
46,46,56,linux-master/tools/testing/selftests/tpm2/Makefile,BSD-3-Clause GPL-2.0-only Dual-license,417,"# SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)
TEST_PROGS_EXTENDED := tpm2.py tpm2_tests.py


include ../lib.mk
TEST_PROGS := test_smoke.sh test_space.sh test_async.sh","# SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)
include ../lib.mk

TEST_PROGS := test_smoke.sh test_space.sh test_async.sh
TEST_PROGS_EXTENDED := tpm2.py tpm2_tests.py
",['# SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)'],BSD-3-Clause GPL-2.0-only Dual-license,"SGI Free Software License B v1.0
Python License 2.0.1
Time::ParseDate License
Time::ParseDate License
Symlinks License
psutils License",1.0,100.0
47,47,57,linux-master/tools/testing/selftests/tpm2/test_smoke.sh,BSD-3-Clause GPL-2.0-only Dual-license,414," !/bin/sh SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)
Kselftest framework requirement - SKIP code is 4.","Kselftest framework requirement - SKIP code is 4.
 !/bin/sh SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)",[' !/bin/sh SPDX-License-Identifier: (GPL-2.0 OR BSD-3-Clause)'],BSD-3-Clause GPL-2.0-only Dual-license,"BSD 4 Clause Shortened
mpi Permissive License",1.0,100.0
48,48,58,linux-master/tools/testing/selftests/proc/proc-pid-vm.c,ISC,950,"Copyright (c) 2019 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
1: pause();
write(0, &c, 1);
lea rsi, [rip]
xor edi, edi
0: vsyscall VMA doesn't exist	vsyscall=none* 1: vsyscall VMA is --xp		vsyscall=xonly* 2: vsyscall VMA is r-xp		vsyscall=emulate
Hide ""segfault at ffffffffff600000"" messages.
file rss
gettimeofday(NULL, NULL);
->data_vm + ->stack_vm","Copyright (c) 2019 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Fork and exec tiny 1 page executable which precisely controls its VM.* Test /proc/$PID/maps* Test /proc/$PID/smaps* Test /proc/$PID/smaps_rollup* Test /proc/$PID/statm** FIXME require CONFIG_TMPFS which can be disabled* FIXME test other values from ""smaps""* FIXME support other archs
Casually unmap stack, vDSO and everything else.
munmap
Ping parent.
write(0, &c, 1);
xor edi, edi
lea rsi, [rip]
mov edx, 1
1: pause();
jmp 1b
Avoid ETXTBSY on exec.
0: vsyscall VMA doesn't exist	vsyscall=none* 1: vsyscall VMA is --xp		vsyscall=xonly* 2: vsyscall VMA is r-xp		vsyscall=emulate
vsyscall page can't be unmapped, probe it directly.
Hide ""segfault at ffffffffff600000"" messages.
gettimeofday(NULL, NULL);
Reserve fd 0 for 1-byte pipe ping from child.
Generate ""head -n1 /proc/$PID/maps""
Test /proc/$PID/maps
Test /proc/$PID/smaps
Test /proc/$PID/smaps_rollup
Test /proc/$PID/statm
->total_vm
rss
file rss
ELF executable segments
->data_vm + ->stack_vm","['Copyright (c) 2019 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.']",ISC,"curl License
Creative Commons Attribution Share Alike 2.1 Japan
mpi Permissive License
SSLeay License - standalone
EU DataGrid Software License
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
FSF All Permissive License
Data licence Germany – zero – version 2.0
CrystalStacker License",1.0,100.0
49,49,59,linux-master/tools/testing/selftests/proc/proc-tid0.c,ISC,944,"Copyright (c) 2021 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
task never contains ""0"".#include <sys/types.h>#include <dirent.h>#include <signal.h>#include <stdio.h>#include <stdlib.h>#include <string.h>#include <unistd.h>#include <pthread.h>static pid_t pid = -1;static void atexit_hook(void){if (pid > 0) {kill(pid, SIGKILL);}}static void *f(void *_){return NULL;}static void sigalrm(int _){exit(0);}int main(void){pid = fork();if (pid == 0) { child
parent
Test that /proc/*/task never contains ""0"".","Test that /proc/*/task never contains ""0"".
Copyright (c) 2021 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
task never contains ""0"".#include <sys/types.h>#include <dirent.h>#include <signal.h>#include <stdio.h>#include <stdlib.h>#include <string.h>#include <unistd.h>#include <pthread.h>static pid_t pid = -1;static void atexit_hook(void){if (pid > 0) {kill(pid, SIGKILL);}}static void *f(void *_){return NULL;}static void sigalrm(int _){exit(0);}int main(void){pid = fork();if (pid == 0) { child
parent","['Copyright (c) 2019 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.']",ISC,"curl License
Creative Commons Attribution Share Alike 2.1 Japan
Mulan Permissive Software License, Version 2
Gutmann License",1.0,100.0
50,50,60,linux-master/tools/testing/selftests/tmpfs/bug-link-o-tmpfile.c,ISC,421,"Copyright (c) 2019 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Our heroes: 1 root inode, 1 O_TMPFILE inode, 1 permanent inode.
Test that open(O_TMPFILE), linkat() doesn't screw accounting.","Copyright (c) 2019 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Test that open(O_TMPFILE), linkat() doesn't screw accounting.
Our heroes: 1 root inode, 1 O_TMPFILE inode, 1 permanent inode.","['Copyright (c) 2019 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.']",ISC,"curl License
mpi Permissive License
Symlinks License",1.0,100.0
51,51,61,linux-master/tools/testing/selftests/proc/proc-uptime-001.c,ISC,943,"Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 Test that boottime value in /proc/uptime and CLOCK_BOOTTIME increment monotonically. We don't test idle time monotonicity due to broken iowait task counting, cf: comment above get_cpu_idle_time_us()
Is CLOCK_BOOTTIME monotonic ?
Is /proc/uptime monotonic ?
Is CLOCK_BOOTTIME VS /proc/uptime monotonic ?"," Test that boottime value in /proc/uptime and CLOCK_BOOTTIME increment monotonically. We don't test idle time monotonicity due to broken iowait task counting, cf: comment above get_cpu_idle_time_us()
Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Is /proc/uptime monotonic ?
Is CLOCK_BOOTTIME monotonic ?
Is CLOCK_BOOTTIME VS /proc/uptime monotonic ?","['Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.']",ISC,"curl License
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan",1.0,100.0
52,52,62,linux-master/tools/testing/selftests/proc/proc-loadavg-001.c,ISC,952,"Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
pid 2
pid 1
Test that /proc/loadavg correctly reports last pid in pid namespace.","Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Test that /proc/loadavg correctly reports last pid in pid namespace.
pid 1
pid 2","['Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.']",ISC,"curl License
Zed License
Zed License
libpng License",1.0,100.0
53,53,63,linux-master/tools/testing/selftests/proc/proc-self-syscall.c,ISC,947,"Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Do direct system call as libc can wrap anything.","Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Do direct system call as libc can wrap anything.","['Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.']",ISC,"curl License
Gutmann License",1.0,100.0
54,54,64,linux-master/tools/testing/selftests/proc/thread-self.c,ISC,936,"Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
side thread
main thread
Test that /proc/thread-self gives correct TGID/PID.","Test that /proc/thread-self gives correct TGID/PID.
Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
main thread
side thread","['Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.']",ISC,"curl License
Gutmann License
Creative Commons Attribution Share Alike 2.1 Japan
Caldera License",1.0,100.0
55,55,65,linux-master/tools/testing/selftests/proc/proc-self-map-files-002.c,ISC,948,"Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
va_max must be enough bigger than vm.mmap_min_addr, which is* 64KB/32KB by default. (depends on CONFIG_LSM_MMAP_MIN_ADDR)
Test readlink /proc/self/map_files/... with minimum address.","Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Test readlink /proc/self/map_files/... with minimum address.
va_max must be enough bigger than vm.mmap_min_addr, which is* 64KB/32KB by default. (depends on CONFIG_LSM_MMAP_MIN_ADDR)","['Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.']",ISC,"curl License
Creative Commons Attribution Share Alike 2.1 Japan
dvipdfm License",1.0,100.0
56,56,66,linux-master/tools/testing/selftests/proc/read.c,ISC,940,"Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Ensure /proc is proc.
 Test 1) read and lseek on every file in /proc 2) readlink of every symlink in /proc 3) recursively (1) + (2) for every directory in /proc 4) write to /proc/*/clear_refs and /proc/*/task/*/clear_refs 5) write to /proc/sysrq-trigger
struct proc_ops::proc_lseek is mandatory if file is seekable.
read from /proc/kmsg can block
clear_refs and /proc/*/task"," Test 1) read and lseek on every file in /proc 2) readlink of every symlink in /proc 3) recursively (1) + (2) for every directory in /proc 4) write to /proc/*/clear_refs and /proc/*/task/*/clear_refs 5) write to /proc/sysrq-trigger
Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
clear_refs and /proc/*/task
read from /proc/kmsg can block
struct proc_ops::proc_lseek is mandatory if file is seekable.
Ensure /proc is proc.","['Copyright © 2018 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.']",ISC,"curl License
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Boehm-Demers-Weiser GC License
Creative Commons Attribution Share Alike 2.1 Japan
Gutmann License",1.0,100.0
57,57,67,linux-master/tools/testing/selftests/proc/setns-sysvipc.c,ISC,937,"Copyright © 2019 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Check for priviledges and syscall availability straight away.
Reliably pin dentry into dcache.
Test that setns(CLONE_NEWIPC) points to new /proc/sysvipc content even* if old one is in dcache.
Distinguisher between two otherwise empty IPC namespaces.","Copyright © 2019 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
Test that setns(CLONE_NEWIPC) points to new /proc/sysvipc content even* if old one is in dcache.
Check for priviledges and syscall availability straight away.
Distinguisher between two otherwise empty IPC namespaces.
Reliably pin dentry into dcache.","['Copyright © 2019 Alexey Dobriyan <adobriyan@gmail.com>** Permission to use, copy, modify, and distribute this software for any* purpose with or without fee is hereby granted, provided that the above* copyright notice and this permission notice appear in all copies.** THE SOFTWARE IS PROVIDED ""AS IS"" AND THE AUTHOR DISCLAIMS ALL WARRANTIES* WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF* MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR* ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES* WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN* ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF* OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.']",ISC,"curl License
Historical Permission Notice and Disclaimer - Markus Kuhn variant
Crossword License
Intel ACPI Software License Agreement
Net-SNMP License",1.0,100.0
58,58,68,pytorch-main/torch/utils/data/datapipes/iter/combining.py,See-doc.OTHER,307,"This method is called by `hook_iterator` in `_typing.py`.
This method is called by `hook_iterator` in `_typing.py`.
The line above invalidates `it1` and `it2`, and resets `ForkerIterDataPipe`.
 The line above doesn't invalidate `it3`, because an iterator for `cdp2` hasn't been created since the last invalidation.
 Note that the logic behind setting iterator ID and `reset` are handled within `hook_iterator` We want to separate the code for reset and yield, so that 'reset' executes before __next__ is called
This is necessary for the DataPipe to reset properly.
r""""""     Container to hold instance-specific information on behalf of DemultiplexerIterDataPipe.      It tracks the state of its child DataPipes, maintains the buffer, classifies and yields the next correct value     as requested by the child DataPipes.
r""""""     Splits the input DataPipe into multiple child DataPipes, using the given classification function (functional name: ``demux``).      A list of the child DataPipes is returned from this operation.      Args:         datapipe: Iterable DataPipe being filtered         num_instances: number of instances of the DataPipe to create         classifier_fn: a function that maps values to an integer within the range ``[0, num_instances - 1]`` or ``None``         drop_none: defaults to ``False``, if ``True``, the function will skip over elements classified as ``None``         buffer_size: this defines the maximum number of inputs that the buffer can hold across all child             DataPipes while waiting for their values to be yielded.             Defaults to ``1000``. Use ``-1`` for the unlimited buffer.      Examples:         >>> # xdoctest: +REQUIRES(module:torchdata)         >>> from torchdata.datapipes.iter import IterableWrapper         >>> def odd_or_even(n):         ...     return n % 2         >>> source_dp = IterableWrapper(range(5))         >>> dp1, dp2 = source_dp.demux(num_instances=2, classifier_fn=odd_or_even)         >>> list(dp1)         [0, 2, 4]         >>> list(dp2)         [1, 3]         >>> # It can also filter out any element that gets `None` from the `classifier_fn`         >>> def odd_or_even_no_zero(n):         ...     return n % 2 if n != 0 else None         >>> dp1, dp2 = source_dp.demux(num_instances=2, classifier_fn=odd_or_even_no_zero, drop_none=True)         >>> list(dp1)         [2, 4]         >>> list(dp2)         [1, 3]
xdoctest: +REQUIRES(module:torchdata)
xdoctest: +REQUIRES(module:torchdata)","xdoctest: +REQUIRES(module:torchdata)
type: ignore[assignment]
xdoctest: +REQUIRES(module:torchdata)
Use buffer
Retrieve one element from main datapipe
type: ignore[arg-type]
Can optimize by avoiding the call to min()
Cleanup _datapipe_iterator for the case that fork exits earlier
The line above invalidates `it1` and `it2`, and resets `ForkerIterDataPipe`.
This method is called by `hook_iterator` in `_typing.py`.
1. First time any child iterator is created
This method is called by `hook_iterator` in `_typing.py`.
xdoctest: +REQUIRES(module:torchdata)
It can also filter out any element that gets `None` from the `classifier_fn`
type: ignore[type-var]
This is necessary for the DataPipe to reset properly.
Cleanup _datapipe_iterator for the case that demux exits earlier
xdoctest: +REQUIRES(module:torchdata)
Store values to be yielded only when every iterator provides one
xdoctest: +REQUIRES(module:torchdata)
type: ignore[assignment]
 Indicate the indices of the next element to get The index to read by the slowest child The index to read by the fastest child The index to stop child
 xdoctest: +REQUIRES(module:torchdata) Singler Iterator per IteraDataPipe Invalidation
 The line above doesn't invalidate `it3`, because an iterator for `cdp2` hasn't been created since the last invalidation.
 Note that the logic behind setting iterator ID and `reset` are handled within `hook_iterator` We want to separate the code for reset and yield, so that 'reset' executes before __next__ is called
 type: ignore[attr-defined] 2. This instance was already in the same generation as `main_datapipe`, we need to increment the ID further by 1 type: ignore[has-type] type: ignore[attr-defined] Whenever a new generation of iterator is created, the `main_datapipe` must reset
 3. Otherwise, the iterator is behind the others, so it will just need to catch up by setting the instance's iterator to match that of `main_datapipe`
 When num_instances == 1, demux can be replaced by filter, but keep it as Demultiplexer for the sake of consistency like throwing Error when classification result is out of o range
r""""""     Concatenates multiple Iterable DataPipes (functional name: ``concat``).      The resulting DataPipe will yield all the elements from the first input DataPipe, before yielding from the subsequent ones.      Args:         datapipes: Iterable DataPipes being concatenated      Example:         >>> # xdoctest: +REQUIRES(module:torchdata)         >>> import random         >>> from torchdata.datapipes.iter import IterableWrapper         >>> dp1 = IterableWrapper(range(3))         >>> dp2 = IterableWrapper(range(5))         >>> list(dp1.concat(dp2))         [0, 1, 2, 0, 1, 2, 3, 4]
r""""""     Creates multiple instances of the same Iterable DataPipe (functional name: ``fork``).      Args:         datapipe: Iterable DataPipe being copied         num_instances: number of instances of the datapipe to create         buffer_size: this restricts how far ahead the leading child DataPipe            can read relative to the slowest child DataPipe.            Defaults to ``1000``. Use ``-1`` for the unlimited buffer.         copy: copy strategy to use for items yielded by each branch. Supported             options are ``None`` for no copying, ``""shallow""`` for shallow object             copies, and ``""deep""`` for deep object copies. Defaults to ``None``.      Note:         All branches of the forked pipeline return the identical object unless         the copy parameter is supplied. If the object is mutable or contains         mutable objects, changing them in one branch will affect all others.      Example:         >>> # xdoctest: +REQUIRES(module:torchdata)         >>> from torchdata.datapipes.iter import IterableWrapper         >>> source_dp = IterableWrapper(range(5))         >>> dp1, dp2 = source_dp.fork(num_instances=2)         >>> list(dp1)         [0, 1, 2, 3, 4]         >>> list(dp2)         [0, 1, 2, 3, 4]
r""""""Abstract class for container ``DataPipes``. The followings are three required methods.
r""""""Raise TypeError if it's not supposed to be implemented to support `list(datapipe)`.
r""""""     Container to hold instance-specific information on behalf of ForkerIterDataPipe.      It tracks the state of its child DataPipes, maintains the buffer, and yields the next value     as requested by the child DataPipes.
r""""""     Iterable Datapipe that is a child of a main DataPipe.      The instance of this class will pass its instance_id to get the next value from its main DataPipe.      Note:         ChildDataPipe, like all other IterDataPipe, follows the single iterator per IterDataPipe constraint.         Since ChildDataPipes share a common buffer, when an iterator is created for one of the ChildDataPipes,         the previous iterators  for all ChildDataPipes must be invalidated, with the exception when a ChildDataPipe         hasn't had an iterator created from it since the last invalidation. See the example below.      Example:         >>> # xdoctest: +REQUIRES(module:torchdata)         >>> # Singler Iterator per IteraDataPipe Invalidation         >>> from torchdata.datapipes.iter import IterableWrapper         >>> source_dp = IterableWrapper(range(10))         >>> cdp1, cdp2 = source_dp.fork(num_instances=2)         >>> it1, it2 = iter(cdp1), iter(cdp2)         >>> it3 = iter(cdp1)         >>> # The line above invalidates `it1` and `it2`, and resets `ForkerIterDataPipe`.         >>> it4 = iter(cdp2)         >>> # The line above doesn't invalidate `it3`, because an iterator for `cdp2` hasn't been created since         >>> # the last invalidation.      Args:         main_datapipe: Main DataPipe with a method 'get_next_element_by_instance(instance_id)'         instance_id: integer identifier of this instance
r""""""         Update the valid iterator ID for both this DataPipe object and `main_datapipe`.          `main_datapipe.reset()` is called when the ID is incremented to a new generation.
r""""""Check the valid iterator ID against that of DataPipe object and that of `main_datapipe`.
r""""""     Splits the input DataPipe into multiple child DataPipes, using the given classification function (functional name: ``demux``).      A list of the child DataPipes is returned from this operation.      Args:         datapipe: Iterable DataPipe being filtered         num_instances: number of instances of the DataPipe to create         classifier_fn: a function that maps values to an integer within the range ``[0, num_instances - 1]`` or ``None``         drop_none: defaults to ``False``, if ``True``, the function will skip over elements classified as ``None``         buffer_size: this defines the maximum number of inputs that the buffer can hold across all child             DataPipes while waiting for their values to be yielded.             Defaults to ``1000``. Use ``-1`` for the unlimited buffer.      Examples:         >>> # xdoctest: +REQUIRES(module:torchdata)         >>> from torchdata.datapipes.iter import IterableWrapper         >>> def odd_or_even(n):         ...     return n % 2         >>> source_dp = IterableWrapper(range(5))         >>> dp1, dp2 = source_dp.demux(num_instances=2, classifier_fn=odd_or_even)         >>> list(dp1)         [0, 2, 4]         >>> list(dp2)         [1, 3]         >>> # It can also filter out any element that gets `None` from the `classifier_fn`         >>> def odd_or_even_no_zero(n):         ...     return n % 2 if n != 0 else None         >>> dp1, dp2 = source_dp.demux(num_instances=2, classifier_fn=odd_or_even_no_zero, drop_none=True)         >>> list(dp1)         [2, 4]         >>> list(dp2)         [1, 3]
r""""""     Container to hold instance-specific information on behalf of DemultiplexerIterDataPipe.      It tracks the state of its child DataPipes, maintains the buffer, classifies and yields the next correct value     as requested by the child DataPipes.
r""""""     Yields one element at a time from each of the input Iterable DataPipes (functional name: ``mux``).      As in, one element from the 1st input DataPipe, then one element from the 2nd DataPipe in the next iteration,     and so on. It ends when the shortest input DataPipe is exhausted.      Args:         datapipes: Iterable DataPipes that will take turn to yield their elements, until the shortest DataPipe is exhausted      Example:         >>> # xdoctest: +REQUIRES(module:torchdata)         >>> from torchdata.datapipes.iter import IterableWrapper         >>> dp1, dp2, dp3 = IterableWrapper(range(3)), IterableWrapper(range(10, 15)), IterableWrapper(range(20, 25))         >>> list(dp1.mux(dp2, dp3))         [0, 10, 20, 1, 11, 21, 2, 12, 22]
r""""""     Aggregates elements into a tuple from each of the input DataPipes (functional name: ``zip``).      The output is stopped as soon as the shortest input DataPipe is exhausted.      Args:         *datapipes: Iterable DataPipes being aggregated      Example:         >>> # xdoctest: +REQUIRES(module:torchdata)         >>> from torchdata.datapipes.iter import IterableWrapper         >>> dp1, dp2, dp3 = IterableWrapper(range(5)), IterableWrapper(range(10, 15)), IterableWrapper(range(20, 25))         >>> list(dp1.zip(dp2, dp3))         [(0, 10, 20), (1, 11, 21), (2, 12, 22), (3, 13, 23), (4, 14, 24)]",[''],No_license_found,"mpi Permissive License
mpi Permissive License
Gutmann License
Creative Commons Attribution Share Alike 2.1 Japan
Gutmann License
Gutmann License
Computational Use of Data Agreement v1.0
mpi Permissive License
Xdebug License v 1.03
Xdebug License v 1.03",0.0,0.0
59,59,69,pytorch-main/torch/fx/experimental/graph_gradual_typechecker.py,See-doc.OTHER,1212,"TODO. We leave it like this till we add a type to represent tensor sizes
Replace all unknown types with fresh type variables.
Replace all unknown types with fresh type variables.
 we return the less precise type because broadcasting may have happened for operands with shape [1,2,Dyn] and [1,2,1] we have to assign the node [1,2,Dyn]
Input and output shapes should be equal.
by this point, we know that args1 and args2 are the same size.
 we check the conditions on the incoming argument and any existing annotation we also check for consistency between both annotations
Type check a given fx node.         Current operations:         - Reshape         - Transpose         - Add         - Relu         - conv2d         - batchnorm2d         - flatten         - maxpool2d         - adaptiveavgpool2d         - linear
 at this point our tensors should be consistent and we can apply the element-wise operation and find the right dimension for the output of the operation
 we would be here in the second iteration where we establish equality between operand type dimensions and the resulting type dimensions","type: ignore[attr-defined]
if either type is Dyn, do nothing since the types are already consistent
handle scalar addition
handle scalar addition
we check for consistency between the new types
TODO. We leave it like this till we add a type to represent tensor sizes
set the default start and end dims
type: ignore[arg-type]
by this point, we know that args1 and args2 are the same size.
 We make the types the same length which is the first requirement for consistency
 we replace occurrences of ""1"" with each tensor with the corresponding type from the other tensor
 at this point our tensors should be consistent and we can apply the element-wise operation and find the right dimension for the output of the operation
 we bring the new types to the point where we can check for consistency any inconsistency would not have been caused by broadcasting at this point
 we return the less precise type because broadcasting may have happened for operands with shape [1,2,Dyn] and [1,2,1] we have to assign the node [1,2,Dyn]
 if we do not know the original tensor dimension, we return the required dimension
 if any of the dimensions are unknown, we check for divisibility
 we check the conditions on the incoming argument and any existing annotation we also check for consistency between both annotations
 we choose the more precise type to be the node type so if an incoming argument has more type information we set this node's type to be the argument type
 type check every node with gradual type rules if any node does not type check return false
 we would be here in the second iteration where we establish equality between operand type dimensions and the resulting type dimensions
Expand a type to the desired tensor dimension if possible     Raise an error otherwise.     - t is the given type     - n is a number of dimensions to expand to
Applies broadcasting to both given types such that they     become consistent with eachother and returns two new     resulting types
Apply the addition inference rule. This includes:     - scalar addition     - broadcasting semantics      Note that we always return the least precise type between     the operands (after applying broadcasting) to be the final type of the operation      Note that we do not modify the operand types themselves after applying broadcasting     to them. We only use them to calculate the final type
The current getattr rule only handles the shape attribute     Can be extended to other attributes     The most representitive type we have is ""Dyn"" but the system     can be extended with more types, such as a type to represent shapes
We check that dimensions for the transpose operations     are within range of the tensor type of the node
Without dynamism, the rule checks that the     product of the elements of the argument tensor     type is equal to the product of the elements     of the required shape. We gradualize this rule     by adding a case to handle fully dynamic input     as well as input where some of the tensor dimensions     are unknown. In this case we check for divisibility
Given a BatchNorm2D instance and a node check the following conditions:     - the input type can be expanded to a size 4 tensor: t =  (x_1, x_2, x_3, x_4)     - the current node type can be expanded to a size 4 tensor: t' =  (x_1', x_2', x_3', x_4')     - t is consistent with t'     - x_2 is consistent with the module's num_features     - x_2' is consistent with the module's num_features     output type: the more precise type of t and t'
For calculating h_in and w_out according to the conv2D documentation
Get the most precise type that's consistent with the given types
Given a Conv2D instance and a node check the following conditions:     - the input type can be expanded to a size 4 tensor: t =  (x_1, x_2, H, W)     - the current node type can be expanded to a size 4 tensor: t' =  (x_1', x_2', x_3', x_4')     - x_2 is consistent with the module's in_channels     - let o = (x_1, out_channels, H_out, W_out)     then the output is the greatest upper bound of o and the existing node type t'.
Input and output shapes should be equal.
Applies the maxpool2d shape information to the input     this affects the last two dimensions
Given a MaxPool2D instance and a node check the following conditions:     - Input size matches size 3 or 4     - Current node type is consistent with the output type we will calculate     - Input size matches output size and the last two dimensions of the output       are w_out and h_out. The remaining dimensions are the same as the input     - Our final result is the greatest upper bound of the output we calculate       and the current node type.
Checks that an input tensor type satisfies the conditions for linear operation     and returns the output type based on in and out features given by module_instance
Applies the shape information to the input then gets the greatest upper bound     of the resulting type and the existing type
The input and output sizes should be the same except for the last     two dimensions taken from the input, which represent width and height
Applies the flatten shape information to the input then gets the     greatest upper bound of the resulting type and the existing type
A gradual type checker for graphs         Effect: every node's field type will be         populated with a type after type-checking is done
Type check a given fx node.         Current operations:         - Reshape         - Transpose         - Add         - Relu         - conv2d         - batchnorm2d         - flatten         - maxpool2d         - adaptiveavgpool2d         - linear
The equality constraints are between the first dimension of     the input and output
The equality constraints are between the first dimension of     the input and output
For operations where the input shape is equal to the output shape
For operations where the first two dimensions of the input and output shape     are equal
For element-wise operations and handles broadcasting.     Note that after applying broadcasting to the arguments     we are able to determine if certain dimensions have not been broadcast     if they are symbolicallu equal.      in this case, we can establish equality between those dimensions and the     corresponding output dimensions.      Note that it takes two iterations for this result. One iteration to establish     equality between certain dimensions of the operands (requiring the whole solver     including unification) and another iteration to establish equality between the operands     and the resulting type, requiring another round of constraint generation and unificaiton.
Generates equality constraints between the dimensions of the input and output     that will not be involved in the flatten operation
Represents the outout in terms of an algrbraic expression w.r.t     the input when possible
Symbolic shape inference.     Generates constraints over type variables.     Currently all constraints are equality constraints.
Generates constraints for         every node in the graph based on         the operation.
Infers algebraic relations
Replace all unknown types with fresh type variables.
Replace all unknown types with fresh type variables.
Returns a list of equality constraints for         call_module and call_function nodes.         Models the relation between input and output dimensions         using constraints in case they are both tensors.         All operations used in resnet50 are defined.
Returns the parameter given by ``target`` if it exists,     otherwise throws an error.      See the docstring for ``get_submodule`` for a more detailed     explanation of this method's functionality as well as how to     correctly specify ``target``.      Args:         target: The fully-qualified string name of the Parameter             to look for. (See ``get_submodule`` for how to specify a             fully-qualified string.)      Returns:         torch.nn.Parameter: The Parameter referenced by ``target``      Raises:         AttributeError: If the target string references an invalid             path or resolves to something that is not an             ``nn.Parameter``",[''],No_license_found,"Time::ParseDate License
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Gutmann License
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan",0.0,0.0
60,60,70,linux-master/tools/testing/selftests/tc-testing/README,See-doc.OTHER,477,"      (This one is a preliminary version, it may not work quite right yet,
--------------
-----------
----------------
------------
-------------------
----------------------
----------------------
-------------------------
Thanks to:","tdc - Linux Traffic Control (tc) unit testing suite

Author: Lucas Bates - lucasb@mojatatu.com

tdc is a Python script to load tc unit tests from a separate JSON file and
execute them inside a network namespace dedicated to the task.


REQUIREMENTS
------------

*  Minimum Python version of 3.4. Earlier 3.X versions may work but are not
   guaranteed.

*  The kernel must have network namespace support if using nsPlugin

*  The kernel must have veth support available, as a veth pair is created
   prior to running the tests when using nsPlugin.

*  The kernel must have the appropriate infrastructure enabled to run all tdc
   unit tests. See the config file in this directory for minimum required
   features. As new tests will be added, config options list will be updated.

*  All tc-related features being tested must be built in or available as
   modules.  To check what is required in current setup run:
   ./tdc.py -c

   Note:
   In the current release, tdc run will abort due to a failure in setup or
   teardown commands - which includes not being able to run a test simply
   because the kernel did not support a specific feature. (This will be
   handled in a future version - the current workaround is to run the tests
   on specific test categories that your kernel supports)


BEFORE YOU RUN
--------------

The path to the tc executable that will be most commonly tested can be defined
in the tdc_config.py file. Find the 'TC' entry in the NAMES dictionary and
define the path.

If you need to test a different tc executable on the fly, you can do so by
using the -p option when running tdc:
	./tdc.py -p /path/to/tc


RUNNING TDC
-----------

To use tdc, root privileges are required.  This is because the
commands being tested must be run as root.  The code that enforces
execution by root uid has been moved into a plugin (see PLUGIN
ARCHITECTURE, below).

Tests that use a network device should have nsPlugin.py listed as a
requirement for that test. nsPlugin executes all commands within a
network namespace and creates a veth pair which may be used in those test
cases. To disable execution within the namespace, pass the -N option
to tdc when starting a test run; the veth pair will still be created
by the plugin.

Running tdc without any arguments will run all tests. Refer to the section
on command line arguments for more information, or run:
	./tdc.py -h

tdc will list the test names as they are being run, and print a summary in
TAP (Test Anything Protocol) format when they are done. If tests fail,
output captured from the failing test will be printed immediately following
the failed test in the TAP output.


OVERVIEW OF TDC EXECUTION
-------------------------

One run of tests is considered a ""test suite"" (this will be refined in the
future).  A test suite has one or more test cases in it.

A test case has four stages:

  - setup
  - execute
  - verify
  - teardown

The setup and teardown stages can run zero or more commands.  The setup
stage does some setup if the test needs it.  The teardown stage undoes
the setup and returns the system to a ""neutral"" state so any other test
can be run next.  These two stages require any commands run to return
success, but do not otherwise verify the results.

The execute and verify stages each run one command.  The execute stage
tests the return code against one or more acceptable values.  The
verify stage checks the return code for success, and also compares
the stdout with a regular expression.

Each of the commands in any stage will run in a shell instance.


USER-DEFINED CONSTANTS
----------------------

The tdc_config.py file contains multiple values that can be altered to suit
your needs. Any value in the NAMES dictionary can be altered without affecting
the tests to be run. These values are used in the tc commands that will be
executed as part of the test. More will be added as test cases require.

Example:
	$TC qdisc add dev $DEV1 ingress

The NAMES values are used to substitute into the commands in the test cases.


COMMAND LINE ARGUMENTS
----------------------

Run tdc.py -h to see the full list of available arguments.

usage: tdc.py [-h] [-p PATH] [-D DIR [DIR ...]] [-f FILE [FILE ...]]
              [-c [CATG [CATG ...]]] [-e ID [ID ...]] [-l] [-s] [-i] [-v] [-N]
              [-d DEVICE] [-P] [-n] [-V]

Linux TC unit tests

optional arguments:
  -h, --help            show this help message and exit
  -p PATH, --path PATH  The full path to the tc executable to use
  -v, --verbose         Show the commands that are being run
  -N, --notap           Suppress tap results for command under test
  -d DEVICE, --device DEVICE
                        Execute test cases that use a physical device, where
                        DEVICE is its name. (If not defined, tests that require
                        a physical device will be skipped)
  -P, --pause           Pause execution just before post-suite stage

selection:
  select which test cases: files plus directories; filtered by categories
  plus testids

  -D DIR [DIR ...], --directory DIR [DIR ...]
                        Collect tests from the specified directory(ies)
                        (default [tc-tests])
  -f FILE [FILE ...], --file FILE [FILE ...]
                        Run tests from the specified file(s)
  -c [CATG [CATG ...]], --category [CATG [CATG ...]]
                        Run tests only from the specified category/ies, or if
                        no category/ies is/are specified, list known
                        categories.
  -e ID [ID ...], --execute ID [ID ...]
                        Execute the specified test cases with specified IDs

action:
  select action to perform on selected test cases

  -l, --list            List all test cases, or those only within the
                        specified category
  -s, --show            Display the selected test cases
  -i, --id              Generate ID numbers for new test cases

netns:
  options for nsPlugin (run commands in net namespace)

  -N, --no-namespace
                        Do not run commands in a network namespace.

valgrind:
  options for valgrindPlugin (run command under test under Valgrind)

  -V, --valgrind        Run commands under valgrind


PLUGIN ARCHITECTURE
-------------------

There is now a plugin architecture, and some of the functionality that
was in the tdc.py script has been moved into the plugins.

The plugins are in the directory plugin-lib.  The are executed from
directory plugins.  Put symbolic links from plugins to plugin-lib,
and name them according to the order you want them to run. This is not
necessary if a test case being run requires a specific plugin to work.

Example:

bjb@bee:~/work/tc-testing$ ls -l plugins
total 4
lrwxrwxrwx  1 bjb  bjb    27 Oct  4 16:12 10-rootPlugin.py -> ../plugin-lib/rootPlugin.py
lrwxrwxrwx  1 bjb  bjb    25 Oct 12 17:55 20-nsPlugin.py -> ../plugin-lib/nsPlugin.py
-rwxr-xr-x  1 bjb  bjb     0 Sep 29 15:56 __init__.py

The plugins are a subclass of TdcPlugin, defined in TdcPlugin.py and
must be called ""SubPlugin"" so tdc can find them.  They are
distinguished from each other in the python program by their module
name.

This base class supplies ""hooks"" to run extra functions.  These hooks are as follows:

pre- and post-suite
pre- and post-case
pre- and post-execute stage
adjust-command (runs in all stages and receives the stage name)

The pre-suite hook receives the number of tests and an array of test ids.
This allows you to dump out the list of skipped tests in the event of a
failure during setup or teardown stage.

The pre-case hook receives the ordinal number and test id of the current test.

The adjust-command hook receives the stage id (see list below) and the
full command to be executed.  This allows for last-minute adjustment
of the command.

The stages are identified by the following strings:

  - pre  (pre-suite)
  - setup
  - command
  - verify
  - teardown
  - post (post-suite)


To write a plugin, you need to inherit from TdcPlugin in
TdcPlugin.py.  To use the plugin, you have to put the
implementation file in plugin-lib, and add a symbolic link to it from
plugins.  It will be detected at run time and invoked at the
appropriate times.  There are a few examples in the plugin-lib
directory:

  - rootPlugin.py:
      implements the enforcement of running as root
  - nsPlugin.py:
      sets up a network namespace and runs all commands in that namespace,
      while also setting up dummy devices to be used in testing.
  - valgrindPlugin.py
      runs each command in the execute stage under valgrind,
      and checks for leaks.
      This plugin will output an extra test for each test in the test file,
      one is the existing output as to whether the test passed or failed,
      and the other is a test whether the command leaked memory or not.
      (This one is a preliminary version, it may not work quite right yet,
      but the overall template is there and it should only need tweaks.)
  - buildebpfPlugin.py:
      builds all programs in $EBPFDIR.


ACKNOWLEDGEMENTS
----------------

Thanks to:

Jamal Hadi Salim, for providing valuable test cases
Keara Leibovitz, who wrote the CLI test driver that I used as a base for the
   first version of the tc testing suite. This work was presented at
   Netdev 1.2 Tokyo in October 2016.
Samir Hussain, for providing help while I dove into Python for the first time
    and being a second eye for this code.
",[''],No_license_found,"mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
Time::ParseDate License",0.0,0.0
61,61,71,pytorch-main/torch/nn/modules/module.py,See-doc.OTHER,878," If we don't have any hooks, we want to skip the rest of the logic in this function, and just call forward.
Note that the hook can modify missing_keys and unexpected_keys.
TODO: Remove `args` and the parsing logic when BC allows.
 TODO: Change `*args` to `*` and remove the corresponding warning in docs when BC allows. Also remove the logic for arg parsing together.
At this point we are sure that inputs and result are tuple of Tensors
 Trick mypy into not applying contravariance rules to inputs by defining forward as a value, rather than a function.  See also https://github.com/python/mypy/issues/8795
 This is used to avoid copying uninitialized parameters into non-lazy modules, since they dont have the hook to do the checks in such case, it will error when accessing the .shape attribute.
At this point the grad_output part of the hook will most likely be correct
Shape checks are already done above
Compile this Module's forward using :func:`torch.compile`.          This Module's `__call__` method is compiled and all arguments are passed as-is         to :func:`torch.compile`.          See :func:`torch.compile` for details on the arguments for this function.","don't do anything for single-line stuff
forward hooks that should always be called even if an exception is raised
Backward compatibility: no args used to be allowed when call_super_init=False
xdoctest: +SKIP(""undefined vars"")
xdoctest: +IGNORE_WANT(""non-deterministic"")
xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)
type: ignore[attr-defined]
type: ignore[attr-defined]
At this point we are sure that inputs and result are tuple of Tensors
At this point the grad_output part of the hook will most likely be correct
type: ignore[attr-defined]
type: ignore[attr-defined]
type: ignore[misc]
type: ignore[misc]
mark that always called hook is run
Handle the non-full backward hooks
raise exception raised in try block
Support loading old checkpoints that don't have the following attrs:
xdoctest: +SKIP(""undefined vars"")
TODO: Remove `args` and the parsing logic when BC allows.
DeprecationWarning is ignored by default
torch.nn.Module.state_dict""
Backward compatibility: loading 1-dim tensor from 0.3.* to version 0.4+
local shape should match the one in checkpoint
Shape checks are already done above
get the name of param/buffer/child
copy state_dict so _load_from_state_dict can modify it
Note that the hook can modify missing_keys and unexpected_keys.
xdoctest: +SKIP(""undefined vars"")
xdoctest: +SKIP(""undefined vars"")
xdoctest: +SKIP(""undefined vars"")
xdoctest: +SKIP(""undefined vars"")
xdoctest: +SKIP(""undefined vars"")
We treat the extra repr like the sub-module, one item per line
empty string will be split into list ['']
simple one-liner info, which most builtin Modules will use
Eliminate attrs that are not legal Python variable names
type: ignore[assignment]
 See https://mypy.readthedocs.io/en/latest/generics.html#generic-methods-and-generic-self for the use of `T` to annotate `self`. Many methods of `Module` return `self` and we want those return values to be the type of the subclass, not the looser type of `Module`.
 Trick mypy into not applying contravariance rules to inputs by defining forward as a value, rather than a function.  See also https://github.com/python/mypy/issues/8795
 Marks whether the corresponding _forward_hooks accept kwargs or not. As JIT does not support Set[int], this dict is used as a set, where all hooks represented in this dict accept kwargs.
 Marks whether the corresponding _forward_hooks accept kwargs or not. As JIT does not support Set[int], this dict is used as a set, where all hooks represented in this dict accept kwargs.
 If the new tensor has compatible tensor type as the existing tensor, the current behavior is to change the tensor in-place using `.data =`, and the future behavior is to overwrite the existing tensor. However, changing the current behavior is a BC-breaking change, and we want it to happen in future releases. So for now we introduce the `torch.__future__.get_overwrite_module_params_on_conversion()` global flag to let the user control whether they want the future behavior of overwriting the existing tensor or not.
 Tensors stored in modules are graph leaves, and we don't want to track autograd history of `param_applied`, so we have to use `with torch.no_grad():`
 type ignore was added because at this point one knows that torch.jit._trace._trace_module_map is not Optional and has type Dict[Any, Any] type: ignore[index, operator] # noqa: B950
 If we don't have any hooks, we want to skip the rest of the logic in this function, and just call forward.
 run always called hooks if they have not already been run For now only forward hooks have the always_call option but perhaps this functionality should be added to full backward hooks as well.
 On the return type: We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`. This is done for better interop with various type checkers for the end users. Having a stricter return type doesn't play nicely with `register_buffer()` and forces people to excessively use type-ignores, asserts, casts, etc. See full discussion on the problems with returning `Union` here https://github.com/microsoft/pyright/issues/4213
 The user can pass an optional arbitrary mappable object to `state_dict`, in which case `state_dict` returns back that same object. But if they pass nothing, an `OrderedDict` is created and returned.
 TODO: Change `*args` to `*` and remove the corresponding warning in docs when BC allows. Also remove the logic for arg parsing together.
 This is used to avoid copying uninitialized parameters into non-lazy modules, since they dont have the hook to do the checks in such case, it will error when accessing the .shape attribute.
 mypy isn't aware that ""_metadata"" exists in state_dict type: ignore[attr-defined]
 replicas do not have parameters themselves, the replicas reference the original module.
r""""""This tracks hooks common to all modules that are executed immediately before
r""""""This tracks hooks common to all modules that are executed before/after calling forward and backward. This is global state used for debugging/profiling
r""""""Register a buffer registration hook common to all modules.      .. warning ::          This adds global state to the `nn.Module` module      The hook will be called every time :func:`register_buffer` is invoked.     It should have the following signature::          hook(module, name, buffer) -> None or new buffer      The hook can modify the input or return a single modified value in the hook.      Returns:         :class:`torch.utils.hooks.RemovableHandle`:             a handle that can be used to remove the added hook by calling             ``handle.remove()``
r""""""Register a module registration hook common to all modules.      .. warning ::          This adds global state to the `nn.Module` module      The hook will be called every time :func:`register_module` is invoked.     It should have the following signature::          hook(module, name, submodule) -> None or new submodule      The hook can modify the input or return a single modified value in the hook.      Returns:         :class:`torch.utils.hooks.RemovableHandle`:             a handle that can be used to remove the added hook by calling             ``handle.remove()``
r""""""Register a parameter registration hook common to all modules.      .. warning ::          This adds global state to the `nn.Module` module      The hook will be called every time :func:`register_parameter` is invoked.     It should have the following signature::          hook(module, name, param) -> None or new parameter      The hook can modify the input or return a single modified value in the hook.      Returns:         :class:`torch.utils.hooks.RemovableHandle`:             a handle that can be used to remove the added hook by calling             ``handle.remove()``
r""""""Register a forward pre-hook common to all modules.      .. warning ::          This adds global state to the `nn.module` module         and it is only intended for debugging/profiling purposes.      The hook will be called every time before :func:`forward` is invoked.     It should have the following signature::          hook(module, input) -> None or modified input      The input contains only the positional arguments given to the module.     Keyword arguments won't be passed to the hooks and only to the ``forward``.     The hook can modify the input. User can either return a tuple or a     single modified value in the hook. We will wrap the value into a tuple     if a single value is returned(unless that value is already a tuple).      This hook has precedence over the specific module hooks registered with     ``register_forward_pre_hook``.      Returns:         :class:`torch.utils.hooks.RemovableHandle`:             a handle that can be used to remove the added hook by calling             ``handle.remove()``
r""""""Register a global forward hook for all the modules.      .. warning ::          This adds global state to the `nn.module` module         and it is only intended for debugging/profiling purposes.      The hook will be called every time after :func:`forward` has computed an output.     It should have the following signature::          hook(module, input, output) -> None or modified output      The input contains only the positional arguments given to the module.     Keyword arguments won't be passed to the hooks and only to the ``forward``.     The hook can modify the output. It can modify the input inplace but     it will not have effect on forward since this is called after     :func:`forward` is called.      Parameters:         hook (Callable): The user defined hook to be registered.         always_call (bool): If ``True`` the ``hook`` will be run regardless of             whether an exception is raised while calling the Module.             Default: ``False``     Returns:         :class:`torch.utils.hooks.RemovableHandle`:             a handle that can be used to remove the added hook by calling             ``handle.remove()``      This hook will be executed before specific module hooks registered with     ``register_forward_hook``.
r""""""Register a backward hook common to all the modules.      This function is deprecated in favor of     :func:`torch.nn.modules.module.register_module_full_backward_hook`     and the behavior of this function will change in future versions.      Returns:         :class:`torch.utils.hooks.RemovableHandle`:             a handle that can be used to remove the added hook by calling             ``handle.remove()``
r""""""Register a backward pre-hook common to all the modules.      .. warning ::         This adds global state to the `nn.module` module         and it is only intended for debugging/profiling purposes.      The hook will be called every time the gradients for the module are computed.     The hook should have the following signature::          hook(module, grad_output) -> Tensor or None      The :attr:`grad_output` is a tuple. The hook should     not modify its arguments, but it can optionally return a new gradient with     respect to the output that will be used in place of :attr:`grad_output` in     subsequent computations. Entries in :attr:`grad_output` will be ``None`` for     all non-Tensor arguments.      For technical reasons, when this hook is applied to a Module, its forward function will     receive a view of each Tensor passed to the Module. Similarly the caller will receive a view     of each Tensor returned by the Module's forward function.      Global hooks are called before hooks registered with `register_backward_pre_hook`      Returns:         :class:`torch.utils.hooks.RemovableHandle`:             a handle that can be used to remove the added hook by calling             ``handle.remove()``
r""""""Register a backward hook common to all the modules.      .. warning ::         This adds global state to the `nn.module` module         and it is only intended for debugging/profiling purposes.      The hook will be called every time the gradients with respect to a module     are computed, i.e. the hook will execute if and only if the gradients with     respect to module outputs are computed. The hook should have the following     signature::          hook(module, grad_input, grad_output) -> Tensor or None      The :attr:`grad_input` and :attr:`grad_output` are tuples. The hook should     not modify its arguments, but it can optionally return a new gradient with     respect to the input that will be used in place of :attr:`grad_input` in     subsequent computations. :attr:`grad_input` will only correspond to the inputs given     as positional arguments and all kwarg arguments will not appear in the hook. Entries     in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor     arguments.      For technical reasons, when this hook is applied to a Module, its forward function will     receive a view of each Tensor passed to the Module. Similarly the caller will receive a view     of each Tensor returned by the Module's forward function.      Global hooks are called before hooks registered with `register_backward_hook`      Returns:         :class:`torch.utils.hooks.RemovableHandle`:             a handle that can be used to remove the added hook by calling             ``handle.remove()``
r""""""Define the computation performed at every call.      Should be overridden by all subclasses.      .. note::         Although the recipe for forward pass needs to be defined within         this function, one should call the :class:`Module` instance afterwards         instead of this since the former takes care of running the         registered hooks while the latter silently ignores them.
r""""""Base class for all neural network modules.      Your models should also subclass this class.      Modules can also contain other Modules, allowing to nest them in     a tree structure. You can assign the submodules as regular attributes::          import torch.nn as nn         import torch.nn.functional as F          class Model(nn.Module):             def __init__(self):                 super().__init__()                 self.conv1 = nn.Conv2d(1, 20, 5)                 self.conv2 = nn.Conv2d(20, 20, 5)              def forward(self, x):                 x = F.relu(self.conv1(x))                 return F.relu(self.conv2(x))      Submodules assigned in this way will be registered, and will have their     parameters converted too when you call :meth:`to`, etc.      .. note::         As per the example above, an ``__init__()`` call to the parent class         must be made before assignment on the child.      :ivar training: Boolean represents whether this module is in training or                     evaluation mode.     :vartype training: bool
r""""""This allows better BC support for :meth:`load_state_dict`. In     :meth:`state_dict`, the version number will be saved as in the attribute     `_metadata` of the returned state dict, and thus pickled. `_metadata` is a     dictionary with keys that follow the naming convention of state dict. See     ``_load_from_state_dict`` on how to use this information in loading.      If new parameters/buffers are added/removed from a module, this number shall     be bumped, and the module's `_load_from_state_dict` method can compare the     version number and do appropriate changes if the state dict is from before
Initialize internal Module state, shared by both nn.Module and ScriptModule.
Calls super().__setattr__('a', a) instead of the typical self.a = a         to avoid Module.__setattr__ overhead. Module's __setattr__ has special         handling for parameters, submodules, and buffers but simply calls into         super().__setattr__ for all other attributes.
r""""""Add a buffer to the module.          This is typically used to register a buffer that should not to be         considered a model parameter. For example, BatchNorm's ``running_mean``         is not a parameter, but is part of the module's state. Buffers, by         default, are persistent and will be saved alongside parameters. This         behavior can be changed by setting :attr:`persistent` to ``False``. The         only difference between a persistent buffer and a non-persistent buffer         is that the latter will not be a part of this module's         :attr:`state_dict`.          Buffers can be accessed as attributes using given names.          Args:             name (str): name of the buffer. The buffer can be accessed                 from this module using the given name             tensor (Tensor or None): buffer to be registered. If ``None``, then operations                 that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,                 the buffer is **not** included in the module's :attr:`state_dict`.             persistent (bool): whether the buffer is part of this module's                 :attr:`state_dict`.          Example::              >>> # xdoctest: +SKIP(""undefined vars"")             >>> self.register_buffer('running_mean', torch.zeros(num_features))
r""""""Add a parameter to the module.          The parameter can be accessed as an attribute using given name.          Args:             name (str): name of the parameter. The parameter can be accessed                 from this module using the given name             param (Parameter or None): parameter to be added to the module. If                 ``None``, then operations that run on parameters, such as :attr:`cuda`,                 are ignored. If ``None``, the parameter is **not** included in the                 module's :attr:`state_dict`.
r""""""Add a child module to the current module.          The module can be accessed as an attribute using the given name.          Args:             name (str): name of the child module. The child module can be                 accessed from this module using the given name             module (Module): child module to be added to the module.
r""""""Alias for :func:`add_module`.
Return the submodule given by ``target`` if it exists, otherwise throw an error.          For example, let's say you have an ``nn.Module`` ``A`` that         looks like this:          .. code-block:: text              A(                 (net_b): Module(                     (net_c): Module(                         (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))                     )                     (linear): Linear(in_features=100, out_features=200, bias=True)                 )             )          (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested         submodule ``net_b``, which itself has two submodules ``net_c``         and ``linear``. ``net_c`` then has a submodule ``conv``.)          To check whether or not we have the ``linear`` submodule, we         would call ``get_submodule(""net_b.linear"")``. To check whether         we have the ``conv`` submodule, we would call         ``get_submodule(""net_b.net_c.conv"")``.          The runtime of ``get_submodule`` is bounded by the degree         of module nesting in ``target``. A query against         ``named_modules`` achieves the same result, but it is O(N) in         the number of transitive modules. So, for a simple check to see         if some submodule exists, ``get_submodule`` should always be         used.          Args:             target: The fully-qualified string name of the submodule                 to look for. (See above example for how to specify a                 fully-qualified string.)          Returns:             torch.nn.Module: The submodule referenced by ``target``          Raises:             AttributeError: If the target string references an invalid                 path or resolves to something that is not an                 ``nn.Module``
Return the parameter given by ``target`` if it exists, otherwise throw an error.          See the docstring for ``get_submodule`` for a more detailed         explanation of this method's functionality as well as how to         correctly specify ``target``.          Args:             target: The fully-qualified string name of the Parameter                 to look for. (See ``get_submodule`` for how to specify a                 fully-qualified string.)          Returns:             torch.nn.Parameter: The Parameter referenced by ``target``          Raises:             AttributeError: If the target string references an invalid                 path or resolves to something that is not an                 ``nn.Parameter``
Return the buffer given by ``target`` if it exists, otherwise throw an error.          See the docstring for ``get_submodule`` for a more detailed         explanation of this method's functionality as well as how to         correctly specify ``target``.          Args:             target: The fully-qualified string name of the buffer                 to look for. (See ``get_submodule`` for how to specify a                 fully-qualified string.)          Returns:             torch.Tensor: The buffer referenced by ``target``          Raises:             AttributeError: If the target string references an invalid                 path or resolves to something that is not a                 buffer
Return any extra state to include in the module's state_dict.          Implement this and a corresponding :func:`set_extra_state` for your module         if you need to store extra state. This function is called when building the         module's `state_dict()`.          Note that extra state should be picklable to ensure working serialization         of the state_dict. We only provide provide backwards compatibility guarantees         for serializing Tensors; other objects may break backwards compatibility if         their serialized pickled form changes.          Returns:             object: Any extra state to store in the module's state_dict
Set extra state contained in the loaded `state_dict`.          This function is called from :func:`load_state_dict` to handle any extra state         found within the `state_dict`. Implement this function and a corresponding         :func:`get_extra_state` for your module if you need to store extra state within its         `state_dict`.          Args:             state (dict): Extra state from the `state_dict`
r""""""Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.          Typical use includes initializing the parameters of a model         (see also :ref:`nn-init-doc`).          Args:             fn (:class:`Module` -> None): function to be applied to each submodule          Returns:             Module: self          Example::              >>> @torch.no_grad()             >>> def init_weights(m):             >>>     print(m)             >>>     if type(m) == nn.Linear:             >>>         m.weight.fill_(1.0)             >>>         print(m.weight)             >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))             >>> net.apply(init_weights)             Linear(in_features=2, out_features=2, bias=True)             Parameter containing:             tensor([[1., 1.],                     [1., 1.]], requires_grad=True)             Linear(in_features=2, out_features=2, bias=True)             Parameter containing:             tensor([[1., 1.],                     [1., 1.]], requires_grad=True)             Sequential(               (0): Linear(in_features=2, out_features=2, bias=True)               (1): Linear(in_features=2, out_features=2, bias=True)             )
r""""""Move all model parameters and buffers to the GPU.          This also makes associated parameters and buffers different objects. So         it should be called before constructing optimizer if the module will         live on GPU while being optimized.          .. note::             This method modifies the module in-place.          Args:             device (int, optional): if specified, all parameters will be                 copied to that device          Returns:             Module: self
r""""""Move all model parameters and buffers to the IPU.          This also makes associated parameters and buffers different objects. So         it should be called before constructing optimizer if the module will         live on IPU while being optimized.          .. note::             This method modifies the module in-place.          Arguments:             device (int, optional): if specified, all parameters will be                 copied to that device          Returns:             Module: self
r""""""Move all model parameters and buffers to the XPU.          This also makes associated parameters and buffers different objects. So         it should be called before constructing optimizer if the module will         live on XPU while being optimized.          .. note::             This method modifies the module in-place.          Arguments:             device (int, optional): if specified, all parameters will be                 copied to that device          Returns:             Module: self
r""""""Move all model parameters and buffers to the CPU.          .. note::             This method modifies the module in-place.          Returns:             Module: self
r""""""Casts all parameters and buffers to :attr:`dst_type`.          .. note::             This method modifies the module in-place.          Args:             dst_type (type or string): the desired type          Returns:             Module: self
r""""""Casts all floating point parameters and buffers to ``float`` datatype.          .. note::             This method modifies the module in-place.          Returns:             Module: self
r""""""Casts all floating point parameters and buffers to ``double`` datatype.          .. note::             This method modifies the module in-place.          Returns:             Module: self
r""""""Casts all floating point parameters and buffers to ``half`` datatype.          .. note::             This method modifies the module in-place.          Returns:             Module: self
r""""""Casts all floating point parameters and buffers to ``bfloat16`` datatype.          .. note::             This method modifies the module in-place.          Returns:             Module: self
r""""""Move the parameters and buffers to the specified device without copying storage.          Args:             device (:class:`torch.device`): The desired device of the parameters                 and buffers in this module.             recurse (bool): Whether parameters and buffers of submodules should                 be recursively moved to the specified device.          Returns:             Module: self
r""""""Move and/or cast the parameters and buffers.          This can be called as          .. function:: to(device=None, dtype=None, non_blocking=False)            :noindex:          .. function:: to(dtype, non_blocking=False)            :noindex:          .. function:: to(tensor, non_blocking=False)            :noindex:          .. function:: to(memory_format=torch.channels_last)            :noindex:          Its signature is similar to :meth:`torch.Tensor.to`, but only accepts         floating point or complex :attr:`dtype`\ s. In addition, this method will         only cast the floating point or complex parameters and buffers to :attr:`dtype`         (if given). The integral parameters and buffers will be moved         :attr:`device`, if that is given, but with dtypes unchanged. When         :attr:`non_blocking` is set, it tries to convert/move asynchronously         with respect to the host if possible, e.g., moving CPU Tensors with         pinned memory to CUDA devices.          See below for examples.          .. note::             This method modifies the module in-place.          Args:             device (:class:`torch.device`): the desired device of the parameters                 and buffers in this module             dtype (:class:`torch.dtype`): the desired floating point or complex dtype of                 the parameters and buffers in this module             tensor (torch.Tensor): Tensor whose dtype and device are the desired                 dtype and device for all parameters and buffers in this module             memory_format (:class:`torch.memory_format`): the desired memory                 format for 4D parameters and buffers in this module (keyword                 only argument)          Returns:             Module: self          Examples::              >>> # xdoctest: +IGNORE_WANT(""non-deterministic"")             >>> linear = nn.Linear(2, 2)             >>> linear.weight             Parameter containing:             tensor([[ 0.1913, -0.3420],                     [-0.5113, -0.2325]])             >>> linear.to(torch.double)             Linear(in_features=2, out_features=2, bias=True)             >>> linear.weight             Parameter containing:             tensor([[ 0.1913, -0.3420],                     [-0.5113, -0.2325]], dtype=torch.float64)             >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)             >>> gpu1 = torch.device(""cuda:1"")             >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)             Linear(in_features=2, out_features=2, bias=True)             >>> linear.weight             Parameter containing:             tensor([[ 0.1914, -0.3420],                     [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')             >>> cpu = torch.device(""cpu"")             >>> linear.to(cpu)             Linear(in_features=2, out_features=2, bias=True)             >>> linear.weight             Parameter containing:             tensor([[ 0.1914, -0.3420],                     [-0.5112, -0.2324]], dtype=torch.float16)              >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)             >>> linear.weight             Parameter containing:             tensor([[ 0.3741+0.j,  0.2382+0.j],                     [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)             >>> linear(torch.ones(3, 2, dtype=torch.cdouble))             tensor([[0.6122+0.j, 0.1150+0.j],                     [0.6122+0.j, 0.1150+0.j],                     [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
r""""""Register a backward pre-hook on the module.          The hook will be called every time the gradients for the module are computed.         The hook should have the following signature::              hook(module, grad_output) -> tuple[Tensor] or None          The :attr:`grad_output` is a tuple. The hook should         not modify its arguments, but it can optionally return a new gradient with         respect to the output that will be used in place of :attr:`grad_output` in         subsequent computations. Entries in :attr:`grad_output` will be ``None`` for         all non-Tensor arguments.          For technical reasons, when this hook is applied to a Module, its forward function will         receive a view of each Tensor passed to the Module. Similarly the caller will receive a view         of each Tensor returned by the Module's forward function.          .. warning ::             Modifying inputs inplace is not allowed when using backward hooks and             will raise an error.          Args:             hook (Callable): The user-defined hook to be registered.             prepend (bool): If true, the provided ``hook`` will be fired before                 all existing ``backward_pre`` hooks on this                 :class:`torch.nn.modules.Module`. Otherwise, the provided                 ``hook`` will be fired after all existing ``backward_pre`` hooks                 on this :class:`torch.nn.modules.Module`. Note that global                 ``backward_pre`` hooks registered with                 :func:`register_module_full_backward_pre_hook` will fire before                 all hooks registered by this method.          Returns:             :class:`torch.utils.hooks.RemovableHandle`:                 a handle that can be used to remove the added hook by calling                 ``handle.remove()``
r""""""Register a backward hook on the module.          This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and         the behavior of this function will change in future versions.          Returns:             :class:`torch.utils.hooks.RemovableHandle`:                 a handle that can be used to remove the added hook by calling                 ``handle.remove()``
r""""""Register a backward hook on the module.          The hook will be called every time the gradients with respect to a module         are computed, i.e. the hook will execute if and only if the gradients with         respect to module outputs are computed. The hook should have the following         signature::              hook(module, grad_input, grad_output) -> tuple(Tensor) or None          The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients         with respect to the inputs and outputs respectively. The hook should         not modify its arguments, but it can optionally return a new gradient with         respect to the input that will be used in place of :attr:`grad_input` in         subsequent computations. :attr:`grad_input` will only correspond to the inputs given         as positional arguments and all kwarg arguments are ignored. Entries         in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor         arguments.          For technical reasons, when this hook is applied to a Module, its forward function will         receive a view of each Tensor passed to the Module. Similarly the caller will receive a view         of each Tensor returned by the Module's forward function.          .. warning ::             Modifying inputs or outputs inplace is not allowed when using backward hooks and             will raise an error.          Args:             hook (Callable): The user-defined hook to be registered.             prepend (bool): If true, the provided ``hook`` will be fired before                 all existing ``backward`` hooks on this                 :class:`torch.nn.modules.Module`. Otherwise, the provided                 ``hook`` will be fired after all existing ``backward`` hooks on                 this :class:`torch.nn.modules.Module`. Note that global                 ``backward`` hooks registered with                 :func:`register_module_full_backward_hook` will fire before                 all hooks registered by this method.          Returns:             :class:`torch.utils.hooks.RemovableHandle`:                 a handle that can be used to remove the added hook by calling                 ``handle.remove()``
r""""""Return the backward hooks for use in the call function.          It returns two lists, one with the full backward hooks and one with the non-full         backward hooks.
r""""""Register a forward pre-hook on the module.          The hook will be called every time before :func:`forward` is invoked.           If ``with_kwargs`` is false or not specified, the input contains only         the positional arguments given to the module. Keyword arguments won't be         passed to the hooks and only to the ``forward``. The hook can modify the         input. User can either return a tuple or a single modified value in the         hook. We will wrap the value into a tuple if a single value is returned         (unless that value is already a tuple). The hook should have the         following signature::              hook(module, args) -> None or modified input          If ``with_kwargs`` is true, the forward pre-hook will be passed the         kwargs given to the forward function. And if the hook modifies the         input, both the args and kwargs should be returned. The hook should have         the following signature::              hook(module, args, kwargs) -> None or a tuple of modified input and kwargs          Args:             hook (Callable): The user defined hook to be registered.             prepend (bool): If true, the provided ``hook`` will be fired before                 all existing ``forward_pre`` hooks on this                 :class:`torch.nn.modules.Module`. Otherwise, the provided                 ``hook`` will be fired after all existing ``forward_pre`` hooks                 on this :class:`torch.nn.modules.Module`. Note that global                 ``forward_pre`` hooks registered with                 :func:`register_module_forward_pre_hook` will fire before all                 hooks registered by this method.                 Default: ``False``             with_kwargs (bool): If true, the ``hook`` will be passed the kwargs                 given to the forward function.                 Default: ``False``          Returns:             :class:`torch.utils.hooks.RemovableHandle`:                 a handle that can be used to remove the added hook by calling                 ``handle.remove()``
r""""""Register a forward hook on the module.          The hook will be called every time after :func:`forward` has computed an output.          If ``with_kwargs`` is ``False`` or not specified, the input contains only         the positional arguments given to the module. Keyword arguments won't be         passed to the hooks and only to the ``forward``. The hook can modify the         output. It can modify the input inplace but it will not have effect on         forward since this is called after :func:`forward` is called. The hook         should have the following signature::              hook(module, args, output) -> None or modified output          If ``with_kwargs`` is ``True``, the forward hook will be passed the         ``kwargs`` given to the forward function and be expected to return the         output possibly modified. The hook should have the following signature::              hook(module, args, kwargs, output) -> None or modified output          Args:             hook (Callable): The user defined hook to be registered.             prepend (bool): If ``True``, the provided ``hook`` will be fired                 before all existing ``forward`` hooks on this                 :class:`torch.nn.modules.Module`. Otherwise, the provided                 ``hook`` will be fired after all existing ``forward`` hooks on                 this :class:`torch.nn.modules.Module`. Note that global                 ``forward`` hooks registered with                 :func:`register_module_forward_hook` will fire before all hooks                 registered by this method.                 Default: ``False``             with_kwargs (bool): If ``True``, the ``hook`` will be passed the                 kwargs given to the forward function.                 Default: ``False``             always_call (bool): If ``True`` the ``hook`` will be run regardless of                 whether an exception is raised while calling the Module.                 Default: ``False``          Returns:             :class:`torch.utils.hooks.RemovableHandle`:                 a handle that can be used to remove the added hook by calling                 ``handle.remove()``
r""""""Register a state-dict hook.          These hooks will be called with arguments: `self`, `state_dict`,         `prefix`, `local_metadata`, after the `state_dict` of `self` is set.         Note that only parameters and buffers of `self` or its children are         guaranteed to exist in `state_dict`. The hooks may modify `state_dict`         inplace or return a new one.
r""""""Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.          These hooks will be called with arguments: ``self``, ``prefix``,         and ``keep_vars`` before calling ``state_dict`` on ``self``. The registered         hooks can be used to perform pre-processing before the ``state_dict``         call is made.
r""""""Save module state to the `destination` dictionary.          The `destination` dictionary will contain the state         of the module, but not its descendants. This is called on every         submodule in :meth:`~torch.nn.Module.state_dict`.          In rare cases, subclasses can achieve class-specific behavior by         overriding this method with custom logic.          Args:             destination (dict): a dict where state will be stored             prefix (str): the prefix for parameters and buffers used in this                 module
r""""""Return a dictionary containing references to the whole state of the module.          Both parameters and persistent buffers (e.g. running averages) are         included. Keys are corresponding parameter and buffer names.         Parameters and buffers set to ``None`` are not included.          .. note::             The returned object is a shallow copy. It contains references             to the module's parameters and buffers.          .. warning::             Currently ``state_dict()`` also accepts positional arguments for             ``destination``, ``prefix`` and ``keep_vars`` in order. However,             this is being deprecated and keyword arguments will be enforced in             future releases.          .. warning::             Please avoid the use of argument ``destination`` as it is not             designed for end-users.          Args:             destination (dict, optional): If provided, the state of module will                 be updated into the dict and the same object is returned.                 Otherwise, an ``OrderedDict`` will be created and returned.                 Default: ``None``.             prefix (str, optional): a prefix added to parameter and buffer                 names to compose the keys in state_dict. Default: ``''``.             keep_vars (bool, optional): by default the :class:`~torch.Tensor` s                 returned in the state dict are detached from autograd. If it's                 set to ``True``, detaching will not be performed.                 Default: ``False``.          Returns:             dict:                 a dictionary containing a whole state of the module          Example::              >>> # xdoctest: +SKIP(""undefined vars"")             >>> module.state_dict().keys()             ['bias', 'weight']
r""""""Register a pre-hook for the :meth:`~torch.nn.Module.load_state_dict` method.          These hooks will be called with arguments: `state_dict`, `prefix`,         `local_metadata`, `strict`, `missing_keys`, `unexpected_keys`,         `error_msgs`, before loading `state_dict` into `self`. These arguments         are exactly the same as those of `_load_from_state_dict`.          If ``with_module`` is ``True``, then the first argument to the hook is         an instance of the module.          Arguments:             hook (Callable): Callable hook that will be invoked before                 loading the state dict.             with_module (bool, optional): Whether or not to pass the module                 instance to the hook as the first parameter.
r""""""Register a post hook to be run after module's ``load_state_dict`` is called.          It should have the following signature::             hook(module, incompatible_keys) -> None          The ``module`` argument is the current module that this hook is registered         on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting         of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``         is a ``list`` of ``str`` containing the missing keys and         ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.          The given incompatible_keys can be modified inplace if needed.          Note that the checks performed when calling :func:`load_state_dict` with         ``strict=True`` are affected by modifications the hook makes to         ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either         set of keys will result in an error being thrown when ``strict=True``, and         clearing out both missing and unexpected keys will avoid an error.          Returns:             :class:`torch.utils.hooks.RemovableHandle`:                 a handle that can be used to remove the added hook by calling                 ``handle.remove()``
r""""""Copy parameters and buffers from :attr:`state_dict` into only this module, but not its descendants.          This is called on every submodule         in :meth:`~torch.nn.Module.load_state_dict`. Metadata saved for this         module in input :attr:`state_dict` is provided as :attr:`local_metadata`.         For state dicts without metadata, :attr:`local_metadata` is empty.         Subclasses can achieve class-specific backward compatible loading using         the version number at `local_metadata.get(""version"", None)`.         Additionally, :attr:`local_metadata` can also contain the key         `assign_to_params_buffers` that indicates whether keys should be         assigned their corresponding tensor in the state_dict.          .. note::             :attr:`state_dict` is not the same object as the input             :attr:`state_dict` to :meth:`~torch.nn.Module.load_state_dict`. So             it can be modified.          Args:             state_dict (dict): a dict containing parameters and                 persistent buffers.             prefix (str): the prefix for parameters and buffers used in this                 module             local_metadata (dict): a dict containing the metadata for this module.                 See             strict (bool): whether to strictly enforce that the keys in                 :attr:`state_dict` with :attr:`prefix` match the names of                 parameters and buffers in this module             missing_keys (list of str): if ``strict=True``, add missing keys to                 this list             unexpected_keys (list of str): if ``strict=True``, add unexpected                 keys to this list             error_msgs (list of str): error messages should be added to this                 list, and will be reported together in                 :meth:`~torch.nn.Module.load_state_dict`
r""""""Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.          If :attr:`strict` is ``True``, then         the keys of :attr:`state_dict` must exactly match the keys returned         by this module's :meth:`~torch.nn.Module.state_dict` function.          .. warning::             If :attr:`assign` is ``True`` the optimizer must be created after             the call to :attr:`load_state_dict`.          Args:             state_dict (dict): a dict containing parameters and                 persistent buffers.             strict (bool, optional): whether to strictly enforce that the keys                 in :attr:`state_dict` match the keys returned by this module's                 :meth:`~torch.nn.Module.state_dict` function. Default: ``True``             assign (bool, optional): whether to assign items in the state                 dictionary to their corresponding keys in the module instead                 of copying them inplace into the module's current parameters and buffers.                 When ``False``, the properties of the tensors in the current                 module are preserved while when ``True``, the properties of the                 Tensors in the state dict are preserved.                 Default: ``False``          Returns:             ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:                 * **missing_keys** is a list of str containing the missing keys                 * **unexpected_keys** is a list of str containing the unexpected keys          Note:             If a parameter or buffer is registered as ``None`` and its corresponding key             exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a             ``RuntimeError``.
r""""""Help yield various names + members of modules.
r""""""Return an iterator over module parameters.          This is typically passed to an optimizer.          Args:             recurse (bool): if True, then yields parameters of this module                 and all submodules. Otherwise, yields only parameters that                 are direct members of this module.          Yields:             Parameter: module parameter          Example::              >>> # xdoctest: +SKIP(""undefined vars"")             >>> for param in model.parameters():             >>>     print(type(param), param.size())             <class 'torch.Tensor'> (20L,)             <class 'torch.Tensor'> (20L, 1L, 5L, 5L)
r""""""Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.          Args:             prefix (str): prefix to prepend to all parameter names.             recurse (bool): if True, then yields parameters of this module                 and all submodules. Otherwise, yields only parameters that                 are direct members of this module.             remove_duplicate (bool, optional): whether to remove the duplicated                 parameters in the result. Defaults to True.          Yields:             (str, Parameter): Tuple containing the name and parameter          Example::              >>> # xdoctest: +SKIP(""undefined vars"")             >>> for name, param in self.named_parameters():             >>>     if name in ['bias']:             >>>         print(param.size())
r""""""Return an iterator over module buffers.          Args:             recurse (bool): if True, then yields buffers of this module                 and all submodules. Otherwise, yields only buffers that                 are direct members of this module.          Yields:             torch.Tensor: module buffer          Example::              >>> # xdoctest: +SKIP(""undefined vars"")             >>> for buf in model.buffers():             >>>     print(type(buf), buf.size())             <class 'torch.Tensor'> (20L,)             <class 'torch.Tensor'> (20L, 1L, 5L, 5L)
r""""""Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.          Args:             prefix (str): prefix to prepend to all buffer names.             recurse (bool, optional): if True, then yields buffers of this module                 and all submodules. Otherwise, yields only buffers that                 are direct members of this module. Defaults to True.             remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.          Yields:             (str, torch.Tensor): Tuple containing the name and buffer          Example::              >>> # xdoctest: +SKIP(""undefined vars"")             >>> for name, buf in self.named_buffers():             >>>     if name in ['running_var']:             >>>         print(buf.size())
r""""""Return an iterator over immediate children modules.          Yields:             Module: a child module
r""""""Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.          Yields:             (str, Module): Tuple containing a name and child module          Example::              >>> # xdoctest: +SKIP(""undefined vars"")             >>> for name, module in model.named_children():             >>>     if name in ['conv4', 'conv5']:             >>>         print(module)
r""""""Return an iterator over all modules in the network.          Yields:             Module: a module in the network          Note:             Duplicate modules are returned only once. In the following             example, ``l`` will be returned only once.          Example::              >>> l = nn.Linear(2, 2)             >>> net = nn.Sequential(l, l)             >>> for idx, m in enumerate(net.modules()):             ...     print(idx, '->', m)              0 -> Sequential(               (0): Linear(in_features=2, out_features=2, bias=True)               (1): Linear(in_features=2, out_features=2, bias=True)             )             1 -> Linear(in_features=2, out_features=2, bias=True)
r""""""Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.          Args:             memo: a memo to store the set of modules already added to the result             prefix: a prefix that will be added to the name of the module             remove_duplicate: whether to remove the duplicated module instances in the result                 or not          Yields:             (str, Module): Tuple of name and module          Note:             Duplicate modules are returned only once. In the following             example, ``l`` will be returned only once.          Example::              >>> l = nn.Linear(2, 2)             >>> net = nn.Sequential(l, l)             >>> for idx, m in enumerate(net.named_modules()):             ...     print(idx, '->', m)              0 -> ('', Sequential(               (0): Linear(in_features=2, out_features=2, bias=True)               (1): Linear(in_features=2, out_features=2, bias=True)             ))             1 -> ('0', Linear(in_features=2, out_features=2, bias=True))
r""""""Set the module in training mode.          This has any effect only on certain modules. See documentations of         particular modules for details of their behaviors in training/evaluation         mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,         etc.          Args:             mode (bool): whether to set training mode (``True``) or evaluation                          mode (``False``). Default: ``True``.          Returns:             Module: self
r""""""Set the module in evaluation mode.          This has any effect only on certain modules. See documentations of         particular modules for details of their behaviors in training/evaluation         mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,         etc.          This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.          See :ref:`locally-disable-grad-doc` for a comparison between         `.eval()` and several similar mechanisms that may be confused with it.          Returns:             Module: self
r""""""Change if autograd should record operations on parameters in this module.          This method sets the parameters' :attr:`requires_grad` attributes         in-place.          This method is helpful for freezing part of the module for finetuning         or training parts of a model individually (e.g., GAN training).          See :ref:`locally-disable-grad-doc` for a comparison between         `.requires_grad_()` and several similar mechanisms that may be confused with it.          Args:             requires_grad (bool): whether autograd should record operations on                                   parameters in this module. Default: ``True``.          Returns:             Module: self
r""""""Reset gradients of all model parameters.          See similar function under :class:`torch.optim.Optimizer` for more context.          Args:             set_to_none (bool): instead of setting to zero, set the grads to None.                 See :meth:`torch.optim.Optimizer.zero_grad` for details.
r""""""See :meth:`torch.Tensor.share_memory_`.
r""""""Set the extra representation of the module.          To print customized extra information, you should re-implement         this method in your own modules. Both single-line and multi-line         strings are acceptable.
Compile this Module's forward using :func:`torch.compile`.          This Module's `__call__` method is compiled and all arguments are passed as-is         to :func:`torch.compile`.          See :func:`torch.compile` for details on the arguments for this function.",[''],No_license_found,"Gutmann License
Gutmann License
mpi Permissive License
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Gutmann License
Gutmann License
mpi Permissive License
mpi Permissive License",0.0,0.0
62,62,72,linux-master/tools/testing/selftests/sysctl/sysctl.sh,Dual-license copyleft-next-0.3.1 GPL-2.0-or-later,566," Once these are enabled please leave them as-is. Write your own test, we have tons of space.
 !/bin/bash SPDX-License-Identifier: GPL-2.0-or-later OR copyleft-next-0.3.1 Copyright (C) 2017 Luis R. Rodriguez <mcgrof@kernel.org>
You used an int array
You are using an int
You are using an unsigned int
 After we echo in, to help diff we need to set on TEST_STR what we expect the result to be.
 Do not reset_vals, carry on the values from the last test. If we only echo in two digits the last two are left intact
 these look like negatives, but without a leading '-' are actually large positives (should be rejected as above despite being zero/+1/-1/INT_MIN/INT_MAX in the lower 32)
Kselftest framework requirement - SKIP code is 4.
 Do not reset_vals, carry on the values from the last test. Even if you use an int array, you are still restricted to MAX_DIGITS, this is a known limitation. Test limit works.","This performs a series tests against the proc sysctl interface.
Kselftest framework requirement - SKIP code is 4.
This represents
TEST_ID:TEST_COUNT:ENABLED:TARGET
write should fail and $TARGET should retain its original value
Your test must accept digits 3 and 4 to use this
You are using an int
You used an int array
You are using an unsigned int
First bit to set
String containing our list of bits to set
Add new bit to the list
Randomly make it a range
Only string sysctls support seeking/appending.
-ne 3 ]; then
$1""
-eq 1 ]; then
-eq 0 ]; then
 !/bin/bash SPDX-License-Identifier: GPL-2.0-or-later OR copyleft-next-0.3.1 Copyright (C) 2017 Luis R. Rodriguez <mcgrof@kernel.org>
 TEST_ID: is the test id number TEST_COUNT: number of times we should run the test ENABLED: 1 if enabled, 0 otherwise TARGET: test target file required on the test_sysctl module
 Once these are enabled please leave them as-is. Write your own test, we have tons of space.
 proc files get read a page at a time, which can confuse diff, and get you incorrect results on proc files with long data. To use diff against them you must first extract the output to a file, and then compare against that file.
 Now that we've validated the sanity of ""set_test"" and ""set_orig"", we can use those functions to set starting states before running specific behavioral tests.
 sysctl conversion functions receive a boolean sign and ulong magnitude; here we list the magnitudes we want to test (each of which will be tested in both positive and negative forms).  Since none of these values fit in 32 bits, writing them to an int- or uint-typed sysctl should fail.
 common boundary-condition values (zero, +1, -1, INT_MIN, and INT_MAX respectively) if truncated to lower 32 bits (potential for being falsely deemed in range)
 these look like negatives, but without a leading '-' are actually large positives (should be rejected as above despite being zero/+1/-1/INT_MIN/INT_MAX in the lower 32)
 Do not reset_vals, carry on the values from the last test. If we only echo in two digits the last two are left intact
 After we echo in, to help diff we need to set on TEST_STR what we expect the result to be.
 Do not reset_vals, carry on the values from the last test. Even if you use an int array, you are still restricted to MAX_DIGITS, this is a known limitation. Test limit works.
 Do not reset_vals, carry on the values from the last test. Now go over limit.
 Total length of bitmaps string to use, a bit under the maximum input size of the test node
 build up the string TEST_STR}"" -le ""$LENGTH"" ]; do Make sure next entry is discontiguous, skip ahead at least 2",[''],No_license_found,"Gutmann License
Xdebug License v 1.03
Gutmann License
Creative Commons Attribution Share Alike 2.1 Japan
eCos license version 2.0
Gutmann License
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
mpi Permissive License
Gutmann License",0.0,0.0
63,63,73,linux-master/tools/testing/selftests/sync/sync_wait.c,MIT,576,"sync fence wait tests*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
confirm you can successfully wait
Confirm fence isn't signaled","sync fence wait tests*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
Confirm fence isn't signaled
confirm you can successfully wait","['Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.']",MIT,"DSDP License
check-cvs License
mpi Permissive License",1.0,100.0
64,64,74,linux-master/tools/testing/selftests/sync/synctest.h,MIT,577,"sync tests*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
Stress test - merging
Stress test - consumer
Allocation tests
Stress test - parallelism
Fence merge tests
Fence wait tests
Fence tests with one timeline","sync tests*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
Allocation tests
Fence tests with one timeline
Fence merge tests
Fence wait tests
Stress test - parallelism
Stress test - consumer
Stress test - merging","['Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.']",MIT,"MIT Khronos - old variant
Creative Commons Attribution Share Alike 2.1 Japan
Open Group Test Suite License
Computational Use of Data Agreement v1.0
Creative Commons Attribution Share Alike 2.1 Japan
XPP License
mpi Permissive License
Zed License",1.0,100.0
65,65,75,linux-master/tools/testing/selftests/sync/sync_stress_consumer.c,MIT,581,"sync stress test: producer/consumer*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
IMPORTANT NOTE: if you see this test failing on your system, it may be* due to a shortage of file descriptors. Please ensure your system has* a sensible limit for this test to finish correctly.
Make sure we see an increment from every producer thread.* Vary the means by which we wait.
Returns 1 on error, 0 on success
Consumer thread runs here
Every producer increments the counter, the consumer* checks and erases it
Release the producer threads
Wait for the consumer to finish. Use alternate* means of waiting on the fence","sync stress test: producer/consumer*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
IMPORTANT NOTE: if you see this test failing on your system, it may be* due to a shortage of file descriptors. Please ensure your system has* a sensible limit for this test to finish correctly.
Returns 1 on error, 0 on success
Wait for the consumer to finish. Use alternate* means of waiting on the fence
Every producer increments the counter, the consumer* checks and erases it
Make sure we see an increment from every producer thread.* Vary the means by which we wait.
Release the producer threads
Consumer thread runs here","['Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.']",MIT,"MIT Khronos - old variant
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
Creative Commons Attribution Share Alike 2.1 Japan",1.0,100.0
66,66,76,pytorch-main/torch/utils/hipify/hipify_python.py,MIT,255," Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
 THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
The Python Hipify script. ## # Copyright (c) 2015-2016 Advanced Micro Devices, Inc. All rights reserved. #               2017-2018 Advanced Micro Devices, Inc. and #                         Facebook Inc. All rights reserved. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the ""Software""), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE.
 Copyright (c) 2015-2016 Advanced Micro Devices, Inc. All rights reserved. 2017-2018 Advanced Micro Devices, Inc. and Facebook Inc. All rights reserved.
 In general, we need to disambiguate the HIPified filename so that it gets a different name from the original filename, so that we don't overwrite the original file
If not found, look in include dirs one by one and first match wins
 This isn't set in stone; we might adjust this to support other naming conventions.
Here's the plan:
 We respect extensions, UNLESS you wrote the entire filename verbatim, in which case we always accept it","!/usr/bin/env python3
Hardcode the PyTorch template map
Exception raised for errors in the input.
Color coding for printing
Blah blah blah O(n) blah blah
for pytorch extensions, consider all files
Show what happened
Print the number of unsupported calls
Print the list of unsupported calls
Print the number of kernel launches
Concat the namespace with the kernel names. (Find cleaner way of doing this later).
The positions for relevant kernel components.
Count for balancing template
Status for whether we are parsing a certain item.
Parse the string character by character
Handle Templating Arguments
Handle Kernel Name
Case: Kernel name starts the string.
Finished
Potential ending point if we're already traversing a kernel's name.
Finished
Continue until we cannot find any more kernels anymore.
Get kernel starting position (starting from the previous ending point)
Get kernel ending position (adjust end point past the >>>)
Add to list of traversed kernels
Outside comments
In // xxx
In /* xxx */
In """"
Grab positional ranges of all kernel launches
Replace each CUDA kernel with a HIP kernel.
Get kernel components
Find parenthesis after kernel launch
Extract cuda kernel
Keep number of kernel launch params consistent (grid dims, group dims, stream, dynamic shared size)
Replace cuda kernel with hip kernel
Update the statistics
include to match the ""magic"" includes provided by NVCC.
Copy the input.
Check if one of the following headers is already included.
Rough logic to detect if we're inside device code
If device logic found, provide the necessary header.
__shared__
Here's the plan:
Concretely, we do the following:
- If the file name contains ""CUDA"", replace it with ""HIP"", AND
Special case to handle caffe2/core/THCCachingAllocator
Keep this synchronized with includes/ignores in build_amd.py
don't use ""special"" mappings for this specific linalg cublas file
Cribbed from https://stackoverflow.com/questions/42742810/speed-up-millions-of-regex-replacements-in-python-3/42789508#42789508
be careful not to pick up .cuh
get_hip_file_path needs a relative path to work correctly
unsupported_calls statistics reporting is broken atm
checks SPECIAL map first, and if a miss occurs, falls back to pytorch mappings
Header rewrites
if filename is one of the files being hipified for this extension
If include_current_dir True, look first in same dir as the including source file
If not found, look in include dirs one by one and first match wins
If header file not found, keep as is
Hipify header file first if needed
get_hip_file_path needs a relative path to work correctly
CMakeLists.txt rewrites
Perform Kernel Launch Replacements
Replace std:: with non-std:: versions
Include header if device code is contained.
Don't write out identical hipified files for extensions if dirpath has not changed
Add hipify breadcrumb for C-style files to avoid re-hipification
Search for final parenthesis
Finished all arguments
Add final argument
Finished current argument
Verify the project directory exists.
If no output directory, provide a default one.
Copy from project directory to output directory if not done already.
List all files in header_include_paths to ensure they are hipified
Preprocessing statistics.
Show detailed summary
 Copyright (c) 2015-2016 Advanced Micro Devices, Inc. All rights reserved. 2017-2018 Advanced Micro Devices, Inc. and Facebook Inc. All rights reserved.
 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
 The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
 THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 To the programmer, the output of hipify most likely are intermediates. This class allows users of hipify to ask for a cleanup by running the hipify and compilation in a with instantiating this context manager class with keep_intermediates=False. The main usecase is the cpp_extensions, specifically the load method. It is a good idea to keep intermediates (in case of errors or to not recompile unchanged files), but in cases where you don't want to keep them (e.g. in the CI), this can be used to remove files.
 This is a very rough heuristic; really, we want to avoid scanning any file which is not checked into source control, but this script needs to work even if you're in a Git or Hg checkout, so easier to just block the biggest time sinks that won't matter in the end.
 We respect extensions, UNLESS you wrote the entire filename verbatim, in which case we always accept it
 Replace comments and string literals from the code so that find_kernel_bounds does not wrongly capture kernels in comments and string literals. This function replaces them with ""x"" to keep positions.
 At the moment, some PyTorch source files are HIPified in place.  The predicate is_out_of_place tells us if this is the case or not.
 In general, we need to disambiguate the HIPified filename so that it gets a different name from the original filename, so that we don't overwrite the original file
 There's a lot of different naming conventions across PyTorch and Caffe2, but the general recipe is to convert occurrences of cuda/gpu to hip, and add hip if there are no occurrences of cuda/gpu anywhere.
 - If there is a directory component named ""cuda"", replace it with ""hip"", AND
 - ALWAYS replace '.cu' with '.hip', because those files contain CUDA kernels that needs to be hipified and processed with hip compiler
 - If we are not hipifying a PyTorch extension, and the parent directory name did not change as a result of the above transformations, insert ""hip"" in the file path as the direct parent folder of the file
 - If we are hipifying a PyTorch extension, and the parent directory name as well as the filename (incl. extension) did not change as a result of the above transformations, insert ""_hip"" in the filename
 This isn't set in stone; we might adjust this to support other naming conventions.
 In PyTorch, we map cuBLAS->rocBLAS and cuSPARSE->hipSPARSE. Note the prefix, roc versus hip. The 'hip' APIs offer a more direct CUDA-friendly mapping, but calling rocBLAS directly has better performance. Unfortunately, the roc* types and hip* types differ, i.e., rocblas_float_complex versus hipComplex. In the case of SPARSE, we must use the hip types for complex instead of the roc types, but the pytorch mappings assume roc. Therefore, we create a new SPARSE mapping that has a higher priority. Its mappings will trigger first, and only when a miss occurs will the lower-priority pytorch mapping take place. When a file contains ""sparse"" in the filename, a mapping marked with API_SPARSE is preferred over other choices. Similarly, ""linalg"" files require rocBLAS -> hipSOLVER so they also need special handling.
 if src is already in PYTORCH_MAP and dst belongs to API_SPECIAL do not overwrite PYTORCH_MAP, store dst separately
 Replace the extern __shared__ NOTE: No longer needed after transition from hcc to hipclang. output_source = replace_extern_shared(output_source)
adds dim3() to the second and third arguments in the kernel launch
The Python Hipify script. ## # Copyright (c) 2015-2016 Advanced Micro Devices, Inc. All rights reserved. #               2017-2018 Advanced Micro Devices, Inc. and #                         Facebook Inc. All rights reserved. # # Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files (the ""Software""), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish, distribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the following conditions: # # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # # THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE # AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN # THE SOFTWARE.
This dictionary provides the mapping from PyTorch kernel template types
Context Manager to clean up generated files
Helper method to see if filename ends with certain extension
Replace the CUDA style Kernel launches with the HIP style kernel launches.
Finds the starting and ending points for all kernel launches in the string.
Generalization for finding a balancing closure group           if group = [""("", "")""], then finds the first balanced parentheses.          if group = [""{"", ""}""], then finds the first balanced bracket.      Given an input string, a starting position in the input string, and the group type,     find_closure_group returns the positions of group[0] and group[1] as a tuple.      Example:         >>> find_closure_group(""(hi)"", 0, [""("", "")""])         (0, 3)
Finds the first balanced parantheses.
Finds the first balanced bracket.
FIXME: Temporarily replace std:: invocations of math functions         with non-std:: versions to prevent linker errors NOTE: This         can lead to correctness issues when running tests, since the         correct version of the math function (exp/expf) might not get         called.  Plan is to remove this function once HIP supports         std:: math function calls inside device code
If the file makes kernel builtin calls and does not include the cuda_runtime.h header,     then automatically add an #include to match the ""magic"" includes provided by NVCC.     TODO:         Update logic to ignore cases where the cuda_runtime.h is included by another file.
Match extern __shared__ type foo[]; syntax and use HIP_DYNAMIC_SHARED() MACRO instead.        https://github.com/ROCm-Developer-Tools/HIP/blob/master/docs/markdown/hip_kernel_language.md#__shared__     Example:         ""extern __shared__ char smemChar[];"" => ""HIP_DYNAMIC_SHARED( char, smemChar)""         ""extern __shared__ unsigned char smem[];"" => ""HIP_DYNAMIC_SHARED( unsigned char, my_smem)
Returns the new name of the hipified file
Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern.
Returns a HipifyResult object with the following details:     ""hipified_path"" : absolute path of hipified source file     ""status""        : ""ok""      if hipified file was written out                       ""skipped"" if an identical hipified file already existed or hipified file couldn't be written out                       ""ignored"" if the source file was a hipified file itself or not meant to be hipified     ""current_state"" : CurrentState.INITIALIZED if source file is first ready to be hipified                       CurrentState.DONE if source file is done with hipification process
Executes the CUDA -> HIP conversion on the specified file.
Static global kernels in HIP results in a compilation error.
Return the list of arguments in the upcoming function parameter closure.         Example:         string (input): '(blocks, threads, 0, THCState_getCurrentStream(state))'         arguments (output):             '[{'start': 1, 'end': 7},             {'start': 8, 'end': 16},             {'start': 17, 'end': 19},             {'start': 20, 'end': 53}]'
ArgumentParser doesn't support type=bool. Thus, this helper method will convert","['SPDX-License-Identifier: GPL-2.0-only', ' Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:', ' The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.', ' THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.', ' Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:', ' The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.', ' THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.']",MIT,"feh License
Historical Permission Notice and Disclaimer - sell xserver variant with MIT disclaimer
curl License
Hippocratic License 2.1
AMD's plpa_map.c License
dvipdfm License
Time::ParseDate License
Gutmann License
Gutmann License
threeparttable License",1.0,85.71428571428572
67,67,77,linux-master/tools/testing/selftests/sync/sw_sync.h,MIT,587,"sw_sync abstraction**  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2013 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
sw_sync is mainly intended for testing and should not be compiled into* production kernels","sw_sync abstraction**  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2013 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
sw_sync is mainly intended for testing and should not be compiled into* production kernels","['Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2013 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR*  OTHER DEALINGS IN THE SOFTWARE.', 'SPDX-License-Identifier: GPL-2.0-only']",MIT,"Systemics BSD variant license
Systemics W3Works BSD variant license",1.0,50.0
68,68,78,pytorch-main/torch/utils/model_dump/preact.mjs,MIT,244,"var n,l,u,i,t,o,r={},f=[],e=/acit|ex(?:s|g|n|p|$)|rph|grid|ows|mnc|ntw|ine[ch]|zoo|^ord|itera/i;function c(e,n){for(var t in n)e[t]=n[t];return e}function s(e){var n=e.parentNode;n&&n.removeChild(e)}function a(e,n,t){var _,l,o,r=arguments,i={};for(o in n)""key""==o?_=n[o]:""ref""==o?l=n[o]:i[o]=n[o];if(arguments.length>3)for(t=[t],o=3;o<arguments.length;o++)t.push(r[o]);if(null!=t&&(i.children=t),""function""==typeof e&&null!=e.defaultProps)for(o in e.defaultProps)void 0===i[o]&&(i[o]=e.defaultProps[o]);return v(e,i,_,l,null)}function v(e,t,_,l,o){var r={type:e,props:t,key:_,ref:l,__k:null,__:null,__b:0,__e:null,__d:void 0,__c:null,__h:null,constructor:void 0,__v:null==o?++n.__v:o};return null!=n.vnode&&n.vnode(r),r}function h(){return{current:null}}function y(e){return e.children}function p(e,n){this.props=e,this.context=n}function d(e,n){if(null==n)return e.__?d(e.__,e.__.__k.indexOf(e)+1):null;for(var t;n<e.__k.length;n++)if(null!=(t=e.__k[n])&&null!=t.__e)return t.__e;return""function""==typeof e.type?d(e):null}function _(e){var n,t;if(null!=(e=e.__)&&null!=e.__c){for(e.__e=e.__c.base=null,n=0;n<e.__k.length;n++)if(null!=(t=e.__k[n])&&null!=t.__e){e.__e=e.__c.base=t.__e;break}return _(e)}}function k(e){(!e.__d&&(e.__d=!0)&&u.push(e)&&!b.__r++||t!==n.debounceRendering)&&((t=n.debounceRendering)||i)(b)}function b(){for(var e;b.__r=u.length;)e=u.sort(function(e,n){return e.__v.__b-n.__v.__b}),u=[],e.some(function(e){var n,t,l,o,r,i;e.__d&&(r=(o=(n=e).__v).__e,(i=n.__P)&&(t=[],(l=c({},o)).__v=o.__v+1,I(i,o,l,n.__n,void 0!==i.ownerSVGElement,null!=o.__h?[r]:null,t,null==r?d(o):r,o.__h),T(t,o),o.__e!=r&&_(o)))})}function m(e,n,t,_,l,o,i,u,s,c){var p,a,h,m,k,b,C,P=_&&_.__k||f,S=P.length;for(t.__k=[],p=0;p<n.length;p++)if(null!=(m=t.__k[p]=null==(m=n[p])||""boolean""==typeof m?null:""string""==typeof m||""number""==typeof m||""bigint""==typeof m?v(null,m,null,null,m):Array.isArray(m)?v(y,{children:m},null,null,null):m.__b>0?v(m.type,m.props,m.key,null,m.__v):m)){if(m.__=t,m.__b=t.__b+1,null===(h=P[p])||h&&m.key==h.key&&m.type===h.type)P[p]=void 0;else for(a=0;a<S;a++){if((h=P[a])&&m.key==h.key&&m.type===h.type){P[a]=void 0;break}h=null}I(e,m,h=h||r,l,o,i,u,s,c),k=m.__e,(a=m.ref)&&h.ref!=a&&(C||(C=[]),h.ref&&C.push(h.ref,null,m),C.push(a,m.__c||k,m)),null!=k?(null==b&&(b=k),""function""==typeof m.type&&null!=m.__k&&m.__k===h.__k?m.__d=s=g(m,s,e):s=x(e,m,h,P,k,s),c||""option""!==t.type?""function""==typeof t.type&&(t.__d=s):e.value=""""):s&&h.__e==s&&s.parentNode!=e&&(s=d(h))}for(t.__e=b,p=S;p--;)null!=P[p]&&(""function""==typeof t.type&&null!=P[p].__e&&P[p].__e==t.__d&&(t.__d=d(_,p+1)),L(P[p],P[p]));if(C)for(p=0;p<C.length;p++)z(C[p],C[++p],C[++p])}function g(e,n,t){var _,l;for(_=0;_<e.__k.length;_++)(l=e.__k[_])&&(l.__=e,n=""function""==typeof l.type?g(l,n,t):x(t,l,l,e.__k,l.__e,n));return n}function w(e,n){return n=n||[],null==e||""boolean""==typeof e||(Array.isArray(e)?e.some(function(e){w(e,n)}):n.push(e)),n}function x(e,n,t,_,l,o){var r,i,u;if(void 0!==n.__d)r=n.__d,n.__d=void 0;else if(null==t||l!=o||null==l.parentNode)e:if(null==o||o.parentNode!==e)e.appendChild(l),r=null;else{for(i=o,u=0;(i=i.nextSibling)&&u<_.length;u+=2)if(i==l)break e;e.insertBefore(l,o),r=o}return void 0!==r?r:l.nextSibling}function A(e,n,t,_,l){var o;for(o in t)""children""===o||""key""===o||o in n||C(e,o,null,t[o],_);for(o in n)l&&""function""!=typeof n[o]||""children""===o||""key""===o||""value""===o||""checked""===o||t[o]===n[o]||C(e,o,n[o],t[o],_)}function P(n,t,_){""-""===t[0]?n.setProperty(t,_):n[t]=null==_?"""":""number""!=typeof _||e.test(t)?_:_+""px""}function C(e,n,t,_,l){var o;e:if(""style""===n)if(""string""==typeof t)e.style.cssText=t;else{if(""string""==typeof _&&(e.style.cssText=_=""""),_)for(n in _)t&&n in t||P(e.style,n,"""");if(t)for(n in t)_&&t[n]===_[n]||P(e.style,n,t[n])}else if(""o""===n[0]&&""n""===n[1])o=n!==(n=n.replace(/Capture$/,"""")),n=n.toLowerCase()in e?n.toLowerCase().slice(2):n.slice(2),e.l||(e.l={}),e.l[n+o]=t,t?_||e.addEventListener(n,o?H:$,o):e.removeEventListener(n,o?H:$,o);else if(""dangerouslySetInnerHTML""!==n){if(l)n=n.replace(/xlink[H:h]/,""h"").replace(/sName$/,""s"");else if(""href""!==n&&""list""!==n&&""form""!==n&&""tabIndex""!==n&&""download""!==n&&n in e)try{e[n]=null==t?"""":t;break e}catch(e){}""function""==typeof t||(null!=t&&(!1!==t||""a""===n[0]&&""r""===n[1])?e.setAttribute(n,t):e.removeAttribute(n))}}function $(e){this.l[e.type+!1](n.event?n.event(e):e)}function H(e){this.l[e.type+!0](n.event?n.event(e):e)}function I(e,t,_,l,o,r,i,u,s){var f,a,d,h,v,k,g,b,C,x,P,S=t.type;if(void 0!==t.constructor)return null;null!=_.__h&&(s=_.__h,u=t.__e=_.__e,t.__h=null,r=[u]),(f=n.__b)&&f(t);try{e:if(""function""==typeof S){if(b=t.props,C=(f=S.contextType)&&l[f.__c],x=f?C?C.props.value:f.__:l,_.__c?g=(a=t.__c=_.__c).__=a.__E:(""prototype""in S&&S.prototype.render?t.__c=a=new S(b,x):(t.__c=a=new p(b,x),a.constructor=S,a.render=M),C&&C.sub(a),a.props=b,a.state||(a.state={}),a.context=x,a.__n=l,d=a.__d=!0,a.__h=[]),null==a.__s&&(a.__s=a.state),null!=S.getDerivedStateFromProps&&(a.__s==a.state&&(a.__s=c({},a.__s)),c(a.__s,S.getDerivedStateFromProps(b,a.__s))),h=a.props,v=a.state,d)null==S.getDerivedStateFromProps&&null!=a.componentWillMount&&a.componentWillMount(),null!=a.componentDidMount&&a.__h.push(a.componentDidMount);else{if(null==S.getDerivedStateFromProps&&b!==h&&null!=a.componentWillReceiveProps&&a.componentWillReceiveProps(b,x),!a.__e&&null!=a.shouldComponentUpdate&&!1===a.shouldComponentUpdate(b,a.__s,x)||t.__v===_.__v){a.props=b,a.state=a.__s,t.__v!==_.__v&&(a.__d=!1),a.__v=t,t.__e=_.__e,t.__k=_.__k,t.__k.forEach(function(e){e&&(e.__=t)}),a.__h.length&&i.push(a);break e}null!=a.componentWillUpdate&&a.componentWillUpdate(b,a.__s,x),null!=a.componentDidUpdate&&a.__h.push(function(){a.componentDidUpdate(h,v,k)})}a.context=x,a.props=b,a.state=a.__s,(f=n.__r)&&f(t),a.__d=!1,a.__v=t,a.__P=e,f=a.render(a.props,a.state,a.context),a.state=a.__s,null!=a.getChildContext&&(l=c(c({},l),a.getChildContext())),d||null==a.getSnapshotBeforeUpdate||(k=a.getSnapshotBeforeUpdate(h,v)),P=null!=f&&f.type===y&&null==f.key?f.props.children:f,m(e,Array.isArray(P)?P:[P],t,_,l,o,r,i,u,s),a.base=t.__e,t.__h=null,a.__h.length&&i.push(a),g&&(a.__E=a.__=null),a.__e=!1}else null==r&&t.__v===_.__v?(t.__k=_.__k,t.__e=_.__e):t.__e=j(_.__e,t,_,l,o,r,i,s);(f=n.diffed)&&f(t)}catch(e){t.__v=null,(s||null!=r)&&(t.__e=u,t.__h=!!s,r[r.indexOf(u)]=null),n.__e(e,t,_)}}function T(e,t){n.__c&&n.__c(t,e),e.some(function(t){try{e=t.__h,t.__h=[],e.some(function(e){e.call(t)})}catch(e){n.__e(e,t.__v)}})}function j(e,n,t,_,l,o,i,u){var c,p,a,d,h=t.props,v=n.props,y=n.type,k=0;if(""svg""===y&&(l=!0),null!=o)for(;k<o.length;k++)if((c=o[k])&&(c===e||(y?c.localName==y:3==c.nodeType))){e=c,o[k]=null;break}if(null==e){if(null===y)return document.createTextNode(v);e=l?document.createElementNS(""http://www.w3.org/2000/svg"",y):document.createElement(y,v.is&&v),o=null,u=!1}if(null===y)h===v||u&&e.data===v||(e.data=v);else{if(o=o&&f.slice.call(e.childNodes),p=(h=t.props||r).dangerouslySetInnerHTML,a=v.dangerouslySetInnerHTML,!u){if(null!=o)for(h={},d=0;d<e.attributes.length;d++)h[e.attributes[d].name]=e.attributes[d].value;(a||p)&&(a&&(p&&a.__html==p.__html||a.__html===e.innerHTML)||(e.innerHTML=a&&a.__html||""""))}if(A(e,v,h,l,u),a)n.__k=[];else if(k=n.props.children,m(e,Array.isArray(k)?k:[k],n,t,_,l&&""foreignObject""!==y,o,i,e.firstChild,u),null!=o)for(k=o.length;k--;)null!=o[k]&&s(o[k]);u||(""value""in v&&void 0!==(k=v.value)&&(k!==e.value||""progress""===y&&!k)&&C(e,""value"",k,h.value,!1),""checked""in v&&void 0!==(k=v.checked)&&k!==e.checked&&C(e,""checked"",k,h.checked,!1))}return e}function z(e,t,_){try{""function""==typeof e?e(t):e.current=t}catch(e){n.__e(e,_)}}function L(e,t,_){var l,o,r;if(n.unmount&&n.unmount(e),(l=e.ref)&&(l.current&&l.current!==e.__e||z(l,null,t)),_||""function""==typeof e.type||(_=null!=(o=e.__e)),e.__e=e.__d=void 0,null!=(l=e.__c)){if(l.componentWillUnmount)try{l.componentWillUnmount()}catch(e){n.__e(e,t)}l.base=l.__P=null}if(l=e.__k)for(r=0;r<l.length;r++)l[r]&&L(l[r],t,_);null!=o&&s(o)}function M(e,n,t){return this.constructor(e,t)}function N(e,t,_){var l,o,i;n.__&&n.__(e,t),o=(l=""function""==typeof _)?null:_&&_.__k||t.__k,i=[],I(t,e=(!l&&_||t).__k=a(y,null,[e]),o||r,r,void 0!==t.ownerSVGElement,!l&&_?[_]:o?null:t.firstChild?f.slice.call(t.childNodes):null,i,!l&&_?_:o?o.__e:t.firstChild,l),T(i,e)}function O(e,n){N(e,n,O)}function S(e,n,t){var _,l,o,r=arguments,i=c({},e.props);for(o in n)""key""==o?_=n[o]:""ref""==o?l=n[o]:i[o]=n[o];if(arguments.length>3)for(t=[t],o=3;o<arguments.length;o++)t.push(r[o]);return null!=t&&(i.children=t),v(e.type,i,_||e.key,l||e.ref,null)}function q(e,n){var t={__c:n=""__cC""+o++,__:e,Consumer:function(e,n){return e.children(n)},Provider:function(e){var t,_;return this.getChildContext||(t=[],(_={})[n]=this,this.getChildContext=function(){return _},this.shouldComponentUpdate=function(e){this.props.value!==e.value&&t.some(k)},this.sub=function(e){t.push(e);var n=e.componentWillUnmount;e.componentWillUnmount=function(){t.splice(t.indexOf(e),1),n&&n.call(e)}}),e.children}};return t.Provider.__=t.Consumer.contextType=t}n={__e:function(e,n){for(var t,_,l;n=n.__;)if((t=n.__c)&&!t.__)try{if((_=t.constructor)&&null!=_.getDerivedStateFromError&&(t.setState(_.getDerivedStateFromError(e)),l=t.__d),null!=t.componentDidCatch&&(t.componentDidCatch(e),l=t.__d),l)return t.__E=t}catch(n){e=n}throw e},__v:0},l=function(e){return null!=e&&void 0===e.constructor},p.prototype.setState=function(e,n){var t;t=null!=this.__s&&this.__s!==this.state?this.__s:this.__s=c({},this.state),""function""==typeof e&&(e=e(c({},t),this.props)),e&&c(t,e),null!=e&&this.__v&&(n&&this.__h.push(n),k(this))},p.prototype.forceUpdate=function(e){this.__v&&(this.__e=!0,e&&this.__h.push(e),k(this))},p.prototype.render=y,u=[],i=""function""==typeof Promise?Promise.prototype.then.bind(Promise.resolve()):setTimeout,b.__r=0,o=0;export{N as render,O as hydrate,a as createElement,a as h,y as Fragment,h as createRef,l as isValidElement,p as Component,S as cloneElement,q as createContext,w as toChildArray,n as options};
// Preact, MIT License
","// Preact, MIT License
var n,l,u,i,t,o,r={},f=[],e=/acit|ex(?:s|g|n|p|$)|rph|grid|ows|mnc|ntw|ine[ch]|zoo|^ord|itera/i;function c(e,n){for(var t in n)e[t]=n[t];return e}function s(e){var n=e.parentNode;n&&n.removeChild(e)}function a(e,n,t){var _,l,o,r=arguments,i={};for(o in n)""key""==o?_=n[o]:""ref""==o?l=n[o]:i[o]=n[o];if(arguments.length>3)for(t=[t],o=3;o<arguments.length;o++)t.push(r[o]);if(null!=t&&(i.children=t),""function""==typeof e&&null!=e.defaultProps)for(o in e.defaultProps)void 0===i[o]&&(i[o]=e.defaultProps[o]);return v(e,i,_,l,null)}function v(e,t,_,l,o){var r={type:e,props:t,key:_,ref:l,__k:null,__:null,__b:0,__e:null,__d:void 0,__c:null,__h:null,constructor:void 0,__v:null==o?++n.__v:o};return null!=n.vnode&&n.vnode(r),r}function h(){return{current:null}}function y(e){return e.children}function p(e,n){this.props=e,this.context=n}function d(e,n){if(null==n)return e.__?d(e.__,e.__.__k.indexOf(e)+1):null;for(var t;n<e.__k.length;n++)if(null!=(t=e.__k[n])&&null!=t.__e)return t.__e;return""function""==typeof e.type?d(e):null}function _(e){var n,t;if(null!=(e=e.__)&&null!=e.__c){for(e.__e=e.__c.base=null,n=0;n<e.__k.length;n++)if(null!=(t=e.__k[n])&&null!=t.__e){e.__e=e.__c.base=t.__e;break}return _(e)}}function k(e){(!e.__d&&(e.__d=!0)&&u.push(e)&&!b.__r++||t!==n.debounceRendering)&&((t=n.debounceRendering)||i)(b)}function b(){for(var e;b.__r=u.length;)e=u.sort(function(e,n){return e.__v.__b-n.__v.__b}),u=[],e.some(function(e){var n,t,l,o,r,i;e.__d&&(r=(o=(n=e).__v).__e,(i=n.__P)&&(t=[],(l=c({},o)).__v=o.__v+1,I(i,o,l,n.__n,void 0!==i.ownerSVGElement,null!=o.__h?[r]:null,t,null==r?d(o):r,o.__h),T(t,o),o.__e!=r&&_(o)))})}function m(e,n,t,_,l,o,i,u,s,c){var p,a,h,m,k,b,C,P=_&&_.__k||f,S=P.length;for(t.__k=[],p=0;p<n.length;p++)if(null!=(m=t.__k[p]=null==(m=n[p])||""boolean""==typeof m?null:""string""==typeof m||""number""==typeof m||""bigint""==typeof m?v(null,m,null,null,m):Array.isArray(m)?v(y,{children:m},null,null,null):m.__b>0?v(m.type,m.props,m.key,null,m.__v):m)){if(m.__=t,m.__b=t.__b+1,null===(h=P[p])||h&&m.key==h.key&&m.type===h.type)P[p]=void 0;else for(a=0;a<S;a++){if((h=P[a])&&m.key==h.key&&m.type===h.type){P[a]=void 0;break}h=null}I(e,m,h=h||r,l,o,i,u,s,c),k=m.__e,(a=m.ref)&&h.ref!=a&&(C||(C=[]),h.ref&&C.push(h.ref,null,m),C.push(a,m.__c||k,m)),null!=k?(null==b&&(b=k),""function""==typeof m.type&&null!=m.__k&&m.__k===h.__k?m.__d=s=g(m,s,e):s=x(e,m,h,P,k,s),c||""option""!==t.type?""function""==typeof t.type&&(t.__d=s):e.value=""""):s&&h.__e==s&&s.parentNode!=e&&(s=d(h))}for(t.__e=b,p=S;p--;)null!=P[p]&&(""function""==typeof t.type&&null!=P[p].__e&&P[p].__e==t.__d&&(t.__d=d(_,p+1)),L(P[p],P[p]));if(C)for(p=0;p<C.length;p++)z(C[p],C[++p],C[++p])}function g(e,n,t){var _,l;for(_=0;_<e.__k.length;_++)(l=e.__k[_])&&(l.__=e,n=""function""==typeof l.type?g(l,n,t):x(t,l,l,e.__k,l.__e,n));return n}function w(e,n){return n=n||[],null==e||""boolean""==typeof e||(Array.isArray(e)?e.some(function(e){w(e,n)}):n.push(e)),n}function x(e,n,t,_,l,o){var r,i,u;if(void 0!==n.__d)r=n.__d,n.__d=void 0;else if(null==t||l!=o||null==l.parentNode)e:if(null==o||o.parentNode!==e)e.appendChild(l),r=null;else{for(i=o,u=0;(i=i.nextSibling)&&u<_.length;u+=2)if(i==l)break e;e.insertBefore(l,o),r=o}return void 0!==r?r:l.nextSibling}function A(e,n,t,_,l){var o;for(o in t)""children""===o||""key""===o||o in n||C(e,o,null,t[o],_);for(o in n)l&&""function""!=typeof n[o]||""children""===o||""key""===o||""value""===o||""checked""===o||t[o]===n[o]||C(e,o,n[o],t[o],_)}function P(n,t,_){""-""===t[0]?n.setProperty(t,_):n[t]=null==_?"""":""number""!=typeof _||e.test(t)?_:_+""px""}function C(e,n,t,_,l){var o;e:if(""style""===n)if(""string""==typeof t)e.style.cssText=t;else{if(""string""==typeof _&&(e.style.cssText=_=""""),_)for(n in _)t&&n in t||P(e.style,n,"""");if(t)for(n in t)_&&t[n]===_[n]||P(e.style,n,t[n])}else if(""o""===n[0]&&""n""===n[1])o=n!==(n=n.replace(/Capture$/,"""")),n=n.toLowerCase()in e?n.toLowerCase().slice(2):n.slice(2),e.l||(e.l={}),e.l[n+o]=t,t?_||e.addEventListener(n,o?H:$,o):e.removeEventListener(n,o?H:$,o);else if(""dangerouslySetInnerHTML""!==n){if(l)n=n.replace(/xlink[H:h]/,""h"").replace(/sName$/,""s"");else if(""href""!==n&&""list""!==n&&""form""!==n&&""tabIndex""!==n&&""download""!==n&&n in e)try{e[n]=null==t?"""":t;break e}catch(e){}""function""==typeof t||(null!=t&&(!1!==t||""a""===n[0]&&""r""===n[1])?e.setAttribute(n,t):e.removeAttribute(n))}}function $(e){this.l[e.type+!1](n.event?n.event(e):e)}function H(e){this.l[e.type+!0](n.event?n.event(e):e)}function I(e,t,_,l,o,r,i,u,s){var f,a,d,h,v,k,g,b,C,x,P,S=t.type;if(void 0!==t.constructor)return null;null!=_.__h&&(s=_.__h,u=t.__e=_.__e,t.__h=null,r=[u]),(f=n.__b)&&f(t);try{e:if(""function""==typeof S){if(b=t.props,C=(f=S.contextType)&&l[f.__c],x=f?C?C.props.value:f.__:l,_.__c?g=(a=t.__c=_.__c).__=a.__E:(""prototype""in S&&S.prototype.render?t.__c=a=new S(b,x):(t.__c=a=new p(b,x),a.constructor=S,a.render=M),C&&C.sub(a),a.props=b,a.state||(a.state={}),a.context=x,a.__n=l,d=a.__d=!0,a.__h=[]),null==a.__s&&(a.__s=a.state),null!=S.getDerivedStateFromProps&&(a.__s==a.state&&(a.__s=c({},a.__s)),c(a.__s,S.getDerivedStateFromProps(b,a.__s))),h=a.props,v=a.state,d)null==S.getDerivedStateFromProps&&null!=a.componentWillMount&&a.componentWillMount(),null!=a.componentDidMount&&a.__h.push(a.componentDidMount);else{if(null==S.getDerivedStateFromProps&&b!==h&&null!=a.componentWillReceiveProps&&a.componentWillReceiveProps(b,x),!a.__e&&null!=a.shouldComponentUpdate&&!1===a.shouldComponentUpdate(b,a.__s,x)||t.__v===_.__v){a.props=b,a.state=a.__s,t.__v!==_.__v&&(a.__d=!1),a.__v=t,t.__e=_.__e,t.__k=_.__k,t.__k.forEach(function(e){e&&(e.__=t)}),a.__h.length&&i.push(a);break e}null!=a.componentWillUpdate&&a.componentWillUpdate(b,a.__s,x),null!=a.componentDidUpdate&&a.__h.push(function(){a.componentDidUpdate(h,v,k)})}a.context=x,a.props=b,a.state=a.__s,(f=n.__r)&&f(t),a.__d=!1,a.__v=t,a.__P=e,f=a.render(a.props,a.state,a.context),a.state=a.__s,null!=a.getChildContext&&(l=c(c({},l),a.getChildContext())),d||null==a.getSnapshotBeforeUpdate||(k=a.getSnapshotBeforeUpdate(h,v)),P=null!=f&&f.type===y&&null==f.key?f.props.children:f,m(e,Array.isArray(P)?P:[P],t,_,l,o,r,i,u,s),a.base=t.__e,t.__h=null,a.__h.length&&i.push(a),g&&(a.__E=a.__=null),a.__e=!1}else null==r&&t.__v===_.__v?(t.__k=_.__k,t.__e=_.__e):t.__e=j(_.__e,t,_,l,o,r,i,s);(f=n.diffed)&&f(t)}catch(e){t.__v=null,(s||null!=r)&&(t.__e=u,t.__h=!!s,r[r.indexOf(u)]=null),n.__e(e,t,_)}}function T(e,t){n.__c&&n.__c(t,e),e.some(function(t){try{e=t.__h,t.__h=[],e.some(function(e){e.call(t)})}catch(e){n.__e(e,t.__v)}})}function j(e,n,t,_,l,o,i,u){var c,p,a,d,h=t.props,v=n.props,y=n.type,k=0;if(""svg""===y&&(l=!0),null!=o)for(;k<o.length;k++)if((c=o[k])&&(c===e||(y?c.localName==y:3==c.nodeType))){e=c,o[k]=null;break}if(null==e){if(null===y)return document.createTextNode(v);e=l?document.createElementNS(""http://www.w3.org/2000/svg"",y):document.createElement(y,v.is&&v),o=null,u=!1}if(null===y)h===v||u&&e.data===v||(e.data=v);else{if(o=o&&f.slice.call(e.childNodes),p=(h=t.props||r).dangerouslySetInnerHTML,a=v.dangerouslySetInnerHTML,!u){if(null!=o)for(h={},d=0;d<e.attributes.length;d++)h[e.attributes[d].name]=e.attributes[d].value;(a||p)&&(a&&(p&&a.__html==p.__html||a.__html===e.innerHTML)||(e.innerHTML=a&&a.__html||""""))}if(A(e,v,h,l,u),a)n.__k=[];else if(k=n.props.children,m(e,Array.isArray(k)?k:[k],n,t,_,l&&""foreignObject""!==y,o,i,e.firstChild,u),null!=o)for(k=o.length;k--;)null!=o[k]&&s(o[k]);u||(""value""in v&&void 0!==(k=v.value)&&(k!==e.value||""progress""===y&&!k)&&C(e,""value"",k,h.value,!1),""checked""in v&&void 0!==(k=v.checked)&&k!==e.checked&&C(e,""checked"",k,h.checked,!1))}return e}function z(e,t,_){try{""function""==typeof e?e(t):e.current=t}catch(e){n.__e(e,_)}}function L(e,t,_){var l,o,r;if(n.unmount&&n.unmount(e),(l=e.ref)&&(l.current&&l.current!==e.__e||z(l,null,t)),_||""function""==typeof e.type||(_=null!=(o=e.__e)),e.__e=e.__d=void 0,null!=(l=e.__c)){if(l.componentWillUnmount)try{l.componentWillUnmount()}catch(e){n.__e(e,t)}l.base=l.__P=null}if(l=e.__k)for(r=0;r<l.length;r++)l[r]&&L(l[r],t,_);null!=o&&s(o)}function M(e,n,t){return this.constructor(e,t)}function N(e,t,_){var l,o,i;n.__&&n.__(e,t),o=(l=""function""==typeof _)?null:_&&_.__k||t.__k,i=[],I(t,e=(!l&&_||t).__k=a(y,null,[e]),o||r,r,void 0!==t.ownerSVGElement,!l&&_?[_]:o?null:t.firstChild?f.slice.call(t.childNodes):null,i,!l&&_?_:o?o.__e:t.firstChild,l),T(i,e)}function O(e,n){N(e,n,O)}function S(e,n,t){var _,l,o,r=arguments,i=c({},e.props);for(o in n)""key""==o?_=n[o]:""ref""==o?l=n[o]:i[o]=n[o];if(arguments.length>3)for(t=[t],o=3;o<arguments.length;o++)t.push(r[o]);return null!=t&&(i.children=t),v(e.type,i,_||e.key,l||e.ref,null)}function q(e,n){var t={__c:n=""__cC""+o++,__:e,Consumer:function(e,n){return e.children(n)},Provider:function(e){var t,_;return this.getChildContext||(t=[],(_={})[n]=this,this.getChildContext=function(){return _},this.shouldComponentUpdate=function(e){this.props.value!==e.value&&t.some(k)},this.sub=function(e){t.push(e);var n=e.componentWillUnmount;e.componentWillUnmount=function(){t.splice(t.indexOf(e),1),n&&n.call(e)}}),e.children}};return t.Provider.__=t.Consumer.contextType=t}n={__e:function(e,n){for(var t,_,l;n=n.__;)if((t=n.__c)&&!t.__)try{if((_=t.constructor)&&null!=_.getDerivedStateFromError&&(t.setState(_.getDerivedStateFromError(e)),l=t.__d),null!=t.componentDidCatch&&(t.componentDidCatch(e),l=t.__d),l)return t.__E=t}catch(n){e=n}throw e},__v:0},l=function(e){return null!=e&&void 0===e.constructor},p.prototype.setState=function(e,n){var t;t=null!=this.__s&&this.__s!==this.state?this.__s:this.__s=c({},this.state),""function""==typeof e&&(e=e(c({},t),this.props)),e&&c(t,e),null!=e&&this.__v&&(n&&this.__h.push(n),k(this))},p.prototype.forceUpdate=function(e){this.__v&&(this.__e=!0,e&&this.__h.push(e),k(this))},p.prototype.render=y,u=[],i=""function""==typeof Promise?Promise.prototype.then.bind(Promise.resolve()):setTimeout,b.__r=0,o=0;export{N as render,O as hydrate,a as createElement,a as h,y as Fragment,h as createRef,l as isValidElement,p as Component,S as cloneElement,q as createContext,w as toChildArray,n as options};
","['// Preact, MIT License']",MIT,"mpi Permissive License
MIT License
Time::ParseDate License",1.0,100.0
69,69,79,linux-master/tools/testing/selftests/sync/sync_fence.c,MIT,584,"sync fence tests with one timeline*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
Go even further, and confirm wait still succeeds
Signal the fence
Wait successfully
Advance timeline from 0 -> 1
create fence a,b,c and then merge them all into fence d
confirm that d is not signaled until the max of a,b,c
confirm all fences have one active point (even d)
Wait on fence until timeout
Wait on fence until timeout","sync fence tests with one timeline*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
Wait on fence until timeout
Advance timeline from 0 -> 1
Wait on fence until timeout
Signal the fence
Wait successfully
Go even further, and confirm wait still succeeds
create fence a,b,c and then merge them all into fence d
confirm all fences have one active point (even d)
confirm that d is not signaled until the max of a,b,c","['Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.', 'SPDX-License-Identifier: GPL-2.0-only']",MIT,"DSDP License
Creative Commons Attribution Share Alike 2.1 Japan
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
Data licence Germany – zero – version 2.0
mpi Permissive License
Creative Commons Attribution Share Alike 2.1 Japan
mpi Permissive License
mpi Permissive License
mpi Permissive License",1.0,50.0
70,70,80,pytorch-main/torch/_appdirs.py,MIT,184,"This is the MIT license
This file is directly from https://github.com/ActiveState/appdirs/blob/3fe6a83776843a46f20c2e5587afcffe05e03b39/appdirs.py  The license of https://github.com/ActiveState/appdirs copied below:   # This is the MIT license  Copyright (c) 2010 ActiveState Software Inc.  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 !/usr/bin/env python3 -*- coding: utf-8 -*- Copyright (c) 2005-2010 ActiveState Software Inc. Copyright (c) 2013 Eddy Petrișor
""Mac OS X"", etc.
This is a fallback technique at best. I'm not sure if using the     registry for this guarantees us the correct answer for all CSIDL_*     names.
Hidden, but writeable on Win 7.
 Downgrade to short path name if have highbit chars. See <http://bugs.activestate.com/show_bug.cgi?id=85099>.
 Downgrade to short path name if have highbit chars. See <http://bugs.activestate.com/show_bug.cgi?id=85099>.
 Downgrade to short path name if have highbit chars. See <http://bugs.activestate.com/show_bug.cgi?id=85099>.
""Windows XP"", ""Windows 7"", etc.","flake8: noqa
This is the MIT license
""Windows XP"", ""Windows 7"", etc.
""Mac OS X"", etc.
or in $XDG_DATA_HOME, if defined
Hidden, but writeable on Win 7.
or in $XDG_CONFIG_HOME, if defined
or in $XDG_STATE_HOME, if defined
state>
or under $XDG_CACHE_HOME if defined
---- internal support stuff
---- self test code
 !/usr/bin/env python3 -*- coding: utf-8 -*- Copyright (c) 2005-2010 ActiveState Software Inc. Copyright (c) 2013 Eddy Petrișor
 Dev Notes: - MSDN on where to store app data files: http://support.microsoft.com/default.aspx?scid=kb;en-us;310294#XSLTH3194121123120121120120 - Mac OS X: http://developer.apple.com/documentation/MacOSX/Conceptual/BPFileSystem/index.html - XDG spec for Un*x: https://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html
 ""Linux"", ""SunOS"", ""FreeBSD"", etc. Setting this to ""linux2"" is not ideal, but only Windows or Mac are actually checked for and the rest of the module expects *sys.platform* style strings.
 XDG default for $XDG_DATA_DIRS only first, if multipath is False
 XDG default for $XDG_CONFIG_DIRS only first, if multipath is False
 Try to make this a unicode path because SHGetFolderPath does not return unicode strings when there is unicode data in the path.
 Downgrade to short path name if have highbit chars. See <http://bugs.activestate.com/show_bug.cgi?id=85099>.
 Downgrade to short path name if have highbit chars. See <http://bugs.activestate.com/show_bug.cgi?id=85099>.
 Downgrade to short path name if have highbit chars. See <http://bugs.activestate.com/show_bug.cgi?id=85099>.
This file is directly from https://github.com/ActiveState/appdirs/blob/3fe6a83776843a46f20c2e5587afcffe05e03b39/appdirs.py  The license of https://github.com/ActiveState/appdirs copied below:   # This is the MIT license  Copyright (c) 2010 ActiveState Software Inc.  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
Utilities for determining application-specific dirs.  See <https://github.com/ActiveState/appdirs> for details and usage.
r""""""Return full path to the user-specific data dir for this application.          ""appname"" is the name of application.             If None, just the system directory is returned.         ""appauthor"" (only used on Windows) is the name of the             appauthor or distributing body for this application. Typically             it is the owning company name. This falls back to appname. You may             pass False to disable it.         ""version"" is an optional version path element to append to the             path. You might want to use this if you want multiple versions             of your app to be able to run independently. If used, this             would typically be ""<major>.<minor>"".             Only applied when appname is present.         ""roaming"" (boolean, default False) can be set True to use the Windows             roaming appdata directory. That means that for users on a Windows             network setup for roaming profiles, this user data will be             sync'd on login. See             <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>             for a discussion of issues.      Typical user data directories are:         Mac OS X:               ~/Library/Application Support/<AppName>         Unix:                   ~/.local/share/<AppName>    # or in $XDG_DATA_HOME, if defined         Win XP (not roaming):   C:\Documents and Settings\<username>\Application Data\<AppAuthor>\<AppName>         Win XP (roaming):       C:\Documents and Settings\<username>\Local Settings\Application Data\<AppAuthor>\<AppName>         Win 7  (not roaming):   C:\Users\<username>\AppData\Local\<AppAuthor>\<AppName>         Win 7  (roaming):       C:\Users\<username>\AppData\Roaming\<AppAuthor>\<AppName>      For Unix, we follow the XDG spec and support $XDG_DATA_HOME.     That means, by default ""~/.local/share/<AppName>"".
r""""""Return full path to the user-shared data dir for this application.          ""appname"" is the name of application.             If None, just the system directory is returned.         ""appauthor"" (only used on Windows) is the name of the             appauthor or distributing body for this application. Typically             it is the owning company name. This falls back to appname. You may             pass False to disable it.         ""version"" is an optional version path element to append to the             path. You might want to use this if you want multiple versions             of your app to be able to run independently. If used, this             would typically be ""<major>.<minor>"".             Only applied when appname is present.         ""multipath"" is an optional parameter only applicable to *nix             which indicates that the entire list of data dirs should be             returned. By default, the first item from XDG_DATA_DIRS is             returned, or '/usr/local/share/<AppName>',             if XDG_DATA_DIRS is not set      Typical site data directories are:         Mac OS X:   /Library/Application Support/<AppName>         Unix:       /usr/local/share/<AppName> or /usr/share/<AppName>         Win XP:     C:\Documents and Settings\All Users\Application Data\<AppAuthor>\<AppName>         Vista:      (Fail! ""C:\ProgramData"" is a hidden *system* directory on Vista.)         Win 7:      C:\ProgramData\<AppAuthor>\<AppName>   # Hidden, but writeable on Win 7.      For Unix, this is using the $XDG_DATA_DIRS[0] default.      WARNING: Do not use this on Windows. See the Vista-Fail note above for why.
r""""""Return full path to the user-specific config dir for this application.          ""appname"" is the name of application.             If None, just the system directory is returned.         ""appauthor"" (only used on Windows) is the name of the             appauthor or distributing body for this application. Typically             it is the owning company name. This falls back to appname. You may             pass False to disable it.         ""version"" is an optional version path element to append to the             path. You might want to use this if you want multiple versions             of your app to be able to run independently. If used, this             would typically be ""<major>.<minor>"".             Only applied when appname is present.         ""roaming"" (boolean, default False) can be set True to use the Windows             roaming appdata directory. That means that for users on a Windows             network setup for roaming profiles, this user data will be             sync'd on login. See             <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>             for a discussion of issues.      Typical user config directories are:         Mac OS X:               ~/Library/Preferences/<AppName>         Unix:                   ~/.config/<AppName>     # or in $XDG_CONFIG_HOME, if defined         Win *:                  same as user_data_dir      For Unix, we follow the XDG spec and support $XDG_CONFIG_HOME.     That means, by default ""~/.config/<AppName>"".
r""""""Return full path to the user-shared data dir for this application.          ""appname"" is the name of application.             If None, just the system directory is returned.         ""appauthor"" (only used on Windows) is the name of the             appauthor or distributing body for this application. Typically             it is the owning company name. This falls back to appname. You may             pass False to disable it.         ""version"" is an optional version path element to append to the             path. You might want to use this if you want multiple versions             of your app to be able to run independently. If used, this             would typically be ""<major>.<minor>"".             Only applied when appname is present.         ""multipath"" is an optional parameter only applicable to *nix             which indicates that the entire list of config dirs should be             returned. By default, the first item from XDG_CONFIG_DIRS is             returned, or '/etc/xdg/<AppName>', if XDG_CONFIG_DIRS is not set      Typical site config directories are:         Mac OS X:   same as site_data_dir         Unix:       /etc/xdg/<AppName> or $XDG_CONFIG_DIRS[i]/<AppName> for each value in                     $XDG_CONFIG_DIRS         Win *:      same as site_data_dir         Vista:      (Fail! ""C:\ProgramData"" is a hidden *system* directory on Vista.)      For Unix, this is using the $XDG_CONFIG_DIRS[0] default, if multipath=False      WARNING: Do not use this on Windows. See the Vista-Fail note above for why.
r""""""Return full path to the user-specific cache dir for this application.          ""appname"" is the name of application.             If None, just the system directory is returned.         ""appauthor"" (only used on Windows) is the name of the             appauthor or distributing body for this application. Typically             it is the owning company name. This falls back to appname. You may             pass False to disable it.         ""version"" is an optional version path element to append to the             path. You might want to use this if you want multiple versions             of your app to be able to run independently. If used, this             would typically be ""<major>.<minor>"".             Only applied when appname is present.         ""opinion"" (boolean) can be False to disable the appending of             ""Cache"" to the base app data dir for Windows. See             discussion below.      Typical user cache directories are:         Mac OS X:   ~/Library/Caches/<AppName>         Unix:       ~/.cache/<AppName> (XDG default)         Win XP:     C:\Documents and Settings\<username>\Local Settings\Application Data\<AppAuthor>\<AppName>\Cache         Vista:      C:\Users\<username>\AppData\Local\<AppAuthor>\<AppName>\Cache      On Windows the only suggestion in the MSDN docs is that local settings go in     the `CSIDL_LOCAL_APPDATA` directory. This is identical to the non-roaming     app data dir (the default returned by `user_data_dir` above). Apps typically     put cache data somewhere *under* the given dir here. Some examples:         ...\Mozilla\Firefox\Profiles\<ProfileName>\Cache         ...\Acme\SuperApp\Cache\1.0     OPINION: This function appends ""Cache"" to the `CSIDL_LOCAL_APPDATA` value.     This can be disabled with the `opinion=False` option.
r""""""Return full path to the user-specific state dir for this application.          ""appname"" is the name of application.             If None, just the system directory is returned.         ""appauthor"" (only used on Windows) is the name of the             appauthor or distributing body for this application. Typically             it is the owning company name. This falls back to appname. You may             pass False to disable it.         ""version"" is an optional version path element to append to the             path. You might want to use this if you want multiple versions             of your app to be able to run independently. If used, this             would typically be ""<major>.<minor>"".             Only applied when appname is present.         ""roaming"" (boolean, default False) can be set True to use the Windows             roaming appdata directory. That means that for users on a Windows             network setup for roaming profiles, this user data will be             sync'd on login. See             <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>             for a discussion of issues.      Typical user state directories are:         Mac OS X:  same as user_data_dir         Unix:      ~/.local/state/<AppName>   # or in $XDG_STATE_HOME, if defined         Win *:     same as user_data_dir      For Unix, we follow this Debian proposal <https://wiki.debian.org/XDGBaseDirectorySpecification#state>     to extend the XDG spec and support $XDG_STATE_HOME.      That means, by default ""~/.local/state/<AppName>"".
r""""""Return full path to the user-specific log dir for this application.          ""appname"" is the name of application.             If None, just the system directory is returned.         ""appauthor"" (only used on Windows) is the name of the             appauthor or distributing body for this application. Typically             it is the owning company name. This falls back to appname. You may             pass False to disable it.         ""version"" is an optional version path element to append to the             path. You might want to use this if you want multiple versions             of your app to be able to run independently. If used, this             would typically be ""<major>.<minor>"".             Only applied when appname is present.         ""opinion"" (boolean) can be False to disable the appending of             ""Logs"" to the base app data dir for Windows, and ""log"" to the             base cache dir for Unix. See discussion below.      Typical user log directories are:         Mac OS X:   ~/Library/Logs/<AppName>         Unix:       ~/.cache/<AppName>/log  # or under $XDG_CACHE_HOME if defined         Win XP:     C:\Documents and Settings\<username>\Local Settings\Application Data\<AppAuthor>\<AppName>\Logs         Vista:      C:\Users\<username>\AppData\Local\<AppAuthor>\<AppName>\Logs      On Windows the only suggestion in the MSDN docs is that local settings     go in the `CSIDL_LOCAL_APPDATA` directory. (Note: I'm interested in     examples of what some windows apps use for a logs dir.)      OPINION: This function appends ""Logs"" to the `CSIDL_LOCAL_APPDATA`     value for Windows and appends ""log"" to the user cache dir for Unix.     This can be disabled with the `opinion=False` option.
Convenience wrapper for getting application dirs.
This is a fallback technique at best. I'm not sure if using the     registry for this guarantees us the correct answer for all CSIDL_*     names.","['SPDX-License-Identifier: GPL-2.0-only', 'This is the MIT license', 'Copyright (c) 2010 ActiveState Software Inc. Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.']",MIT,"MIT License
MIT License
Python License 2.0
Any OSI License
Gutmann License
mpi Permissive License
mpi Permissive License
mpi Permissive License
mpi Permissive License
Any OSI License",1.0,66.66666666666667
71,71,81,linux-master/tools/testing/selftests/sync/sync_test.c,MIT,578,"sync test runner*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
need this return to keep gcc happy","sync test runner*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
need this return to keep gcc happy","['Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR*  OTHER DEALINGS IN THE SOFTWARE.', 'SPDX-License-Identifier: GPL-2.0-only']",MIT,"X11 License Distribution Modification Variant
GNU General Public License v3.0 w/GCC Runtime Library exception",1.0,50.0
72,72,82,linux-master/tools/testing/selftests/sync/sync.c,MIT,585,"sync / sw_sync abstraction*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
Same code!
SW_SYNC ioctls","sync / sw_sync abstraction*  Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.
SW_SYNC ioctls
Same code!","['Copyright 2015-2016 Collabora Ltd.**  Based on the implementation from the Android Open Source Project,**  Copyright 2012 Google, Inc**  Permission is hereby granted, free of charge, to any person obtaining a*  copy of this software and associated documentation files (the ""Software""),*  to deal in the Software without restriction, including without limitation*  the rights to use, copy, modify, merge, publish, distribute, sublicense,*  and/or sell copies of the Software, and to permit persons to whom the*  Software is furnished to do so, subject to the following conditions:**  The above copyright notice and this permission notice shall be included in*  all copies or substantial portions of the Software.**  THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR*  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,*  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL*  THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR*  OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,*  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR*  OTHER DEALINGS IN THE SOFTWARE.', 'SPDX-License-Identifier: GPL-2.0-only']",MIT,"Copyfree Open Innovation License
Gutmann License
Spencer License 94",1.0,50.0
73,73,83,linux-master/tools/testing/selftests/sched/cs_prctl_test.c,LGPL-2.1-only,645,"SPDX-License-Identifier: GPL-2.0-only
Use the core scheduling prctl() to test core scheduling cookies control.** Copyright (c) 2021 Oracle and/or its affiliates.* Author: Chris Hyser <chris.hyser@oracle.com>*** This library is free software; you can redistribute it and/or modify it* under the terms of version 2.1 of the GNU Lesser General Public License as* published by the Free Software Foundation.** This library is distributed in the hope that it will be useful, but WITHOUT* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License* for more details.** You should have received a copy of the GNU Lesser General Public License* along with this library; if not, see <http://www.gnu.org/licenses>.
put into separate process group
get a random process pid
pull core_sched cookie to pid
push core_sched cookie to pid
create unique core_sched cookie","SPDX-License-Identifier: GPL-2.0-only
Use the core scheduling prctl() to test core scheduling cookies control.** Copyright (c) 2021 Oracle and/or its affiliates.* Author: Chris Hyser <chris.hyser@oracle.com>*** This library is free software; you can redistribute it and/or modify it* under the terms of version 2.1 of the GNU Lesser General Public License as* published by the Free Software Foundation.** This library is distributed in the hope that it will be useful, but WITHOUT* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License* for more details.** You should have received a copy of the GNU Lesser General Public License* along with this library; if not, see <http://www.gnu.org/licenses>.
create unique core_sched cookie
push core_sched cookie to pid
pull core_sched cookie to pid
put into separate process group
get a random process pid","['SPDX-License-Identifier: GPL-2.0-only', 'This library is free software; you can redistribute it and/or modify it* under the terms of version 2.1 of the GNU Lesser General Public License as* published by the Free Software Foundation.** This library is distributed in the hope that it will be useful, but WITHOUT* ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or* FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License* for more details.** You should have received a copy of the GNU Lesser General Public License* along with this library; if not, see <http://www.gnu.org/licenses>']",LGPL-2.1-only,"GL2PS License
BSD 3-Clause No Nuclear License 2014
dvipdfm License
Checkmk License
Zed License
Zed License
mpi Permissive License",1.0,100.0
74,74,84,linux-master/tools/testing/selftests/rseq/rseq.c,LGPL-2.1-only,677,"rseq.c** Copyright (C) 2016 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>** This library is free software; you can redistribute it and/or* modify it under the terms of the GNU Lesser General Public* License as published by the Free Software Foundation; only* version 2.1 of the License.** This library is distributed in the hope that it will be useful,* but WITHOUT ANY WARRANTY; without even the implied warranty of* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU* Lesser General Public License for more details.
At least one rseq registration has succeded.
SPDX-License-Identifier: LGPL-2.1
Treat libc's ownership as a successful registration.
Treat libc's ownership as a successful unregistration.
rseq registration owned by glibc
Incoherent success/failure within process.
Allocate a large area for the TLS.
rseq feature size supported by the kernel. 0 if the registration was* unsuccessful.
Flags used during rseq registration.","SPDX-License-Identifier: LGPL-2.1
rseq.c** Copyright (C) 2016 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>** This library is free software; you can redistribute it and/or* modify it under the terms of the GNU Lesser General Public* License as published by the Free Software Foundation; only* version 2.1 of the License.** This library is distributed in the hope that it will be useful,* but WITHOUT ANY WARRANTY; without even the implied warranty of* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU* Lesser General Public License for more details.
Offset from the thread pointer to the rseq area.
Size of the registered rseq area. 0 if the registration was* unsuccessful.
Flags used during rseq registration.
rseq feature size supported by the kernel. 0 if the registration was* unsuccessful.
At least one rseq registration has succeded.
Allocate a large area for the TLS.
Original struct rseq feature size is 20 bytes.
Original struct rseq allocation size is 32 bytes.
Treat libc's ownership as a successful registration.
Incoherent success/failure within process.
Treat libc's ownership as a successful unregistration.
rseq registration owned by glibc","['Copyright (C) 2016 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>** This library is free software; you can redistribute it and/or* modify it under the terms of the GNU Lesser General Public* License as published by the Free Software Foundation; only* version 2.1 of the License.** This library is distributed in the hope that it will be useful,* but WITHOUT ANY WARRANTY; without even the implied warranty of* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU* Lesser General Public License for more details.', 'SPDX-License-Identifier: LGPL-2.1']",LGPL-2.1-only,"Spencer License 86
Time::ParseDate License
GL2PS License
Business Source License 1.1
Business Source License 1.1
CeCILL-C Free Software License Agreement
Creative Commons Attribution Share Alike 2.1 Japan
Matrix Template Library License
Universal Permissive License v1.0
Detection Rule License 1.1",1.0,100.0
75,75,85,linux-master/tools/testing/selftests/powerpc/mm/subpage_prot.c,LGPL-2.1-only,1217,"Copyright IBM Corp.** This program is free software; you can redistribute it and/or modify it* under the terms of version 2.1 of the GNU Lesser General Public License* as published by the Free Software Foundation.** This program is distributed in the hope that it would be useful, but* WITHOUT ANY WARRANTY; without even the implied warranty of* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
for each page, mark subpage i % 16 read only and subpage* (i + 3) % 16 inaccessible","Copyright IBM Corp.** This program is free software; you can redistribute it and/or modify it* under the terms of version 2.1 of the GNU Lesser General Public License* as published by the Free Software Foundation.** This program is distributed in the hope that it would be useful, but* WITHOUT ANY WARRANTY; without even the implied warranty of* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
for each page, mark subpage i % 16 read only and subpage* (i + 3) % 16 inaccessible","['Copyright IBM Corp.** This program is free software; you can redistribute it and/or modify it* under the terms of version 2.1 of the GNU Lesser General Public License* as published by the Free Software Foundation.** This program is distributed in the hope that it would be useful, but* WITHOUT ANY WARRANTY; without even the implied warranty of* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.', 'SPDX-License-Identifier: GPL-2.0-only']",LGPL-2.1-only,"IBM PowerPC Initialization and Boot Software
mpi Permissive License",1.0,50.0
76,76,86,linux-master/tools/testing/selftests/rseq/param_test.c,LGPL-2.1-only,685,[],"SPDX-License-Identifier: LGPL-2.1
BENCHMARK
Membarrier does not currently support targeting a mm_cid, so* issue the barrier on all cpus.
TEST_MEMBARRIER
TEST_MEMBARRIER
A simple percpu spinlock. Grabs lock on current cpu.
Retry if comparison fails or rseq aborts.
Acquire semantic when taking lock after control dependency.* Matches rseq_smp_store_release().
Release lock, with release semantic. Matches* rseq_smp_acquire__after_ctrl_dep().
A simple test which implements a sharded counter using a per-cpu* lock.  Obviously real applications might prefer to simply use a* per-cpu increment; however, this is reasonable for a test and the* lock can be extended to synchronize more complicated operations.
Load list->c[cpu].head with single-copy atomicity.
Retry if comparison fails or rseq aborts.
Unlike a traditional lock-less linked list; the availability of a* rseq primitive allows us to implement pop without concerns over* ABA-type races.
Retry if rseq aborts.
__percpu_list_pop is not safe against concurrent accesses. Should* only be used on lists that are not concurrently modified.
encourage shuffling
Simultaneous modification to a per-cpu linked list from many threads.
Generate list entries for every usable cpu.
All entries should now be accounted for (unless some external* actor is interfering with our allowed affinity while this* test is running).
Retry if comparison fails or rseq aborts.
Load offset with single-copy atomicity.
Retry if comparison fails or rseq aborts.
__percpu_buffer_pop is not safe against concurrent accesses. Should* only be used on buffers that are not concurrently modified.
encourage shuffling
Should increase buffer size.
Simultaneous modification to a per-cpu buffer from many threads.
Generate list entries for every usable cpu.
Worse-case is every item in same CPU.
We could theoretically put the word-sized* ""data"" directly in the buffer. However, we* want to model objects that would not fit* within a single word, so allocate an object* for each node.
All entries should now be accounted for (unless some external* actor is interfering with our allowed affinity while this* test is running).
Load offset with single-copy atomicity.
copylen must be <= 4kB.
Retry if comparison fails or rseq aborts.
Load offset with single-copy atomicity.
copylen must be <= 4kB.
Retry if comparison fails or rseq aborts.
__percpu_memcpy_buffer_pop is not safe against concurrent accesses. Should* only be used on buffers that are not concurrently modified.
encourage shuffling
Should increase buffer size.
Simultaneous modification to a per-cpu buffer from many threads.
Generate list entries for every usable cpu.
Worse-case is every item in same CPU.
We could theoretically put the word-sized* ""data"" directly in the buffer. However, we* want to model objects that would not fit* within a single word, so allocate an object* for each node.
All entries should now be accounted for (unless some external* actor is interfering with our allowed affinity while this* test is running).
Test MEMBARRIER_CMD_PRIVATE_RESTART_RSEQ_ON_CPU membarrier command.
Worker threads modify data in their ""active"" percpu lists.
Wait for initialization.
The manager thread swaps per-cpu lists that worker threads see,* and validates that there are no unexpected modifications.
Init lists.
list_a is ""active"".
As list_b is ""inactive"", we should never see changes* to list_b.
Make list_b ""active"".
missing CPU
Cpu A should now only modify list_b, so the values* in list_a should be stable.
As list_a is ""inactive"", we should never see changes* to list_a.
Make list_a ""active"".
missing CPU
Remember a value from list_b.
TEST_MEMBARRIER",['SPDX-License-Identifier: LGPL-2.1'],LGPL-2.1-only,,0.0,0.0
77,77,87,linux-master/tools/testing/selftests/rseq/basic_percpu_ops_test.c,LGPL-2.1-only,690,[],"SPDX-License-Identifier: LGPL-2.1
A simple percpu spinlock.  Returns the cpu lock was acquired on.
Retry if comparison fails or rseq aborts.
Acquire semantic when taking lock after control dependency.* Matches rseq_smp_store_release().
Release lock, with release semantic. Matches* rseq_smp_acquire__after_ctrl_dep().
A simple test which implements a sharded counter using a per-cpu* lock.  Obviously real applications might prefer to simply use a* per-cpu increment; however, this is reasonable for a test and the* lock can be extended to synchronize more complicated operations.
Load list->c[cpu].head with single-copy atomicity.
Retry if comparison fails or rseq aborts.
Unlike a traditional lock-less linked list; the availability of a* rseq primitive allows us to implement pop without concerns over* ABA-type races.
Retry if rseq aborts.
__percpu_list_pop is not safe against concurrent accesses. Should* only be used on lists that are not concurrently modified.
encourage shuffling
Simultaneous modification to a per-cpu linked list from many threads.
Generate list entries for every usable cpu.
All entries should now be accounted for (unless some external* actor is interfering with our allowed affinity while this* test is running).",['SPDX-License-Identifier: LGPL-2.1'],LGPL-2.1-only,,0.0,0.0
78,78,88,linux-master/tools/testing/selftests/rseq/basic_test.c,LGPL-2.1-only,689,[],"SPDX-License-Identifier: LGPL-2.1
Basic test coverage for critical regions and rseq_current_cpu().",['SPDX-License-Identifier: LGPL-2.1'],LGPL-2.1-only,,0.0,0.0
79,79,89,linux-master/tools/testing/selftests/rseq/run_param_test.sh,Dual-license GPL-2.0-or-later MIT,661,[],"TEST_LIST[@]}"" ]; do
INJECT_LIST[@]}"" ]; do
INJECT_LIST[@]}"" ]; do
 !/bin/bash SPDX-License-Identifier: GPL-2.0+ or MIT",['SPDX-License-Identifier: GPL-2.0+ or MIT'],Dual-license GPL-2.0-or-later MIT,,0.0,0.0
80,80,90,linux-master/tools/testing/selftests/rseq/Makefile,Dual-license GPL-2.0-or-later MIT,686,[],"# SPDX-License-Identifier: GPL-2.0+ OR MIT

ifneq ($(shell $(CC) --version 2>&1 | head -n 1 | grep clang),)
CLANG_FLAGS += -no-integrated-as
endif

CFLAGS += -O2 -Wall -g -I./ $(KHDR_INCLUDES) -L$(OUTPUT) -Wl,-rpath=./ \
	  $(CLANG_FLAGS)
LDLIBS += -lpthread -ldl

# Own dependencies because we only want to build against 1st prerequisite, but
# still track changes to header files and depend on shared object.
OVERRIDE_TARGETS = 1

TEST_GEN_PROGS = basic_test basic_percpu_ops_test basic_percpu_ops_mm_cid_test param_test \
		param_test_benchmark param_test_compare_twice param_test_mm_cid \
		param_test_mm_cid_benchmark param_test_mm_cid_compare_twice

TEST_GEN_PROGS_EXTENDED = librseq.so

TEST_PROGS = run_param_test.sh

TEST_FILES := settings

include ../lib.mk

$(OUTPUT)/librseq.so: rseq.c rseq.h rseq-*.h
	$(CC) $(CFLAGS) -shared -fPIC $< $(LDLIBS) -o $@

$(OUTPUT)/%: %.c $(TEST_GEN_PROGS_EXTENDED) rseq.h rseq-*.h
	$(CC) $(CFLAGS) $< $(LDLIBS) -lrseq -o $@

$(OUTPUT)/basic_percpu_ops_mm_cid_test: basic_percpu_ops_test.c $(TEST_GEN_PROGS_EXTENDED) rseq.h rseq-*.h
	$(CC) $(CFLAGS) -DBUILDOPT_RSEQ_PERCPU_MM_CID_ID $< $(LDLIBS) -lrseq -o $@

$(OUTPUT)/param_test_benchmark: param_test.c $(TEST_GEN_PROGS_EXTENDED) \
					rseq.h rseq-*.h
	$(CC) $(CFLAGS) -DBENCHMARK $< $(LDLIBS) -lrseq -o $@

$(OUTPUT)/param_test_compare_twice: param_test.c $(TEST_GEN_PROGS_EXTENDED) \
					rseq.h rseq-*.h
	$(CC) $(CFLAGS) -DRSEQ_COMPARE_TWICE $< $(LDLIBS) -lrseq -o $@

$(OUTPUT)/param_test_mm_cid: param_test.c $(TEST_GEN_PROGS_EXTENDED) \
					rseq.h rseq-*.h
	$(CC) $(CFLAGS) -DBUILDOPT_RSEQ_PERCPU_MM_CID $< $(LDLIBS) -lrseq -o $@

$(OUTPUT)/param_test_mm_cid_benchmark: param_test.c $(TEST_GEN_PROGS_EXTENDED) \
					rseq.h rseq-*.h
	$(CC) $(CFLAGS) -DBUILDOPT_RSEQ_PERCPU_MM_CID -DBENCHMARK $< $(LDLIBS) -lrseq -o $@

$(OUTPUT)/param_test_mm_cid_compare_twice: param_test.c $(TEST_GEN_PROGS_EXTENDED) \
					rseq.h rseq-*.h
	$(CC) $(CFLAGS) -DBUILDOPT_RSEQ_PERCPU_MM_CID -DRSEQ_COMPARE_TWICE $< $(LDLIBS) -lrseq -o $@
","[""`\n['']\n`""]",Dual-license GPL-2.0-or-later MIT,,0.0,0.0
81,81,91,linux-master/tools/testing/selftests/net/forwarding/Makefile,Dual-license GPL-2.0-or-later MIT,1629,[],"# SPDX-License-Identifier: GPL-2.0+ OR MIT

TEST_PROGS = bridge_igmp.sh \
	bridge_locked_port.sh \
	bridge_mdb.sh \
	bridge_mdb_host.sh \
	bridge_mdb_max.sh \
	bridge_mdb_port_down.sh \
	bridge_mld.sh \
	bridge_port_isolation.sh \
	bridge_sticky_fdb.sh \
	bridge_vlan_aware.sh \
	bridge_vlan_mcast.sh \
	bridge_vlan_unaware.sh \
	custom_multipath_hash.sh \
	dual_vxlan_bridge.sh \
	ethtool_extended_state.sh \
	ethtool_mm.sh \
	ethtool.sh \
	gre_custom_multipath_hash.sh \
	gre_inner_v4_multipath.sh \
	gre_inner_v6_multipath.sh \
	gre_multipath_nh_res.sh \
	gre_multipath_nh.sh \
	gre_multipath.sh \
	hw_stats_l3.sh \
	hw_stats_l3_gre.sh \
	ip6_forward_instats_vrf.sh \
	ip6gre_custom_multipath_hash.sh \
	ip6gre_flat_key.sh \
	ip6gre_flat_keys.sh \
	ip6gre_flat.sh \
	ip6gre_hier_key.sh \
	ip6gre_hier_keys.sh \
	ip6gre_hier.sh \
	ip6gre_inner_v4_multipath.sh \
	ip6gre_inner_v6_multipath.sh \
	ipip_flat_gre_key.sh \
	ipip_flat_gre_keys.sh \
	ipip_flat_gre.sh \
	ipip_hier_gre_key.sh \
	ipip_hier_gre_keys.sh \
	ipip_hier_gre.sh \
	local_termination.sh \
	loopback.sh \
	mirror_gre_bound.sh \
	mirror_gre_bridge_1d.sh \
	mirror_gre_bridge_1d_vlan.sh \
	mirror_gre_bridge_1q_lag.sh \
	mirror_gre_bridge_1q.sh \
	mirror_gre_changes.sh \
	mirror_gre_flower.sh \
	mirror_gre_lag_lacp.sh \
	mirror_gre_neigh.sh \
	mirror_gre_nh.sh \
	mirror_gre.sh \
	mirror_gre_vlan_bridge_1q.sh \
	mirror_gre_vlan.sh \
	mirror_vlan.sh \
	no_forwarding.sh \
	pedit_dsfield.sh \
	pedit_ip.sh \
	pedit_l4port.sh \
	q_in_vni_ipv6.sh \
	q_in_vni.sh \
	router_bridge.sh \
	router_bridge_vlan.sh \
	router_broadcast.sh \
	router_mpath_nh_res.sh \
	router_mpath_nh.sh \
	router_multicast.sh \
	router_multipath.sh \
	router_nh.sh \
	router.sh \
	router_vid_1.sh \
	sch_ets.sh \
	sch_red.sh \
	sch_tbf_ets.sh \
	sch_tbf_prio.sh \
	sch_tbf_root.sh \
	skbedit_priority.sh \
	tc_actions.sh \
	tc_chains.sh \
	tc_flower_router.sh \
	tc_flower.sh \
	tc_mpls_l2vpn.sh \
	tc_police.sh \
	tc_shblocks.sh \
	tc_tunnel_key.sh \
	tc_vlan_modify.sh \
	vxlan_asymmetric_ipv6.sh \
	vxlan_asymmetric.sh \
	vxlan_bridge_1d_ipv6.sh \
	vxlan_bridge_1d_port_8472_ipv6.sh \
	vxlan_bridge_1d_port_8472.sh \
	vxlan_bridge_1d.sh \
	vxlan_bridge_1q_ipv6.sh \
	vxlan_bridge_1q_port_8472_ipv6.sh \
	vxlan_bridge_1q_port_8472.sh \
	vxlan_bridge_1q.sh \
	vxlan_symmetric_ipv6.sh \
	vxlan_symmetric.sh

TEST_PROGS_EXTENDED := devlink_lib.sh \
	ethtool_lib.sh \
	fib_offload_lib.sh \
	forwarding.config.sample \
	ip6gre_lib.sh \
	ipip_lib.sh \
	lib.sh \
	mirror_gre_lib.sh \
	mirror_gre_topo_lib.sh \
	mirror_lib.sh \
	mirror_topo_lib.sh \
	sch_ets_core.sh \
	sch_ets_tests.sh \
	sch_tbf_core.sh \
	sch_tbf_etsprio.sh \
	tc_common.sh

include ../../lib.mk
",['# SPDX-License-Identifier: GPL-2.0+ OR MIT'],Dual-license GPL-2.0-or-later MIT,,0.0,0.0
82,82,92,linux-master/tools/testing/selftests/rseq/rseq-x86-thread-pointer.h,LGPL-2.1-only MIT Dual-license,662,[],"SPDX-License-Identifier: LGPL-2.1-only OR MIT
rseq-x86-thread-pointer.h** (C) Copyright 2021 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
!GCC 11","['SPDX-License-Identifier: LGPL-2.1-only OR MIT', 'SPDX-License-Identifier: LGPL-2.1-only']",LGPL-2.1-only MIT Dual-license,,0.0,0.0
83,83,93,linux-master/tools/testing/selftests/rseq/rseq-x86.h,LGPL-2.1-only MIT Dual-license,663,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq-x86.h** (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
RSEQ_SIG is used with the following reserved undefined instructions, which* trap in user-space:** x86-32:    0f b9 3d 53 30 05 53      ud1    0x53053053,%edi* x86-64:    0f b9 3d 53 30 05 53      ud1    0x53053053(%rip),%edi
Due to a compiler optimization bug in gcc-8 with asm goto and TLS asm input* operands, we cannot use ""m"" input operands, and rather pass the __rseq_abi* address through a ""r"" input operand.
Offset of cpu_id, rseq_cs, and mm_cid fields in struct rseq.
Exit points of a rseq critical section consist of all instructions outside* of the critical section where a critical section can either branch to or* reach through the normal course of its execution. The abort IP and the* post-commit IP are already part of the __rseq_cs section and should not be* explicitly defined as additional exit points. Knowing all exit points is* useful to assist debuggers stepping over the critical section.
Disassembler-friendly signature: ud1 <sig>(%rip),%edi.
Use eax as scratch register and take memory operands as input to* lessen register pressure. Especially needed when compiling in O0.
Exit points of a rseq critical section consist of all instructions outside* of the critical section where a critical section can either branch to or* reach through the normal course of its execution. The abort IP and the* post-commit IP are already part of the __rseq_cs section and should not be* explicitly defined as additional exit points. Knowing all exit points is* useful to assist debuggers stepping over the critical section.
Disassembler-friendly signature: ud1 <sig>,%edi.
Per-cpu-id indexing.
Per-mm-cid indexing.
APIs which are not based on cpu ids.",['SPDX-License-Identifier: LGPL-2.1 OR MIT'],LGPL-2.1-only MIT Dual-license,,0.0,0.0
84,84,94,linux-master/tools/testing/selftests/rseq/rseq.h,LGPL-2.1-only MIT Dual-license,675,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq.h** (C) Copyright 2016-2018 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Empty code injection macros, override when testing.* It is important to consider that the ASM injection macros need to be* fully reentrant (e.g. do not modify the stack).
Offset from the thread pointer to the rseq area.
Size of the registered rseq area. 0 if the registration was* unsuccessful.
Flags used during rseq registration.
rseq feature size supported by the kernel. 0 if the registration was* unsuccessful.
Unused
Unused
Unused
Unused
Register rseq for the current thread. This needs to be called once* by any thread which uses restartable sequences, before they start* using restartable sequences, to ensure restartable sequences* succeed. A restartable sequence executed from a non-registered* thread will always fail.
Unregister rseq for current thread.
Restartable sequence fallback for reading the current CPU number.
Restartable sequence fallback for reading the current node number.
Values returned can be either the current CPU number, -1 (rseq is* uninitialized), or -2 (rseq initialization has failed).
Returns a possible CPU number, which is typically the current CPU.* The returned CPU number can be used to prepare for an rseq critical* section, which will confirm whether the cpu number is indeed the* current one, and whether rseq is initialized.** The CPU number returned by rseq_cpu_start should always be validated* by passing it to a rseq asm sequence, or by comparing it to the* return value of rseq_current_cpu_raw() if the rseq asm sequence* does not need to be invoked.
Current NUMA node number.
rseq_prepare_unload() should be invoked by each thread executing a rseq* critical section at least once between their last critical section and* library unload of the library defining the rseq critical section (struct* rseq_cs) or the code referred to by the struct rseq_cs start_ip and* post_commit_offset fields. This also applies to use of rseq in code* generated by JIT: rseq_prepare_unload() should be invoked at least once by* each thread executing a rseq critical section before reclaim of the memory* holding the struct rseq_cs or reclaim of the code pointed to by struct* rseq_cs start_ip and post_commit_offset fields.
Compare @v against @expectnot. When it does _not_ match, load @v* into @load, and store the content of *@v + voffp into @v.
pval = *(ptr+off)*  *pval += inc;
RSEQ_H_",['SPDX-License-Identifier: LGPL-2.1 OR MIT'],LGPL-2.1-only MIT Dual-license,,0.0,0.0
85,85,95,linux-master/tools/testing/selftests/rseq/rseq-x86-bits.h,LGPL-2.1-only MIT Dual-license,664,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq-x86-bits.h** (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
Compare @v against @expectnot. When it does _not_ match, load @v* into @load, and store the content of *@v + voffp into @v.
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
final store input
pval = *(ptr+off)*  *pval += inc;
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
get p+v
get pv
pv += inc
gcc asm goto does not allow outputs
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
cmp2 input
final store input
#if defined(RSEQ_TEMPLATE_MO_RELAXED) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
try store
final store
gcc asm goto does not allow outputs
try store input
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
try memcpy
final store
teardown
gcc asm goto does not allow outputs
final store input
try memcpy input
#if (defined(RSEQ_TEMPLATE_MO_RELAXED) || defined(RSEQ_TEMPLATE_MO_RELEASE)) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
Compare @v against @expectnot. When it does _not_ match, load @v* into @load, and store the content of *@v + voffp into @v.
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
cmp2 input
final store input
#if defined(RSEQ_TEMPLATE_MO_RELAXED) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
try store
final store
gcc asm goto does not allow outputs
try store input
final store input
TODO: implement a faster memcpy.
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
try memcpy
final store
teardown
gcc asm goto does not allow outputs
final store input
try memcpy input
#if (defined(RSEQ_TEMPLATE_MO_RELAXED) || defined(RSEQ_TEMPLATE_MO_RELEASE)) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))","['SPDX-License-Identifier: LGPL-2.1 OR MIT', 'SPDX-License-Identifier: LGPL-2.1 OR MIT']",LGPL-2.1-only MIT Dual-license,,0.0,0.0
86,86,96,linux-master/tools/testing/selftests/rseq/rseq-mips-bits.h,LGPL-2.1-only MIT Dual-license,674,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
Author: Paul Burton <paul.burton@mips.com>* (C) Copyright 2018 MIPS Tech LLC* (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
cmp2 input
final store input
#if defined(RSEQ_TEMPLATE_MO_RELAXED) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
try store
full sync provides store-release
final store
gcc asm goto does not allow outputs
try store input
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
try memcpy
full sync provides store-release
final store
teardown
teardown
teardown
teardown
teardown
gcc asm goto does not allow outputs
final store input
try memcpy input
#if (defined(RSEQ_TEMPLATE_MO_RELAXED) || defined(RSEQ_TEMPLATE_MO_RELEASE)) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))",['SPDX-License-Identifier: LGPL-2.1 OR MIT'],LGPL-2.1-only MIT Dual-license,,0.0,0.0
87,87,97,linux-master/tools/testing/selftests/rseq/rseq-thread-pointer.h,LGPL-2.1-only MIT Dual-license,665,[],"SPDX-License-Identifier: LGPL-2.1-only OR MIT
rseq-thread-pointer.h** (C) Copyright 2021 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>","['SPDX-License-Identifier: LGPL-2.1-only OR MIT', 'SPDX-License-Identifier: LGPL-2.1-only OR MIT']",LGPL-2.1-only MIT Dual-license,,0.0,0.0
88,88,98,linux-master/tools/testing/selftests/rseq/rseq-arm-bits.h,LGPL-2.1-only MIT Dual-license,681,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq-arm-bits.h** (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
cmp2 input
final store input
#if defined(RSEQ_TEMPLATE_MO_RELAXED) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
try store
full mb provides store-release
final store
gcc asm goto does not allow outputs
try store input
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
try memcpy
full mb provides store-release
final store
teardown
teardown
teardown
teardown
teardown
gcc asm goto does not allow outputs
final store input
try memcpy input
#if (defined(RSEQ_TEMPLATE_MO_RELAXED) || defined(RSEQ_TEMPLATE_MO_RELEASE)) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))","['SPDX-License-Identifier: LGPL-2.1 OR MIT', 'SPDX-License-Identifier: LGPL-2.1 OR MIT']",LGPL-2.1-only MIT Dual-license,,0.0,0.0
89,89,99,linux-master/tools/testing/selftests/rseq/rseq-bits-reset.h,MIT LGPL-2.1-only Dual-license,679,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq-bits-reset.h** (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>","['SPDX-License-Identifier: LGPL-2.1 OR MIT', 'SPDX-License-Identifier: LGPL-2.1 OR MIT']",MIT LGPL-2.1-only Dual-license,,0.0,0.0
90,90,100,linux-master/tools/testing/selftests/rseq/rseq-s390.h,MIT LGPL-2.1-only Dual-license,666,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
RSEQ_SIG uses the trap4 instruction. As Linux does not make use of the* access-register mode nor the linkage stack this instruction will always* cause a special-operation exception (the trap-enabled bit in the DUCT* is and will stay 0). The instruction pattern is*	b2 ff 0f ff	trap4	4095(%r0)
Exit points of a rseq critical section consist of all instructions outside* of the critical section where a critical section can either branch to or* reach through the normal course of its execution. The abort IP and the* post-commit IP are already part of the __rseq_cs section and should not be* explicitly defined as additional exit points. Knowing all exit points is* useful to assist debuggers stepping over the critical section.
Exit points of a rseq critical section consist of all instructions outside* of the critical section where a critical section can either branch to or* reach through the normal course of its execution. The abort IP and the* post-commit IP are already part of the __rseq_cs section and should not be* explicitly defined as additional exit points. Knowing all exit points is* useful to assist debuggers stepping over the critical section.
Per-cpu-id indexing.
Per-mm-cid indexing.
APIs which are not based on cpu ids.",['SPDX-License-Identifier: LGPL-2.1 OR MIT'],MIT LGPL-2.1-only Dual-license,,0.0,0.0
91,91,101,linux-master/tools/testing/selftests/rseq/rseq-ppc-bits.h,Dual-license MIT LGPL-2.1-only,672,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq-ppc-bits.h** (C) Copyright 2016-2018 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>* (C) Copyright 2016-2018 - Boqun Feng <boqun.feng@gmail.com>
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
cmp cpuid
cmp @v equal to @expect
cmp cpuid
cmp @v equal to @expect
final store
gcc asm goto does not allow outputs
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
cmp cpuid
cmp @v not equal to @expectnot
cmp cpuid
cmp @v not equal to @expectnot
load the value of @v
store it in @load
dereference voffp(v)
final store the value at voffp(v)
gcc asm goto does not allow outputs
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
cmp cpuid
cmp cpuid
load the value of @v
add @count to it
final store
gcc asm goto does not allow outputs
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
cmp cpuid
cmp @v equal to @expect
cmp @v2 equal to @expct2
cmp cpuid
cmp @v equal to @expect
cmp @v2 equal to @expct2
final store
gcc asm goto does not allow outputs
cmp2 input
final store input
#if defined(RSEQ_TEMPLATE_MO_RELAXED) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
cmp cpuid
cmp @v equal to @expect
cmp cpuid
cmp @v equal to @expect
try store
for 'release'
final store
gcc asm goto does not allow outputs
try store input
final store input
start, commit, abort
setup for mempcy
Start rseq by storing table entry pointer into rseq_cs.
cmp cpuid
cmp @v equal to @expect
cmp cpuid
cmp @v equal to @expect
try memcpy
for 'release'
final store
teardown
gcc asm goto does not allow outputs
final store input
try memcpy input
#if (defined(RSEQ_TEMPLATE_MO_RELAXED) || defined(RSEQ_TEMPLATE_MO_RELEASE)) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))",['SPDX-License-Identifier: LGPL-2.1 OR MIT'],Dual-license MIT LGPL-2.1-only,,0.0,0.0
92,92,102,linux-master/tools/testing/selftests/rseq/rseq-arm.h,Dual-license MIT LGPL-2.1-only,680,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq-arm.h** (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
- ARM little endian** RSEQ_SIG uses the udf A32 instruction with an uncommon immediate operand* value 0x5de3. This traps if user-space reaches this instruction by mistake,* and the uncommon operand ensures the kernel does not move the instruction* pointer to attacker-controlled code on rseq abort.** The instruction pattern in the A32 instruction set is:** e7f5def3    udf    #24035    ; 0x5de3** This translates to the following instruction pattern in the T16 instruction* set:** little endian:* def3        udf    #243      ; 0xf3* e7f5        b.n    <7f5>** - ARMv6+ big endian (BE8):** ARMv6+ -mbig-endian generates mixed endianness code vs data: little-endian* code and big-endian data. The data value of the signature needs to have its* byte order reversed to generate the trap instruction:** Data: 0xf3def5e7** Translates to this A32 instruction pattern:** e7f5def3    udf    #24035    ; 0x5de3** Translates to this T16 instruction pattern:** def3        udf    #243      ; 0xf3* e7f5        b.n    <7f5>** - Prior to ARMv6 big endian (BE32):** Prior to ARMv6, -mbig-endian generates big-endian code and data* (which match), so the endianness of the data representation of the* signature should not be reversed. However, the choice between BE32* and BE8 is done by the linker, so we cannot know whether code and* data endianness will be mixed before the linker is invoked. So rather* than try to play tricks with the linker, the rseq signature is simply* data (not a trap instruction) prior to ARMv6 on big endian. This is* why the signature is expressed as data (.word) rather than as* instruction (.inst) in assembler.
udf    #24035    ; 0x5de3 (ARMv6+)
udf    #24035    ; 0x5de3
Exit points of a rseq critical section consist of all instructions outside* of the critical section where a critical section can either branch to or* reach through the normal course of its execution. The abort IP and the* post-commit IP are already part of the __rseq_cs section and should not be* explicitly defined as additional exit points. Knowing all exit points is* useful to assist debuggers stepping over the critical section.
Per-cpu-id indexing.
Per-mm-cid indexing.
APIs which are not based on cpu ids.",['SPDX-License-Identifier: LGPL-2.1 OR MIT'],Dual-license MIT LGPL-2.1-only,,0.0,0.0
93,93,103,linux-master/tools/testing/selftests/rseq/rseq-s390-bits.h,Dual-license MIT LGPL-2.1-only,667,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
Compare @v against @expectnot. When it does _not_ match, load @v* into @load, and store the content of *@v + voffp into @v.
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
final store input
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
final store
gcc asm goto does not allow outputs
cmp2 input
final store input
#if defined(RSEQ_TEMPLATE_MO_RELAXED) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))
s390 is TSO.
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
try store
final store
gcc asm goto does not allow outputs
try store input
final store input
s390 is TSO.
start, commit, abort
Start rseq by storing table entry pointer into rseq_cs.
try memcpy
final store
teardown
gcc asm goto does not allow outputs
final store input
try memcpy input
#if (defined(RSEQ_TEMPLATE_MO_RELAXED) || defined(RSEQ_TEMPLATE_MO_RELEASE)) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))","['SPDX-License-Identifier: LGPL-2.1 OR MIT', 'SPDX-License-Identifier: GPL-2.0-only']",Dual-license MIT LGPL-2.1-only,,0.0,0.0
94,94,104,linux-master/tools/testing/selftests/rseq/rseq-generic-thread-pointer.h,Dual-license MIT LGPL-2.1-only,676,[],"SPDX-License-Identifier: LGPL-2.1-only OR MIT
rseq-generic-thread-pointer.h** (C) Copyright 2021 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Use gcc builtin thread pointer.","['SPDX-License-Identifier: LGPL-2.1-only OR MIT', 'SPDX-License-Identifier: LGPL-2.1-only OR MIT']",Dual-license MIT LGPL-2.1-only,,0.0,0.0
95,95,105,linux-master/tools/testing/selftests/rseq/rseq-riscv-bits.h,LGPL-2.1-only Dual-license MIT,669,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
gcc asm goto does not allow outputs
gcc asm goto does not allow outputs
gcc asm goto does not allow outputs
gcc asm goto does not allow outputs
pval = *(ptr+off)*  *pval += inc;
gcc asm goto does not allow outputs
#if defined(RSEQ_TEMPLATE_MO_RELAXED) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))
gcc asm goto does not allow outputs
gcc asm goto does not allow outputs
#if (defined(RSEQ_TEMPLATE_MO_RELAXED) || defined(RSEQ_TEMPLATE_MO_RELEASE)) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))","[""`\n['SPDX-License-Identifier: LGPL-2.1 OR MIT"", ""SPDX-License-Identifier: GPL-2.0-only']\n`""]",LGPL-2.1-only Dual-license MIT,,0.0,0.0
96,96,106,linux-master/tools/testing/selftests/rseq/rseq-ppc.h,LGPL-2.1-only Dual-license MIT,671,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq-ppc.h** (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>* (C) Copyright 2016-2018 - Boqun Feng <boqun.feng@gmail.com>
RSEQ_SIG is used with the following trap instruction:** powerpc-be:    0f e5 00 0b           twui   r5,11* powerpc64-le:  0b 00 e5 0f           twui   r5,11* powerpc64-be:  0f e5 00 0b           twui   r5,11
The __rseq_cs_ptr_array and __rseq_cs sections can be used by debuggers to* better handle single-stepping through the restartable critical sections.
To memory (""m"" constraint)
To memory (""m"" constraint)
From memory (""m"" constraint)
From memory (""m"" constraint)
From base register (""b"" constraint)
Exit points of a rseq critical section consist of all instructions outside* of the critical section where a critical section can either branch to or* reach through the normal course of its execution. The abort IP and the* post-commit IP are already part of the __rseq_cs section and should not be* explicitly defined as additional exit points. Knowing all exit points is* useful to assist debuggers stepping over the critical section.
#ifdef __PPC64__
To memory (""m"" constraint)
To memory (""m"" constraint)
From memory (""m"" constraint)
From memory (""m"" constraint)
From base register (""b"" constraint)
32-bit only supported on BE
Exit points of a rseq critical section consist of all instructions outside* of the critical section where a critical section can either branch to or* reach through the normal course of its execution. The abort IP and the* post-commit IP are already part of the __rseq_cs section and should not be* explicitly defined as additional exit points. Knowing all exit points is* useful to assist debuggers stepping over the critical section.
32-bit only supported on BE
#ifdef __PPC64__
RSEQ_ASM_OPs: asm operations for rseq* 	RSEQ_ASM_OP_R_*: has hard-code registers in it* 	RSEQ_ASM_OP_* (else): doesn't have hard-code registers(unless cr7)
Load @var to r17
Store r17 to @var
Add @count to r17
Load (r17 + voffp) to r17
TODO: implement a faster memcpy.
Per-cpu-id indexing.
Per-mm-cid indexing.
APIs which are not based on cpu ids.",['SPDX-License-Identifier: LGPL-2.1 OR MIT'],LGPL-2.1-only Dual-license MIT,,0.0,0.0
97,97,107,linux-master/tools/testing/selftests/rseq/rseq-riscv.h,LGPL-2.1-only Dual-license MIT,668,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
Select the instruction ""csrw mhartid, x0"" as the RSEQ_SIG. Unlike* other architectures, the ebreak instruction has no immediate field for* distinguishing purposes. Hence, ebreak is not suitable as RSEQ_SIG.* ""csrw mhartid, x0"" can also satisfy the RSEQ requirement because it* is an uncommon instruction and will raise an illegal instruction* exception when executed in all modes.
csrr mhartid, x0
Exit points of a rseq critical section consist of all instructions outside* of the critical section where a critical section can either branch to or* reach through the normal course of its execution. The abort IP and the* post-commit IP are already part of the __rseq_cs section and should not be* explicitly defined as additional exit points. Knowing all exit points is* useful to assist debuggers stepping over the critical section.
Per-cpu-id indexing.
Per-mm-cid indexing.
APIs which are not based on cpu ids.",['SPDX-License-Identifier: LGPL-2.1 OR MIT'],LGPL-2.1-only Dual-license MIT,,0.0,0.0
98,98,108,linux-master/tools/testing/selftests/rseq/rseq-ppc-thread-pointer.h,LGPL-2.1-only Dual-license MIT,670,[],"SPDX-License-Identifier: LGPL-2.1-only OR MIT
rseq-ppc-thread-pointer.h** (C) Copyright 2021 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>","['SPDX-License-Identifier: LGPL-2.1-only OR MIT', 'SPDX-License-Identifier: LGPL-2.1-only OR MIT']",LGPL-2.1-only Dual-license MIT,,0.0,0.0
99,99,109,linux-master/tools/testing/selftests/rseq/rseq-mips.h,MIT Dual-license LGPL-2.1-only,673,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
Author: Paul Burton <paul.burton@mips.com>* (C) Copyright 2018 MIPS Tech LLC* (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
RSEQ_SIG uses the break instruction. The instruction pattern is:** On MIPS:*	0350000d        break     0x350** On nanoMIPS:*      00100350        break     0x350** On microMIPS:*      0000d407        break     0x350** For nanoMIPS32 and microMIPS, the instruction stream is encoded as 16-bit* halfwords, so the signature halfwords need to be swapped accordingly for* little-endian.
Unknown MIPS architecture.
Exit points of a rseq critical section consist of all instructions outside* of the critical section where a critical section can either branch to or* reach through the normal course of its execution. The abort IP and the* post-commit IP are already part of the __rseq_cs section and should not be* explicitly defined as additional exit points. Knowing all exit points is* useful to assist debuggers stepping over the critical section.
Per-cpu-id indexing.
Per-mm-cid indexing.
APIs which are not based on cpu ids.","['Copyright 2018 MIPS Tech LLC', 'SPDX-License-Identifier: LGPL-2.1 OR MIT']",MIT Dual-license LGPL-2.1-only,,0.0,0.0
100,100,110,linux-master/tools/testing/selftests/rseq/rseq-bits-template.h,MIT Dual-license LGPL-2.1-only,678,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq-bits-template.h** (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>","['SPDX-License-Identifier: LGPL-2.1 OR MIT', 'SPDX-License-Identifier: LGPL-2.1 OR MIT']",MIT Dual-license LGPL-2.1-only,,0.0,0.0
101,101,111,linux-master/tools/testing/selftests/rseq/rseq-arm64.h,MIT Dual-license LGPL-2.1-only,682,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq-arm64.h** (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>* (C) Copyright 2018 - Will Deacon <will.deacon@arm.com>
aarch64 -mbig-endian generates mixed endianness code vs data:* little-endian code and big-endian data. Ensure the RSEQ_SIG signature* matches code endianness.
BRK #0x45E0.
BRK #0x45E0.
Exit points of a rseq critical section consist of all instructions outside* of the critical section where a critical section can either branch to or* reach through the normal course of its execution. The abort IP and the* post-commit IP are already part of the __rseq_cs section and should not be* explicitly defined as additional exit points. Knowing all exit points is* useful to assist debuggers stepping over the critical section.
Per-cpu-id indexing.
Per-mm-cid indexing.
APIs which are not based on cpu ids.",['SPDX-License-Identifier: LGPL-2.1 OR MIT'],MIT Dual-license LGPL-2.1-only,,0.0,0.0
102,102,112,linux-master/tools/testing/selftests/rseq/compiler.h,Dual-license LGPL-2.1-only MIT,688,[],"SPDX-License-Identifier: LGPL-2.1-only OR MIT
rseq/compiler.h** Work-around asm goto compiler bugs.** (C) Copyright 2021 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
gcc prior to 4.8.2 miscompiles asm goto.* https://gcc.gnu.org/bugzilla/show_bug.cgi?id=58670** gcc prior to 8.1.0 miscompiles asm goto at O1.* https://gcc.gnu.org/bugzilla/show_bug.cgi?id=103908** clang prior to version 13.0.1 miscompiles asm goto at O2.* https://github.com/llvm/llvm-project/issues/52735** Work around these issues by adding a volatile inline asm with* memory clobber in the fallthrough after the asm goto and at each* label target.  Emit this for all compilers in case other similar* issues are found in the future.
Combine two tokens.
RSEQ_COMPILER_H_","['SPDX-License-Identifier: LGPL-2.1-only OR MIT', 'SPDX-License-Identifier: LGPL-2.1-only OR MIT']",Dual-license LGPL-2.1-only MIT,,0.0,0.0
103,103,113,linux-master/tools/testing/selftests/rseq/rseq-arm64-bits.h,Dual-license LGPL-2.1-only MIT,683,[],"SPDX-License-Identifier: LGPL-2.1 OR MIT
rseq-arm64-bits.h** (C) Copyright 2016-2022 - Mathieu Desnoyers <mathieu.desnoyers@efficios.com>* (C) Copyright 2018 - Will Deacon <will.deacon@arm.com>
gcc asm goto does not allow outputs
gcc asm goto does not allow outputs
gcc asm goto does not allow outputs
gcc asm goto does not allow outputs
#if defined(RSEQ_TEMPLATE_MO_RELAXED) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))
gcc asm goto does not allow outputs
gcc asm goto does not allow outputs
#if (defined(RSEQ_TEMPLATE_MO_RELAXED) || defined(RSEQ_TEMPLATE_MO_RELEASE)) &&	(defined(RSEQ_TEMPLATE_CPU_ID) || defined(RSEQ_TEMPLATE_MM_CID))","['SPDX-License-Identifier: LGPL-2.1 OR MIT', 'SPDX-License-Identifier: LGPL-2.1 OR MIT']",Dual-license LGPL-2.1-only MIT,,0.0,0.0
104,104,114,linux-master/tools/testing/selftests/rseq/rseq-abi.h,Linux-syscall-note GPL-2.0-or-later,684,[],"SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note
rseq-abi.h** Restartable sequences system call API** Copyright (c) 2015-2022 Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
struct rseq_abi_cs is aligned on 4 * 8 bytes to ensure it is always* contained within a single cache-line. It is usually declared as* link-time constant data.
Version of this structure.
enum rseq_abi_cs_flags
Offset from start_ip.
struct rseq_abi is aligned on 4 * 8 bytes to ensure it is always* contained within a single cache-line.** A single struct rseq_abi per thread is allowed.
Restartable sequences cpu_id_start field. Updated by the* kernel. Read by user-space with single-copy atomicity* semantics. This field should only be read by the thread which* registered this data structure. Aligned on 32-bit. Always* contains a value in the range of possible CPUs, although the* value may not be the actual current CPU (e.g. if rseq is not* initialized). This CPU number value should always be compared* against the value of the cpu_id field before performing a rseq* commit or returning a value read from a data structure indexed* using the cpu_id_start value.
Restartable sequences cpu_id field. Updated by the kernel.* Read by user-space with single-copy atomicity semantics. This* field should only be read by the thread which registered this* data structure. Aligned on 32-bit. Values* RSEQ_CPU_ID_UNINITIALIZED and RSEQ_CPU_ID_REGISTRATION_FAILED* have a special semantic: the former means ""rseq uninitialized"",* and latter means ""rseq initialization failed"". This value is* meant to be read within rseq critical sections and compared* with the cpu_id_start value previously read, before performing* the commit instruction, or read and compared with the* cpu_id_start value before returning a value loaded from a data* structure indexed using the cpu_id_start value.
Restartable sequences rseq_cs field.** Contains NULL when no critical section is active for the current* thread, or holds a pointer to the currently active struct rseq_cs.** Updated by user-space, which sets the address of the currently* active rseq_cs at the beginning of assembly instruction sequence* block, and set to NULL by the kernel when it restarts an assembly* instruction sequence block, as well as when the kernel detects that* it is preempting or delivering a signal outside of the range* targeted by the rseq_cs. Also needs to be set to NULL by user-space* before reclaiming memory that contains the targeted struct rseq_cs.** Read and set by the kernel. Set by user-space with single-copy* atomicity semantics. This field should only be updated by the* thread which registered this data structure. Aligned on 64-bit.
The ""arch"" field provides architecture accessor for* the ptr field based on architecture pointer size and* endianness.
Initialized to zero.
Initialized to zero.
Restartable sequences flags field.** This field should only be updated by the thread which* registered this data structure. Read by the kernel.* Mainly used for single-stepping through rseq critical sections* with debuggers.** - RSEQ_ABI_CS_FLAG_NO_RESTART_ON_PREEMPT*     Inhibit instruction sequence block restart on preemption*     for this thread.* - RSEQ_ABI_CS_FLAG_NO_RESTART_ON_SIGNAL*     Inhibit instruction sequence block restart on signal*     delivery for this thread.* - RSEQ_ABI_CS_FLAG_NO_RESTART_ON_MIGRATE*     Inhibit instruction sequence block restart on migration for*     this thread.
Restartable sequences node_id field. Updated by the kernel. Read by* user-space with single-copy atomicity semantics. This field should* only be read by the thread which registered this data structure.* Aligned on 32-bit. Contains the current NUMA node ID.
Restartable sequences mm_cid field. Updated by the kernel. Read by* user-space with single-copy atomicity semantics. This field should* only be read by the thread which registered this data structure.* Aligned on 32-bit. Contains the current thread's concurrency ID* (allocated uniquely within a memory map).
Flexible array member at end of structure, after last feature field.
_RSEQ_ABI_H","['SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note', 'SPDX-License-Identifier: GPL-2.0-only']",Linux-syscall-note GPL-2.0-or-later,,0.0,0.0
105,105,115,linux-master/tools/testing/selftests/net/nat6to4.c,GPL-2.0-only Apache-2.0,1482,[],"SPDX-License-Identifier: GPL-2.0-only
Flag: ""Don't Fragment""
used iff is_ethernet
Require ethernet dst mac address to be our unicast address.
Must be meta-ethernet IPv6 frame
Must have (ethernet and) ipv6 header
Ethertype - if present - must be IPv6
IP version must be 6
Maximum IPv6 payload length that can be translated to IPv4
do not know how to handle anything else
used iff is_ethernet
Calculate the IPv4 one's complement checksum of the IPv4 header.
Calculate the *negative* IPv6 16-bit one's complement checksum of the IPv6 header.
We'll end up with a non-zero sum due to ip6->version == 6 (which has '0' bits)
note the bitwise negation
Copy over the updated ethernet header
Copy over the new ipv4 header.
used iff is_ethernet
Must be meta-ethernet IPv4 frame
Must have ipv4 header
Ethertype - if present - must be IPv4
IP version must be 4
We cannot handle IP options, just standard 20 byte == 5 dword minimal IPv4 header
Maximum IPv6 payload length that can be translated to IPv4
Calculate the IPv4 one's complement checksum of the IPv4 header.
Minimum IPv4 total length is the size of the header
We are incapable of dealing with IPv4 fragments
See above comment, but must also have UDP header...
do not know how to handle anything else
used iff is_ethernet
Calculate the IPv6 16-bit one's complement checksum of the IPv6 header.
We'll end up with a non-zero sum due to ip6.version == 6
bpf_skb_change_proto() invalidates all pointers - reload them.
Copy over the updated ethernet header
Copy over the new ipv4 header.
 For TCP & UDP the checksum neutrality of the chosen IPv6 address means there is no need to update their checksums. We do not need to bother looking at GRE/ESP headers, since there is never a checksum to update.
 Copy over the ethernet header (src/dst mac) But replace the ethertype
 u4 u4 u8 u16 u16 u16 u8 u8 u16 u32 u32
 Note that sum4 is guaranteed to be non-zero by virtue of ip.version == 4 collapse u32 into range 1 .. 0x1FFFE collapse any potential carry into u16 sum4 cannot be zero, so this is never 0xFFFF
 Note that there is no L4 checksum update: we are relying on the checksum neutrality of the ipv6 address chosen by netd's ClatdController.
 Packet mutations begin - point of no return, but if this first modification fails the packet is probably still pristine, so let clatd handle it.
 Note that sum4 is guaranteed to be non-zero by virtue of ip4->version == 4 collapse u32 into range 1 .. 0x1FFFE collapse any potential carry into u16 for a correct checksum we should get *a* zero, but sum4 must be positive, ie 0xFFFF
 For TCP & UDP the checksum neutrality of the chosen IPv6 address means there is no need to update their checksums. We do not need to bother looking at GRE/ESP headers, since there is never a checksum to update.
 If IPv4/UDP checksum is 0 then fallback to clatd so it can calculate the checksum.  Otherwise the network or more likely the NAT64 gateway might drop the packet because in most cases IPv6/UDP packets with a zero checksum are invalid. See RFC 6935.  TODO: calculate checksum via bpf_csum_diff()
 Copy over the ethernet header (src/dst mac) But replace the ethertype
 __u8:4 __u8:4 __u8[3] __be16 __u8 __u8
 Packet mutations begin - point of no return, but if this first modification fails the packet is probably still pristine, so let clatd handle it.
 This takes care of updating the skb->csum field for a CHECKSUM_COMPLETE packet. In such a case, skb->csum is a 16-bit one's complement sum of the entire payload, thus we need to subtract out the ipv4 header's sum, and add in the ipv6 header's sum. However, we've already verified the ipv4 checksum is correct and thus 0. Thus we only need to add the ipv6 header's sum.
 bpf_csum_update() always succeeds if the skb is CHECKSUM_COMPLETE and returns an error (-ENOTSUPP) if it isn't.  So we just ignore the return code (see above for more details).
 I cannot think of any valid way for this error condition to trigger, however I do believe the explicit check is required to keep the in kernel ebpf verifier happy.
This code is taken from the Android Open Source Project and the author* (Maciej Żenczykowski) has gave permission to relicense it under the* GPLv2. Therefore this program is free software;* You can redistribute it and/or modify it under the terms of the GNU* General Public License version 2 as published by the Free Software* Foundation* The original headers, including the original license headers, are* included below for completeness.** Copyright (C) 2019 The Android Open Source Project** Licensed under the Apache License, Version 2.0 (the ""License"");* you may not use this file except in compliance with the License.* You may obtain a copy of the License at**      http://www.apache.org/licenses/LICENSE-2.0** Unless required by applicable law or agreed to in writing, software* distributed under the License is distributed on an ""AS IS"" BASIS,* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.* See the License for the specific language governing permissions and* limitations under the License.","['Copyright (C) 2019 The Android Open Source Project** Licensed under the Apache License, Version 2.0 (the ""License"");* you may not use this file except in compliance with the License.* You may obtain a copy of the License at**      http://www.apache.org/licenses/LICENSE-2.0** Unless required by applicable law or agreed to in writing, software* distributed under the License is distributed on an ""AS IS"" BASIS,* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.* See the License for the specific language governing permissions and* limitations under the License.', 'SPDX-License-Identifier: GPL-2.0-only']",GPL-2.0-only Apache-2.0,,0.0,0.0
106,106,116,linux-master/tools/testing/selftests/net/ip_local_port_range.c,GPL-2.0-only BSD-3-Clause Dual-license,1493,[]," SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause Copyright (c) 2023 Cloudflare
Test IP_LOCAL_PORT_RANGE socket option: IPv4 + IPv6, TCP + UDP.** Tests assume that net.ipv4.ip_local_port_range is [40000, 49999].* Don't run these directly but with ip_local_port_range.sh script.
Too few bytes
Empty range: low port > high port
Too many bytes
socket range below netns range
socket range above netns range
Bind a couple of sockets, not just one, to check* that the range wasn't clamped to a single port from* the netns range. That is [40000, 40000] or [49999,* 49999], respectively for each test case.
Check that socket port range outside of ephemeral range is ignored
single port range within ephemeral range
first port in the ephemeral range (clamp from above)
last port in the ephemeral range (clamp from below)
Check that all every port from the test range is in use
Check that bind() fails because the whole range is busy
Invalid destination
connect() doesn't need to succeed for late bind to happen
Get range before it will be set
Get range after it has been set
Unset the port range
Get range after it has been unset",['SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause'],GPL-2.0-only BSD-3-Clause Dual-license,,0.0,0.0
107,107,117,linux-master/tools/testing/selftests/mqueue/mq_open_tests.c,GPL-3.0-only,1695,[],"This application is Copyright 2012 Red Hat, Inc.*	Doug Ledford <dledford@redhat.com>** mq_open_tests is free software: you can redistribute it and/or modify* it under the terms of the GNU General Public License as published by* the Free Software Foundation, version 3.** mq_open_tests is distributed in the hope that it will be useful,* but WITHOUT ANY WARRANTY; without even the implied warranty of* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the* GNU General Public License for more details.** For the full text of the license, see <http://www.gnu.org/licenses/>.** mq_open_tests.c*   Tests the various situations that should either succeed or fail to*   open a posix message queue and then reports whether or not they*   did as they were supposed to.
In case we get called recursively by a set() call below
Be silent if this fails, if we cleaned up already it's* expected to fail
test_queue - Test opening a queue, shutdown if we fail.  This should* only be called in situations that should never fail.  We clean up* after ourselves and return the queue attributes in *result.
Same as test_queue above, but failure is not fatal.* Returns:* 0 - Failed to create a queue* 1 - Created a queue, attributes in *result
Although we can create a msg queue with a non-absolute path name,* unlink will fail.  So, if the name doesn't start with a /, add one* when we save it.
Find out what files there are for us to make tweaks in
Load up the current system values for everything we can
Tell the user our initial state
While we are here, go ahead and test that the kernel* properly follows the default settings
In case max was the same as the default
Test #2 - open with an attr struct that exceeds rlimit","['Copyright 2012 Red Hat, Inc.*\\tDoug Ledford <dledford@redhat.com>** SPDX-License-Identifier: GPL-2.0-only', 'For the full text of the license, see <http://www.gnu.org/licenses/>', 'GNU General Public License as published by* the Free Software Foundation, version 3.', 'mq_open_tests is free software: you can redistribute it and/or modify* it under the terms of the GNU General Public License as published by* the Free Software Foundation, version 3.']",GPL-3.0-only,,0.0,0.0
108,108,118,linux-master/tools/testing/selftests/mqueue/mq_perf_tests.c,GPL-3.0-only,1694,[],"This application is Copyright 2012 Red Hat, Inc.*	Doug Ledford <dledford@redhat.com>** mq_perf_tests is free software: you can redistribute it and/or modify* it under the terms of the GNU General Public License as published by* the Free Software Foundation, version 3.** mq_perf_tests is distributed in the hope that it will be useful,* but WITHOUT ANY WARRANTY; without even the implied warranty of* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the* GNU General Public License for more details.** For the full text of the license, see <http://www.gnu.org/licenses/>.** mq_perf_tests.c*   Tests various types of message queue workloads, concentrating on those*   situations that invole large message sizes, large message queue depths,*   or both, and reports back useful metrics about kernel message queue*   performance.
In case we get called by multiple threads or from an sighandler
Free the cpu_set allocated using CPU_ALLOC in main function
Be silent if this fails, if we cleaned up already it's* expected to fail
open_queue - open the global queue for testing* @attr - An attr struct specifying the desired queue traits* @result - An attr struct that lists the actual traits the queue has** This open is not allowed to fail, failure will result in an orderly* shutdown of the program.  The global queue_path is used to set what* queue to open, the queue descriptor is saved in the global queue* variable.
Tests to perform (all done with MSG_SIZE messages):** 1) Time to add/remove message with 0 messages on queue* 1a) with constant prio* 2) Time to add/remove message when queue close to capacity:* 2a) with constant prio* 2b) with increasing prio* 2c) with decreasing prio* 2d) with random prio* 3) Test limits of priorities honored (double check _SC_MQ_PRIO_MAX)
Double check that they didn't give us the same CPU			 * more than once
Although we can create a msg queue with a* non-absolute path name, unlink will fail.  So,* if the name doesn't start with a /, add one* when we save it.
Load up the current system values for everything we can
Tell the user our initial state","['This application is Copyright 2012 Red Hat, Inc.*\\tDoug Ledford <dledford@redhat.com>** mq_perf_tests is free software: you can redistribute it and/or modify* it under the terms of the GNU General Public License as published by* the Free Software Foundation, version 3.** mq_perf_tests is distributed in the hope that it will be useful,* but WITHOUT ANY WARRANTY; without even the implied warranty of* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the* GNU General Public License for more details.** For the full text of the license, see <http://www.gnu.org/licenses/>.** mq_perf_tests.c*   Tests various types of message queue workloads, concentrating on those*   situations that invole large message sizes, large message queue depths,*   or both, and reports back useful metrics about kernel message queue*   performance.', 'SPDX-License-Identifier: GPL-2.0-only']",GPL-3.0-only,,0.0,0.0
109,109,119,linux-master/tools/testing/selftests/mm/transhuge-stress.c,Unlicense,1728,[],"Stress test for transparent huge pages, memory compaction and migration.** Authors: Konstantin Khlebnikov <koct9i@gmail.com>** This is free and unencumbered software released into the public domain.
split transhuge page, keep last page","['Stress test for transparent huge pages, memory compaction and migration.** Authors: Konstantin Khlebnikov <koct9i@gmail.com>** This is free and unencumbered software released into the public domain.', 'SPDX-License-Identifier: GPL-2.0-only']",Unlicense,,0.0,0.0
110,110,120,linux-master/tools/testing/selftests/kmod/kmod.sh,copyleft-next-0.3.1 GPL-2.0-or-later Dual-license,2022,[],"This represents
TEST_ID:TEST_COUNT:ENABLED
Kselftest framework requirement - SKIP code is 4.
Alanis: ""Oh isn't it ironic?""
*:*}
*:*:}
-ne 3 ]; then
$1""
-eq 1 ]; then
-eq 2 ]; then
-eq 0 ]; then
 !/bin/bash SPDX-License-Identifier: GPL-2.0-or-later OR copyleft-next-0.3.1 Copyright (C) 2017 Luis R. Rodriguez <mcgrof@kernel.org>
 This is a stress test script for kmod, the kernel module loader. It uses test_kmod which exposes a series of knobs for the API for us so we can tweak each test in userspace rather than in kernelspace.
 The way kmod works is it uses the kernel's usermode helper API to eventually call /sbin/modprobe. It has a limit of the number of concurrent calls possible. The kernel interface to load modules is request_module(), however mount uses get_fs_type(). Both behave slightly differently, but the differences are important enough to test each call separately. For this reason test_kmod starts by providing tests for both calls.
 The test driver test_kmod assumes a series of defaults which you can override by exporting to your environment prior running this script. For instance this script assumes you do not have xfs loaded upon boot. If this is false, export DEFAULT_KMOD_FS=""ext4"" prior to running this script if the filesystem module you don't have loaded upon bootup is ext4 instead. Refer to allow_user_defaults() for a list of user override variables possible.
 You'll want at least 4 GiB of RAM to expect to run these tests without running out of memory on them. For other requirements refer to test_reqs()
 TEST_ID: is the test id number TEST_COUNT: number of times we should run the test ENABLED: 1 if enabled, 0 otherwise
 Once these are enabled please leave them as-is. Write your own test, we have tons of space.
 kmod 19 has a bad bug where it returns 0 when modprobe gets called *even* if the module was not loaded due to some bad heuristics. For details see:
 A work around is possible in-kernel but its rather complex.
 kmod calls modprobe and upon of a module not found modprobe returns just 1... However in the kernel we *sometimes* see 256...
 For special characters use printf directly, refer to kmod_test_0001
 This causes the kernel to not even try executing modprobe.  The error code is still -ENOENT like when modprobe doesn't exist, so we can't easily test for the exact difference.  But this still is a useful test since there was a bug where request_module() returned 0 in this case.","['SPDX-License-Identifier: GPL-2.0-or-later', 'SPDX-License-Identifier: GPL-2.0-or-later']",copyleft-next-0.3.1 GPL-2.0-or-later Dual-license,,0.0,0.0
111,111,121,pytorch-main/torch/distributed/pipeline/sync/__init__.py,BSD,1626,[],"Copyright 2019 Kakao Brain
Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.
A Pipe implementation in PyTorch.","['This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.', 'Copyright SPDX-License-Identifier: GPL-2.0-only']",BSD,,0.0,0.0
112,112,122,pytorch-main/torch/distributed/pipeline/sync/skip/namespace.py,BSD,1634,[],"Copyright 2019 Kakao Brain
Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.
 Namespaces should support ordering, since SkipLayout will sort tuples including a namespace. But actual order between namespaces is not important. That's why they are ordered by version 4 UUID which generates random numbers.
 'None' is the default namespace, which means that 'isinstance(None, Namespace)' is 'True'.
Provides isolated namespace of skip tensors.
Namespace for isolating skip tensors used by :meth:`isolate()     <torchpipe.skip.skippable.Skippable.isolate>`.","['This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.', 'Copyright 2019 Kakao Brain', 'Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'SPDX-License-Identifier: GPL-2.0-only']",BSD,,0.0,0.0
113,113,123,pytorch-main/setup.py,BSD,5,[],"Welcome to the PyTorch setup.py.
Environment variables you are probably interested in:
Environment variables for feature toggles:
Parameters parsed from environment
rebuild is gone, make it build
Make distutils respect --quiet too
Constant known variables used throughout this file
CMAKE: full path to python library
Fix virtualenv builds
Version, create_version_file, and package_name
If none of the submodule folders exists, try to initialize them
Create the dirs involved in new_path if they don't exist
Copy the files from the orig location to the new location
copytree fails if the tree exists already, so remove it.
all the work we need to do _before_ setup runs
Building dependent libraries
Copy libiomp5.dylib inside the wheel package on OS X
Parse libtorch_cpu load commands
Copy libiomp5 from rpath locations
It's an old-style class in Python 2.7...
Copy the essential export library to compile C++ extensions.
Copy functorch extension
Need to create the proper LICENSE.txt for the wheel
( BEGIN NOT-CLEAN-FILES )?"")
Marker is found and stop reading .gitignore.
Ignore lines which begin with '#'.
Don't remove absolute paths from the system
CMakeCache.txt does not exist. Probably running ""python setup.py clean"" over a clean directory.
Configure compile flags
Cross-compile for M1
Declare extensions and package
Triton is only needed for CUDA or ROCm
post run, warnings, printed at the end to make them more visible
the list of runtime dependencies required by this built package
Triton is only available on Linux atm
Read in README.md for our long_description
PyPI package information.
 DEBUG build with -O0 and -g (debug symbols)
 REL_WITH_DEB_INFO build with optimizations and -g (debug symbols)
 USE_CUSTOM_DEBINFO=""path/to/file1.cpp;path/to/file2.cpp"" build with debug info only for specified files
 MAX_JOBS maximum number of compile jobs we should use to compile your code
 USE_CUDA=0 disables CUDA build
 CFLAGS flags to apply to both C and C++ files to be compiled (a quirk of setup.py which we have faithfully adhered to in our build system is that CFLAGS also applies to C++ files (unless CXXFLAGS is set), in contrast to the default behavior of autogoo and cmake build systems.)
 CC the C/C++ compiler to use
 DEBUG_CUDA=1 if used in conjunction with DEBUG or REL_WITH_DEB_INFO, will also build CUDA kernels with -lineinfo --source-in-ptx.  Note that on CUDA 12 this may cause nvcc to OOM, so this is disabled by default.
 USE_CUDNN=0 disables the cuDNN build
 USE_CUSPARSELT=0 disables the cuSPARSELt build
 USE_FBGEMM=0 disables the FBGEMM build
 USE_KINETO=0 disables usage of libkineto library for profiling
 USE_NUMPY=0 disables the NumPy build
 BUILD_TEST=0 disables the test build
 USE_MKLDNN=0 disables use of MKLDNN
 USE_MKLDNN_ACL enables use of Compute Library backend for MKLDNN on Arm; USE_MKLDNN must be explicitly enabled.
 MKLDNN_CPU_RUNTIME MKL-DNN threading mode: TBB or OMP (default)
 USE_STATIC_MKL Prefer to link with MKL statically - Unix only USE_ITT=0 disable use of Intel(R) VTune Profiler's ITT functionality
 USE_NNPACK=0 disables NNPACK build
 USE_QNNPACK=0 disables QNNPACK build (quantized 8-bit operators)
 USE_DISTRIBUTED=0 disables distributed (c10d, gloo, mpi, etc.) build
 USE_TENSORPIPE=0 disables distributed Tensorpipe backend build
 USE_GLOO=0 disables distributed gloo backend build
 USE_MPI=0 disables distributed MPI backend build
 USE_SYSTEM_NCCL=0 disables use of system-wide nccl (we will use our submoduled copy in third_party/nccl)
 BUILD_CAFFE2_OPS=0 disable Caffe2 operators build
 BUILD_CAFFE2=0 disable Caffe2 build
 USE_IBVERBS toggle features related to distributed support
 USE_OPENCV enables use of OpenCV for additional operators
 USE_OPENMP=0 disables use of OpenMP for parallelization
 USE_FFMPEG enables use of ffmpeg for additional operators
 USE_FLASH_ATTENTION=0 disables building flash attention for scaled dot product attention
 USE_MEM_EFF_ATTENTION=0 disables building memory efficient attention for scaled dot product attention
 USE_LEVELDB enables use of LevelDB for storage
 USE_LMDB enables use of LMDB for storage
 BUILD_BINARY enables the additional binaries/ build
 ATEN_AVX512_256=TRUE ATen AVX2 kernels can use 32 ymm registers, instead of the default 16. This option can be used if AVX512 doesn't perform well on a machine. The FBGEMM library also uses AVX512_256 kernels on Xeon D processors, but it also has some (optimized) assembly code.
 PYTORCH_BUILD_VERSION PYTORCH_BUILD_NUMBER specify the version of PyTorch, rather than the hard-coded version in this file; used when we're building binaries for distribution
 TORCH_CUDA_ARCH_LIST specify which CUDA architectures to build for. ie `TORCH_CUDA_ARCH_LIST=""6.0;7.0""` These are not CUDA versions, instead, they specify what classes of NVIDIA hardware we should generate PTX for.
 PYTORCH_ROCM_ARCH specify which AMD GPU targets to build for. ie `PYTORCH_ROCM_ARCH=""gfx900;gfx906""`
 ONNX_NAMESPACE specify a namespace for ONNX built here rather than the hard-coded one in this file; needed to build with other frameworks that share ONNX.
 BLAS BLAS to be used by Caffe2. Can be MKL, Eigen, ATLAS, FlexiBLAS, or OpenBLAS. If set then the build will fail if the requested BLAS is not found, otherwise the BLAS will be chosen based on what is found on your system.
 MKL_THREADING MKL threading mode: SEQ, TBB or OMP (default)
 USE_REDIS Whether to use Redis for distributed workflows (Linux only)
 USE_ZSTD Enables use of ZSTD, if the libraries are found
 Environment variables we respect (these environment variables are conventional and are often understood/set by other software.)
 CUDA_HOME (Linux/OS X) CUDA_PATH (Windows) specify where CUDA is installed; usually /usr/local/cuda or /usr/local/cuda-x.y CUDAHOSTCXX specify a different compiler than the system one to use as the CUDA host compiler for nvcc.
 CUDA_NVCC_EXECUTABLE Specify a NVCC to use. This is used in our CI to point to a cached nvcc
 CUDNN_LIB_DIR CUDNN_INCLUDE_DIR CUDNN_LIBRARY specify where cuDNN is installed
 MIOPEN_LIB_DIR MIOPEN_INCLUDE_DIR MIOPEN_LIBRARY specify where MIOpen is installed
 NCCL_ROOT NCCL_LIB_DIR NCCL_INCLUDE_DIR specify where nccl is installed
 NVTOOLSEXT_PATH (Windows only) specify where nvtoolsext is installed
 ACL_ROOT_DIR specify where Compute Library is installed
 LIBRARY_PATH LD_LIBRARY_PATH we will search for libraries in these paths
 ATEN_THREADING ATen parallel backend to use for intra- and inter-op parallelism possible values: OMP - use OpenMP for intra-op and native backend for inter-op tasks NATIVE - use native thread pool for both intra- and inter-op tasks TBB - using TBB for intra- and native thread pool for inter-op parallelism
 USE_TBB enable TBB support
 USE_SYSTEM_TBB Use system-provided Intel TBB.
 USE_SYSTEM_LIBS (work in progress) Use system-provided libraries to satisfy the build dependencies. When turned on, the following cmake variables will be toggled as well: USE_SYSTEM_CPUINFO=ON USE_SYSTEM_SLEEF=ON BUILD_CUSTOM_PROTOBUF=OFF
 USE_MIMALLOC Static link mimalloc into C10, and use mimalloc in alloc_cpu & alloc_free. By default, It is only enabled on Windows.
 see if the user passed a quiet flag to setup.py arguments and respect that in our parts of the build
 Stop once cmake terminates. Leave users a chance to adjust build options.
 Windows has very bad support for symbolic links. Instead of using symlinks, we're going to copy files over
 (new_path, orig_path) Directories are OK and are recursively mirrored.
 Use copies instead of symbolic files. Windows has very poor support for them.
 Report build options. This is run after the build completes so # `CMakeCache.txt` exists and we can get an accurate report on what is used and what is not.
 Do not use clang to compile extensions if `-fstack-clash-protection` is defined in system CFLAGS
 Create ""torch/lib"" directory if not exists. (It is not created yet in ""develop"" mode.)
 The caffe2 extensions are created in tmp_install/lib/pythonM.m/site-packages/caffe2/python/ and need to be copied to build/lib.linux.... , which will be a platform dependent build folder created by the ""build"" command of setuptools. Only the contents of this folder are installed in the ""install"" command by default. We only make this copy for Caffe2's pybind extensions
 cquery does not like c++ compiles that start with gcc. It forgets to include the c++ header directories. We can work around this by replacing the gcc calls that python setup.py generates with g++ calls instead
 This is useful when wheel is not installed and bdist_wheel is not specified on the command line. If it _is_ specified, parsing the command line will fail before wheel_concatenate is needed
 /NODEFAULTLIB makes sure we only link to DLL runtime and matches the flags set for protobuf and ONNX
 /MD links against DLL runtime and matches the flags set for protobuf and ONNX /EHsc is about standard C++ exception handling
 Python 2.6 requires -fno-strict-aliasing, see http://legacy.python.org/dev/peps/pep-3123/ We also depend on it in our code (even Python 3).
 pypi cuda package that requires installation of cuda runtime, cudnn and cublas should be included in all wheels uploaded to pypi
 These extensions are built by cmake and copied manually in build_extensions() inside the build_ext implementation
 NB: If the installation requirments list already includes triton dependency, there is no need to add it one more time as an extra dependency. In nightly or when release PyTorch, that is done by setting PYTORCH_EXTRA_INSTALL_REQUIREMENTS environment variable on pytorch/builder
 Parse the command line and check the arguments before we proceed with building deps and setup. We need to set values so `--help` works.
 Recursive glob doesn't work in setup.py, https://github.com/pypa/setuptools/issues/1806 To make this robust we should replace it with some code that returns a list of everything under packaged/
missing_pydep = """""" Missing build dependency: Unable to `import {importname}`. Please install it via `conda install {module}` or `pip install {module}`
Merge LICENSE and LICENSES_BUNDLED.txt as a context manager      LICENSE is the main PyTorch license, LICENSES_BUNDLED.txt is auto-generated     from all the licenses found in ./third_party/. We concatenate them so there     is a single license file in the sdist and wheels with all of the necessary     licensing info.
Concatenate files
Restore content of f1
check submodules on sdist to prevent incomplete tarballs
r""""""Configures extension build options according to system environment and user's choice.      Returns:       The input to parameters ext_modules, cmdclass, packages, and entry_points as required in setuptools.setup.
Add triton package as a dependency when it's needed
build_update_message = """"""     It is no longer necessary to use the 'build' or 'rebuild' targets      To install:       $ python setup.py install     To develop locally:       $ python setup.py develop     To force cmake to re-generate native build files (off by default):       $ python setup.py develop --cmake","['Need to create the proper LICENSE.txt for the wheel', 'SPDX-License-Identifier: GPL-2.0-only', 'Merge LICENSE and LICENSES_BUNDLED.txt as a context manager      LICENSE is the main PyTorch license, LICENSES_BUNDLED.txt is auto-generated     from all the licenses found in ./third_party/. We concatenate them so there     is a single license file in the sdist and wheels with all of the necessary     licensing info.']",BSD,,0.0,1.4210854715202004e-14
114,114,124,pytorch-main/torch/distributed/optim/zero_redundancy_optimizer.py,BSD,1646,[],"Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
Credits:  classy_vision/generic/distributed_util.py
Send the object
Receive the object
DDP guarantees all parameters in the bucket have the same device
Modified per bucket reconstruction
Group Ranks
Modified per iteration
Used by `hook_with_zero_step()`
xdoctest: +SKIP
Perform type and assumption checks on the input parameters
Internal data structures (`_cache` indicates lazily evaluated)
Default device for collective communication and buckets
Force a re-partitioning of the parameters
Update the bucketing strategy accordingly
Pull the sharded state from all ranks and store them in rank order
Directly append own optimizer state
Receive the optimizer state from the source rank
Send the optimizer state to the target rank
Partition the parameters optimizing for uniformity
Sort the parameters by size (largest first)
Greedily add the parameter to rank with smallest size so far
Apply the constructed partition of the parameter group
Partition the parameters according to `params_per_rank`
Apply the passed-in partition of the parameter group
Define the assignment threshold to approximate uniformity
type: ignore[operator]
Assign each DDP bucket entirely to a single rank
Check if the model trainability has changed
Run the optimizer step on this shard only
Perform the local optimizer step
Sync all of the updated parameter shards across the ranks
Clear any state irrelevant to this rank
Load the parameter state to the local optimizer
Force zero-dimensional tensors (like Adam ""step"") on CPU
Sync the input state with the exposed and local optimizer states
Update the global parameter state, if any
Sort the parameters in the state
Sync all attributes except the parameters
type: ignore[assignment]
assumes all same dtype
Create a dummy bucket if there are no parameters
Construct the bucket (assuming all dense and same dtype)
type: ignore[arg-type]
assumes all same dtype
Construct the bucket tensor (assuming all dense and same dtype)
Ensure that `self._all_params` contains a list of all parameters
`all_params` contains parameter groups (not parameters)
`overlap_with_ddp=True` requires a local functional optimizer
Try to pass `_allow_empty_param_list=True` to avoid erroring
type: ignore[no-redef]
Log information about the DDP and ZeRO bucketing
type: ignore[attr-defined]
Already a functional optimizer
 This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.
 NOTE: The parent constructor uses `add_param_group()` which is partially overloaded in ZeroRedundancyOptimizer, so we use the `initialized` flag to dissociate the behaviour of `add_param_group()` between the parent and child.
 Now, all parameters are held in both `self._all_params` and `self.param_groups`
 If `overlap_with_ddp=True`, local optimizer initialization is delayed to run time after the necessary information has been collected
 `self._buckets` is used if `parameters_as_bucket_view=True`, in which case parameter data is flattened into contiguous bucket tensors
 Optional consolidated optimizer state, only populated if this rank is the target in `consolidate_state_dict()`
 NOTE: The rest of the method assumes that the call to the parent's `add_param_group()` appends the new parameter group and preserves the previous parameter-group ordering
 NOTE: All parameters in the old parameter groups should be assigned to the same ranks so that the local optimizers do not need to be reinitialized
 Add the parameters assigned to this rank from the new parameter group to the local optimizer, if any
 Sync the exposed `param_groups` attributes to the local optimizer in case they have been updated
 NOTE: We wastefully use `broadcast()` (e.g. instead of `gather()`) due to compatibility issues with NCCL backend; a possible follow-up is to move all sharded state management to RPC RRef
 Consolidate all local `state_dict`s on this rank, storing on CPU to save GPU memory
 Discard the received object; `broadcast()` is used for compatibility reasons
 Assign each DDP bucket to possibly multiple ranks Specifically, sort the DDP buckets by increasing size, and for each bucket, iteratively assign the maximal unassigned subset with size less than `threshold` to the rank with the least total size so far -- each such assignment is represented by a `_DDPBucketAssignment` instance and only contains parameters from a single DDP bucket
 Include up to but not including the parameter that exceeded the threshold
 Assign the remainder of the bucket so that no assignment spans across two buckets
 Sync the exposed `param_groups` attributes to the local optimizer in case they have been updated
 Sync any updated attributes in the local optimizer to the exposed `param_groups`
 Get the possibly-stale global optimizer state that uses global parameter indexing
 Update the global optimizer state with local state information, factoring in the translation from local to global indexing
 `local_param_group` stores local indices, while `global_param_group` stores the tensors directly
 `self._buckets[i][j]` are the parameters stored on device i and assigned to rank j
 Clone in case the parameter was previously part of a bucket to avoid the data from being destroyed
 Functional optimizers only support a single parameter group and require passing in the parameters as a list
 NOTE: Passing `param_groups` into the local optimizer constructor bypasses the empty parameter list check type: ignore[no-redef]
 TODO: Manually add `self.param_groups` if using a functional optimizer; remove this if/when the functional optimizers support multiple parameter groups
 Using a functional optimizer is only supported when `overlap_with_ddp=True`
 Translate the passed-in optimizer class to its functional equivalent if `overlap_with_ddp=True`
r""""""Zero Redundancy Optimizer.
r""""""     Recursively searches lists, tuples, dicts and copies tensors to device if possible.      Non-tensor values are passed as-is in the result.      .. note:  These are all copies, so if there are two objects that reference     the same object, then after this call, there will be two different objects     referenced on the device.
r""""""Return if a parameter is trainable, where trainability is equivalent to requiring a gradient.
r""""""     Broadcasts an object to the given group.      It will be sending the object if called from the source rank and receiving     the object otherwise.      Arguments:         obj: object to broadcast; only used if called on the source rank.         src_rank (int): source rank.         group (``ProcessGroup``, optional): group used for the broadcast             (default: ``dist.group.WORLD``).         device (``torch.device``, optional): device to send from or receive             to (default: ``torch.device(""cpu"")``).      Returns:         The broadcasted object.
Perform an optimizer step.          This step updates the joined process's shard of         the parameters and broadcasts those parameters.
r""""""     Represent a :class:`DistributedDataParallel` bucket assignment.      This means that a (possibly non-strict) subset of the parameters corresponding to     a DDP bucket assigned to a rank to update.      Attributes:         bucket_index (int): index of the bucket determined by the DDP gradient             bucket all-reduce order.         parameters (List[torch.Tensor]): model parameters in the bucket             assigned to this rank.         offset (int): offset into the :class:`GradBucket` 's :meth:`parameters`             giving the index of the first element in the passed-in             ``parameters``; this equivalently indexes into the             :class:`GradBucket` 's :meth:`gradients`.         device (torch.device): device on which the parameters are stored.         tensor (torch.Tensor): flattened tensor giving the data of the             parameter subset assigned to the rank.
r""""""     Define possible statuses that :class:`ZeroRedundancyOptimizer` can be in when overlapping with :class:`DistributedDataParallel`.      Attributes:         ``UNINITIALIZED``: The ZeRO instance is effectively uninitialized and             is waiting for DDP to finalize its bucketing.         ``DDP_HAS_REBUILT_BUCKETS``: DDP has rebuilt its buckets, meaning that             its bucketing is finalized. The ZeRO instance can now collect the             necessary information about the DDP bucketing.         ``INITIALIZED``: The ZeRO instance is fully initialized and can now             optimize parameters.
r""""""     Information needed by :class:`ZeroRedundancyOptimizer` to overlap with :class:`DistributedDataParallel`.      Arguments:         world_size (int): world size of the process group being used.      Attributes:         shard_buckets (bool): if ``True``, then the assignment of each             :class:`DistributedDataParallel` bucket is partitioned across             possibly multiple :class:`ZeroRedundancyOptimizer` instances (i.e.             across possibly multiple ranks) to approximate uniformity following             a threshold given by the total parameter size divided by the world             size; if ``False``, then each bucket is wholly assigned to a single             :class:`ZeroRedundancyOptimizer` instance (i.e. to a single rank);             this should be set to the value passed into the hook constructor.         status (_OverlapStatus): current status; see :class:`_OverlapStatus`             for more information.         params_per_bucket (List[List[torch.Tensor]]): ``params_per_bucket[i]``             gives the model parameters in the ``i``th bucket.         params_per_rank (List[List[torch.Tensor]]): ``params_per_rank[i]``             gives the model parameters assigned to the ``i``th rank, where the             parameters are grouped by increasing bucket indices.         offsets (Dict[int, int]): maps from bucket index to the offset in             ``self.params_per_rank[rank]`` giving the index of the first             parameter in that bucket, where ``rank`` is this process's own             rank; the keys of this :class:`dict` are the bucket indices             assigned to this rank.         num_bucket_assignments (int): total number of bucket assignments across             all ranks; this is equal to the number of             :class:`DistributedDataParallel` gradient buckets if             ``shard_buckets=False`` and possibly greater otherwise.         total_size (int, optional): total size of all buckets (i.e. sum of             ``param.numel()`` for all ``param`` across all buckets) if             ``shard_buckets=True``; otherwise, ``None``.         broadcast_handles (List[Work]): :class:`list` of async work handles for             the parameter broadcasts.         bucket_index_to_future (Dict[int, torch.futures.Future]):             :class:`dict` mapping bucket index to the corresponding all-reduce             future.         bucket_index_to_bucket (Dict[int, dist.GradBucket]): :class:`dict`             mapping bucket index to the corresponding bucket.         bucket_indices_seen (List[int]): :class:`list` of the bucket indices             seen on this iteration.
r""""""         Wait for all parameter broadcasts.          This function should be called once all broadcasts have been scheduled,         meaning ``self.broadcast_handles`` is filled. This clears ``self.broadcast_handles``         in preparation for the next iteration.
r""""""         Clear the data structures that are modified per-iteration.          This function should be called at the end of an iteration.
r""""""     Wrap an arbitrary :class:`optim.Optimizer <torch.optim.Optimizer>` and shards its states across ranks in the group.      The sharing is done as described by ZeRO_.      The local optimizer instance in each rank is only     responsible for updating approximately ``1 / world_size`` parameters and     hence only needs to keep ``1 / world_size`` optimizer states. After     parameters are updated locally, each rank will broadcast its parameters to     all other peers to keep all model replicas in the same state.     ``ZeroRedundancyOptimizer`` can be used in conjunction with     :class:`torch.nn.parallel.DistributedDataParallel` to reduce per-rank peak     memory consumption.      ``ZeroRedundancyOptimizer`` uses a sorted-greedy algorithm to pack a number     of parameters at each rank. Each parameter belongs to a single rank and is     not divided among ranks. The partition is arbitrary and might not match the     the parameter registration or usage order.      Arguments:         params (``Iterable``): an ``Iterable`` of :class:`torch.Tensor` s             or :class:`dict` s giving all parameters, which will be sharded             across ranks.      Keyword Args:         optimizer_class (:class:`torch.nn.Optimizer`): the class of the local             optimizer.         process_group (``ProcessGroup``, optional): ``torch.distributed``             ``ProcessGroup`` (default: ``dist.group.WORLD`` initialized by             :meth:`torch.distributed.init_process_group`).         parameters_as_bucket_view (bool, optional): if ``True``, parameters are             packed into buckets to speed up communication, and ``param.data``             fields point to bucket views at different offsets; if ``False``,             each individual parameter is communicated separately, and each             ``params.data`` stays intact (default: ``False``).         overlap_with_ddp (bool, optional): if ``True``, :meth:`step` is             overlapped with :class:`DistributedDataParallel` 's gradient             synchronization; this requires (1) either a functional optimizer             for the ``optimizer_class`` argument or one with a functional             equivalent and (2) registering a DDP communication hook             constructed from one of the functions in ``ddp_zero_hook.py``;             parameters are packed into buckets matching those in             :class:`DistributedDataParallel`, meaning that the             ``parameters_as_bucket_view`` argument is ignored.             If ``False``, :meth:`step` runs disjointly after the backward pass             (per normal).             (default: ``False``)         **defaults: any trailing arguments, which are forwarded to the local             optimizer.      Example::          >>> # xdoctest: +SKIP         >>> import torch.nn as nn         >>> from torch.distributed.optim import ZeroRedundancyOptimizer         >>> from torch.nn.parallel import DistributedDataParallel as DDP         >>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])         >>> ddp = DDP(model, device_ids=[rank])         >>> opt = ZeroRedundancyOptimizer(         >>>     ddp.parameters(),         >>>     optimizer_class=torch.optim.Adam,         >>>     lr=0.01         >>> )         >>> ddp(inputs).sum().backward()         >>> opt.step()      .. warning::         Currently, ``ZeroRedundancyOptimizer`` requires that all of the         passed-in parameters are the same dense type.      .. warning::         If you pass ``overlap_with_ddp=True``, be wary of the following: Given         the way that overlapping :class:`DistributedDataParallel` with         :class:`ZeroRedundancyOptimizer` is currently implemented, the first         two or three training iterations do not perform parameter updates in         the optimizer step, depending on if ``static_graph=False`` or         ``static_graph=True``, respectively. This is because it needs         information about the gradient bucketing strategy used by         :class:`DistributedDataParallel`, which is not finalized until the         second forward pass if ``static_graph=False`` or until the third         forward pass if ``static_graph=True``. To adjust for this, one option         is to prepend dummy inputs.      .. warning:: ZeroRedundancyOptimizer is experimental and subject to change.      .. _ZeRO: https://arxiv.org/abs/1910.02054
r""""""Init.
r""""""Clear the cached data structures giving partition information.
r""""""         Add a parameter group to the :class:`Optimizer` 's ``param_groups``.          This can be useful when fine tuning a pre-trained network, as frozen         layers can be made trainable and added to the :class:`Optimizer` as         training progresses.          Arguments:             param_group (dict): specifies the parameters to be optimized and                 group-specific optimization options.          .. warning:: This method handles updating the shards on all partitions             but needs to be called on all ranks. Calling this on a subset of             the ranks will cause the training to hang because communication             primitives are called depending on the managed parameters and             expect all the ranks to participate on the same set of parameters.
r""""""         Consolidate a list of ``state_dict`` s (one per rank) on the target rank.          Arguments:             to (int): the rank that receives the optimizer states (default: 0).          Raises:             RuntimeError: if ``overlap_with_ddp=True`` and this method is                 called before this :class:`ZeroRedundancyOptimizer` instance                 has been fully initialized, which happens once                 :class:`DistributedDataParallel` gradient buckets have been                 rebuilt.          .. warning:: This needs to be called on all ranks.
r""""""         Verify ``params_per_rank`` for :meth:`_partition_parameters`.          The verification is done by checking that ``params_per_rank`` has length equal         to the world size and that it does not contain any parameters not passed into the         :class:`ZeroRedundancyOptimizer` constructor.          The parameters in ``params_per_rank`` being a strict subset of those         passed into the constructor is valid since some parameters may be         frozen.          Raises:             ValueError: if ``params_per_rank`` does not have length equal to                 the world size or if it contains a parameter that was not                 passed into the :class:`ZeroRedundancyOptimizer` constructor.
r""""""         Partition the parameter group ``param_group`` according to ``params_per_rank``.          The partition will modify the ``self._partition_parameters_cache``. This method should         only be used as a subroutine for :meth:`_partition_parameters`.          Arguments:             param_group (dict[str, Any]): a parameter group as normally defined                 in an optimizer state.             params_per_rank (list[list[torch.Tensor]]): a :class:`list` of                 length world size containing :class:`list` s of parameters to                 assign to each rank.
r""""""         Partitions parameters across distributed data parallel ranks.          Arguments:             params_per_rank (list[list[torch.Tensor]], optional): a                 :class:`list` of length world size containing :class:`list` s                 of parameters to assign to each rank; this provides a way to                 specify a partition manually.                 If ``None``, the parameters are partitioned according to an                 internal algorithm.                 (default: ``None``)          Returns:             A :class:`list` where each element of the list contains the             ``param_groups`` for a rank (which itself is a :class:`list` of             :class:`dict`); element 0 corresponds to rank 0, etc.; each rank             stores the ``param_groups`` for all ranks for the collective             communication in :meth:`step`.          Raises:             ValueError: see :meth:`_validate_params_per_rank`.             RuntimeError: if ``params_per_rank`` is not ``None`` and this                 :class:`ZeroRedundancyOptimizer` instance is using more than                 one parameter group.
r"""""":class:`dict` mapping parameters to their assigned data parallel rank in the partition.
r""""""         :class:`dict` mapping parameters to their indices in the global optimizer state.          NOTE: This assumes that the global optimizer state's indexing (in         ``state_dict``) follows a linear ordering over the parameter groups.
r""""""List mapping parameter indices in the global optimizer scheme to the actual params.
r""""""         Broadcast the shard of parameters from a given rank to all other ranks asynchronously.          Arguments:             rank (int): the source rank.          Returns:             A :class:`list` of async work handles for the ``broadcast()`` s             performed to synchronize the parameters.
r""""""         Sync all parameter shards across the ranks.          This rank sends its shard of the parameters to all other ranks and         receives a shard from each other rank. This is done using         ``broadcast()``. Parameters are sent bucket-by-bucket if         ``parameters_as_bucket_view=True``and sent parameter-by-parameter         otherwise.
r""""""         Return device parameters assigned per rank.          :class:`dict` mapping each device to a :class:`list` of the per-rank parameter         lists filtered to only include the parameters stored on that device.         Each per-rank parameter list gives the parameters assigned to that rank         to update.          This is used for constructing the parameter buckets if         ``parameters_as_bucket_view=True``.          Let ``dev_i`` denote the ``i``th device for this rank. Then:         ``dev_0`` maps to a list containing:             rank 0's assigned parameters stored on ``dev_0``,             rank 1's assigned parameters stored on ``dev_0``,             ...         ``dev_1`` maps to a list containing:             rank 0's assigned parameters stored on ``dev_1``,             rank 1's assigned parameters stored on ``dev_1``,             ...         ...
r""""""         Return ``values.index(min(values))``, except only uses one pass.          It also excludes any indices in ``disallowed_indices`` if provided.          Arguments:             values: (List[int]): :class:`list` of values.             disallowed_indices (Optional[Set[int]]): indices that are                 disallowed from being the returned min index.
r""""""         Assign ``bucket_params`` to the rank with the least size assigned so far and collects relevant information.          The model parameters given by ``bucket_params`` represents a (possibly non-strict)         subset of the parameters corresponding to a :class:`DistributedDataParallel` bucket.          Arguments:             bucket_index (int): index of the :class:`DistributedDataParallel`                 gradient bucket.             bucket_params (List[torch.Tensor]): subset of the parameters                 corresponding to the bucket to assign.             bucket_offset (int): offset giving the index of the first element                 in ``bucket_params`` in the bucket's full parameter list.             assigned_rank (int): group rank to assign to.             assigned_ranks_per_bucket (List[Set[int]]): :class:`set` of group ranks                 assigned to each bucket.
r""""""         Return DDP bucket parameters assigned per rank.          :class:`list` of length world size consisting of :class:`dict` s         mapping bucket indices to :class:`_DDPBucketAssignment` s for each         rank.
r""""""         Perform a single optimizer step without syncing parameters across ranks.          Arguments:             gradients (list[Optional[torch.Tensor]], optional): a :class:`list`                 of length equal to the number of parameters assigned to this                 rank containing gradient tensors or ``None`` as its elements;                 a ``None`` in the :class:`list` indicates that the                 corresponding parameter should not be updated.                 If the argument itself is ``None``, then all parameters are                 updated, and the gradients are assumed to be already populated.                 (default: ``None``)             closure (Callable): a closure that re-evaluates the model and                 returns the loss; optional for most optimizers and should be                 ``None`` if ``gradients`` is not ``None``; (default: ``None``)         Returns:             Optional loss depending on the underlying local optimizer.          .. warning::             The argument ``gradients`` should only be specified (i.e. not             ``None``) if ``overlap_with_ddp=True``, in which case             :class:`ZeroRedundancyOptimizer` wraps a functional optimizer.
r""""""         Perform a single optimizer step and syncs parameters across all ranks.          Arguments:             closure (Callable): a closure that re-evaluates the model and                 returns the loss; optional for most optimizers.         Returns:             Optional loss depending on the underlying local optimizer.          .. note: Any extra parameters are passed to the base optimizer as-is.
r""""""         Return the ZeRO join hook.          It enables training on uneven inputs by         shadowing the collective communications in the optimizer step.          Gradients must be properly set before this hook is called.          Arguments:             kwargs (dict): a :class:`dict` containing any keyword arguments                 to modify the behavior of the join hook at run time; all                 :class:`Joinable` instances sharing the same join context                 manager are forwarded the same value for ``kwargs``.          This hook does not support any keyword arguments; i.e. ``kwargs`` is         unused.
r""""""Return default device.
r""""""Return process group.
r""""""         Load the state pertaining to the given rank from the input ``state_dict``, updating the local optimizer as needed.          Arguments:             state_dict (dict): optimizer state; should be an object returned                 from a call to :meth:`state_dict`.          Raises:             RuntimeError: if ``overlap_with_ddp=True`` and this method is                 called before this :class:`ZeroRedundancyOptimizer` instance                 has been fully initialized, which happens once                 :class:`DistributedDataParallel` gradient buckets have been                 rebuilt.
r""""""         Return the last global optimizer state known to this rank.          .. warning:             If the state has not been consolidated to this rank, this raises a             runtime error, and even if it has, the state may not be up-to-date,             depending on when :meth:`consolidate_state_dict` was last called.          Raises:             RuntimeError: if ``overlap_with_ddp=True`` and this method is                 called before this :class:`ZeroRedundancyOptimizer` instance                 has been fully initialized, which happens once                 :class:`DistributedDataParallel` gradient buckets have been                 rebuilt; or if this method is called without a preceding call                 to :meth:`consolidate_state_dict`.
r""""""         Sync the attributes from the source parameter groups to the destination parameter groups.          Example attributes include learning rate or scheduler attributes. The         two parameter groups should have the same length (i.e. same number of         parameter groups).          Arguments:             src_param_groups (list[dict]): parameter groups giving the                 attribute settings to copy.             dst_param_groups (list[dict]): parameter groups giving the                 attribute settings to set.
r""""""         Build parameter buckets if ``parameters_as_bucket_view=True``.          For each device that stores this rank's parameters, there is a         bucket (represented as a tensor) containing all of the parameters on         that device that are assigned to a given rank in the parameter update         partition.          This method is called in the constructor and any time parameter         trainability is changed.          .. warning::             The current implementation assumes that all of the parameters in a             bucket are of the same dense type when allocating the bucket's             tensor.          .. warning::             If the model parameters are stored across more than one device,             then the storage partitioning must be the same across all             processes in order for parameter synchronization to work.
r""""""         Build the DDP bucket with parameters assigned to this rank.          For each DDP bucket with parameters assigned to this rank, flattens the         data of those parameters into a single tensor and saves the tensor to         the ``tensor`` attribute in the corresponding         :class:`_DDPBucketAssignment` instance stored in         ``self._bucket_assignments_per_rank``.          :class:`DistributedDataParallel` guarantees that the parameters         corresponding to a gradient bucket have the same device and the same         dtype.
r""""""         Verify the type of ``params`` and initializes ``self._all_params`` as a :class:`list` of all parameters.          The initializagtion will first make sure that provided ``params`` is valid.          Arguments:             params (Any): Candidate parameter list or parameter groups to verify.          Raises:             TypeError: ``params`` has an invalid type.             ValueError: ``params`` is empty.          Returns:             The persistent form of ``params`` to be passed into the parent             :class:`Optimizer` constructor -- i.e. returns ``params`` as a             :class:`list` to ensure that it can be iterated over again.
r""""""         Verify that all parameters are of the same dense type.          The method assumes that ``self._all_params`` has been initialized         and is non-empty.          Raises:             ValueError: ``params`` contains sparse parameters or parameters             of varying dense types.          NOTE: This method can be removed once support for sparse parameters         and varying parameter types is added.
r""""""Return a boolean mask indicating if each parameter is trainable (``requires_grad``) or not.
r""""""         Initialize this rank's local optimizer, responsible for its subset of the parameters.          The local optimizer is saved in ``self.optim``.
r""""""Perform a delayed initialization of the local optimizer and the supporting data structures.
r""""""         Return the single rank assigned to a :class:`DistributedDataParallel` gradient bucket.          Arguments:             bucket_index (int): index of the :class:`DistributedDataParallel`                 bucket for which to get the assigned rank.
r""""""         Check the delayed initialization depending on the value of ``overlap_with_ddp``.          The delayed initialization has occurred (see         :meth:`_init_zero_for_overlap`) if ``overlap_with_ddp=True``, and         raises a ``RuntimeError`` if not. This should preface methods that         should not be run before that delayed initialization.          Raises:             RuntimeError: if ``overlap_with_ddp=True`` and                 :meth:`_init_zero_for_overlap` has not been called.
r""""""         Return the optimizer constructor using validation and transformation depending on ``overlap_with_ddp``.          Returns:             - ``optimizer_class`` if ``overlap_with_ddp=False`` and                 ``optimizer_class`` is not a functional optimizer.             - ``optimizer_class`` if ``overlap_with_ddp=True`` and                 ``optimizer_class`` is already a functional optimizer.             - The functional equivalent of ``optimizer_class`` if                 ``overlap_with_ddp=True`` and ``optimizer_class`` is not                 already a functional optimizer (assuming the equivalent                 exists).          Raises:             ValueError:                  - if ``overlap_with_ddp=True`` but ``optimizer_class`` is                     neither a functional optimizer nor translatable to a                     functional optimizer.                 - if ``overlap_with_ddp=False`` and ``optimizer_class`` is a                     functional optimizer.","['SPDX-License-Identifier: GPL-2.0-only', 'Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.']",BSD,,0.0,0.0
115,115,125,pytorch-main/torch/distributed/pipeline/sync/checkpoint.py,BSD,1629,[],"Copyright 2019 Kakao Brain
Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
Gradients are only supported for float Tensors.
Use a tensor in the batch to tie together fork-join
pragma: no cover
type: ignore[override]
pragma: no cover
type: ignore[override]
pragma: no cover
Get the device for the inputs from a tensor
 This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.
 Types for shared memory between Checkpoint and Recompute. (output, input_leaf) (cpu_rng_state, gpu_rng_state)
 Protocol with __call__ instead of Callable can be used as an attribute type. See: https://github.com/python/mypy/issues/708#issuecomment-561735949
 Shared memory between Checkpoint and Recompute. 1-length deque is used for mutability and length limitation.
 Use a phony which requires grad to ensure that Checkpoint can be tracked by the autograd engine even when none of the input tensors require grad.
 batch[tensor_idx] is always requiring grad, because it has been passed checkpoint with a phony requiring grad.
Checkpointing with preceding recomputation.  PyTorch already provides the official checkpointing utilities in :mod:`torch.utils.checkpoint`. The official checkpointing combines recomputation and recursive backpropagation into one autograd function named ``CheckpointFunction``. Hence, the recomputation can be started only when the gradients arrive to the function. In Pipe, the recomputation needs to precede the gradient arrival to minimize the GPU idle time.  We solve this problem by introducing separate autograd functions named :class:`Recompute` and :class:`Checkpoint`. Each function represents recomputation and recursive backpropagation, respectively. We can manipulate the control flow in aspect of both the autograd engine and CUDA with a pair of the functions.  Specifically, we place CUDA stream synchronization between :class:`Recompute` and :class:`Checkpoint` to delay only :class:`Checkpoint` until the gradient is copied entirely.
Makes a checkpoint with a simple interface like     :func:`torch.utils.checkpoint.checkpoint`. It's only used to test or debug     :class:`Checkpoint` and :class:`Recompute` without boilerplate.
Generates a pair of :class:`Checkpoint` and :class:`Recompute`.
Returns a batch applied by :class:`Checkpoint`.
Applies :class:`Recompute` to the batch in place.
Makes :func:`is_checkpointing` return :data:`True` within a context.
Makes :func:`is_recomputing` return :data:`True` within a context.
Whether the current forward propagation is under checkpointing.      Returns:         bool: :data:`True` if it's under checkpointing.
Whether the current forward propagation is under checkpoint     recomputation. Use this to prevent duplicated side-effects at forward     propagation::          class Counter(nn.Module):             def __init__(self):                 super().__init__()                 self.counter = 0              def forward(self, input):                 if not is_recomputing():                     self.counter += 1                 return input      Returns:         bool: :data:`True` if it's under checkpoint recomputation.      .. seealso:: :ref:`Detecting Recomputation`
The common interface between the :class:`Checkpoint` and     :class:`Recompute` context.
:meth:`Checkpoint.forward` captures the current PyTorch's random number     generator states at CPU and GPU to reuse in :meth:`Recompute.backward`.      .. seealso:: :ref:`Referential Transparency`
:meth:`Recompute.backward` restores the random number generator states     captured by :func:`save_rng_states` within its context.      .. seealso:: :ref:`Referential Transparency`","['This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.', 'SPDX-License-Identifier: GPL-2.0-only', 'Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'Copyright 2019 Kakao Brain']",BSD,,0.0,0.0
116,116,126,pytorch-main/torch/distributed/pipeline/sync/dependency.py,BSD,1627,[],"Copyright 2019 Kakao Brain
Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
type: ignore[override]
type: ignore[override]
type: ignore[override]
type: ignore[override]
 This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.
Arbitrary dependency between two autograd lanes.
Branches out from an autograd lane of the given tensor.
Merges two autograd lanes.","['This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.', 'Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'Copyright 2019 Kakao Brain', 'SPDX-License-Identifier: GPL-2.0-only']",BSD,,0.0,0.0
117,117,127,pytorch-main/torch/distributed/pipeline/sync/skip/tracker.py,BSD,1631,[],"Copyright 2019 Kakao Brain
Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
Delete at [6. PortalOrange.forward]
Under recomputation, the portal already exists.
 This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.
 See [Tensor Life of Portal] at Portal.put_tensor() to understand the below tensor_life values. Here are the selected events which retrieve the tensor in portal:
 1. [x] blue() ... 6. [x]   PortalOrange.forward ... 8. [x]   PortalOrange.forward (recomputed) ... 11. [x] blue() (recomputed)
 Under checkpointing, the tensor used by the first PortalOrange should be alive in the portal. This tensor will be used again by the second PortalOrange during the recomputation. Delete at [8. PortalOrange.forward (recomputed)]
 The existing tensor life already became 0. It should be reset as 1 to delete the tensor after the second PortalBlue immediately. Delete at [11. blue() (recomputed)]
Tracks skip tensors on a thread.
Tracks saved skip tensors.      It will update the given micro-batch in place. This is because when it     manipulates the underlying skip tensors, the current micro-batch also has     to be connected with the skip tensors.      One thread has one skip tracker. Call :func:`current_skip_tracker` to get     the skip tracker on the current thread.
Tracks saved skip tensors through portals. The skip tensors will be     hidden in portals so that the autograd engine does not need to track them.      This tracker is only used when the training or evaluating module is wrapped     with :class:`torchpipe.Pipe`.
Saves the stashed skip tensor in a portal. The portal is then         connected to the given micro-batch with :class:`Join`.
Loads a skip tensor from the corresponding portal to pop. The given         micro-batch is connected to the portal with :class:`Fork`.
Copies the skip tensor in the corresponding portal. The given         micro-batch and the portal will be tied with :class:`Fork` and         :class:`Join`.
Registers the given skip tracker on the current thread within a     context::          with use_skip_tracker(my_skip_tracker):             ...
Gets the skip tracker on the current thread.","['This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.', 'Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'Copyright 2019 Kakao Brain', 'SPDX-License-Identifier: GPL-2.0-only']",BSD,,0.0,0.0
118,118,128,pytorch-main/torch/distributed/pipeline/sync/worker.py,BSD,1617,[],"Copyright 2019 Kakao Brain
Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
Spawn workers.
 This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.
 Queue is generic only in stubs. https://mypy.readthedocs.io/en/latest/common_issues.html#using-classes-that-are-generic-in-stubs-but-not-at-runtime
Multithreading in pipeline parallelism.
A task represents how to compute a micro-batch on a partition.      It consists of two parts: :meth:`compute` and :meth:`finalize`.     :meth:`compute` should be executed in worker threads concurrently.     :meth:`finalize` should be executed after when worker threads complete to     execute :meth:`compute`.      :meth:`compute` might be boosted by worker threads. Because it produces     several CUDA API calls by user code. In PyTorch, parallel CUDA API calls     are not serialized through GIL. So more than one CUDA API call can be     produced at the same time.
The main loop of a worker thread.
Spawns worker threads. A worker thread is bound to a device.","['This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.', 'Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'Copyright 2019 Kakao Brain', 'SPDX-License-Identifier: GPL-2.0-only']",BSD,,0.0,0.0
119,119,129,pytorch-main/torch/distributed/pipeline/sync/_balance/blockpartition.py,BSD,1641,[],"Copyright 2019 Kakao Brain
Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
Normalize the sequence in [0, 1].
max_size: M(P)
min_size: m(P)
 This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.
Implements ""Block Partitions of Sequences"" by Imre Bárány et al.  Paper: https://arxiv.org/pdf/1308.2452.pdf
Splits a sequence into several partitions to minimize variance for each     partition.      The result might not be optimal. However, it can be done only in O(kn³),     where k is the number of partitions and n is the length of the sequence.
(1) Fix p ∈ [k] with M(P) = bp. So Bp is a maximal block of P.
(2) If M(P) ≤ m(P) + 1, then stop.
(3) If M(P) > m(P) + 1, then let m(P) = bq for the q ∈ [k] which is             closest to p (ties broken arbitrarily). Thus Bq is a minimal block             of P. Let Bh be the block next to Bq between Bp and Bq. (Note that             Bh is a non-empty block: if it were, then m(P) = 0 and we should             have chosen Bh instead of Bq.)
So either p < q and then h = q−1 and we define P ∗ by moving                 the last element from Bh = Bq−1 to Bq,
or q < p, and then h = q + 1 and P ∗ is obtained by moving the                 first element of Bh = Bq+1 to Bq.
Set P = P ∗ . If p = h, then go to (1), else go to (2).","['SPDX-License-Identifier: GPL-2.0-only', 'Copyright 2015, Anton Blanchard, IBM Corp.', 'This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.']",BSD,,0.0,1.4210854715202004e-14
120,120,130,pytorch-main/torch/distributed/pipeline/sync/pipeline.py,BSD,1622,[],"Copyright 2019 Kakao Brain
Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
Gradients are only supported for float Tensors.
Gradients are only supported for float Tensors.
Disable checkpointing if in eval mode.
Synchronize with the copied input. ([1] in the diagram)
Determine whether checkpointing or not.
type: ignore[arg-type]
Compute tasks in parallel. ([2] in the diagram)
Hold the first exception.
Fail at the first exception.
 This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.
 Queue is generic only in stubs. https://mypy.readthedocs.io/en/latest/common_issues.html#using-classes-that-are-generic-in-stubs-but-not-at-runtime
 m: number of micro-batches n: number of partitions i: index of micro-batch j: index of partition k: clock number
 k (i,j) (i,j) (i,j) - ----- ----- ----- 0 (0,0) 1 (1,0) (0,1) 2 (2,0) (1,1) (0,2) 3       (2,1) (1,2) 4             (2,2)
 Ensure that batches[i-1] is executed after batches[i] in backpropagation by an explicit dependency.
 With checkpointing, the autograd graph looks like this diagram: ┌─────┸──────┐ │    Copy    │ └─────┰──────┘   (fence) ─ ─ ─ ╂ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┃          (compute) ┌─────┸──────┐ │    Wait    │ [1] Synchronize the current stream with the copy stream. └─────┰──────┘ ┌─────┸──────┐ │ Checkpoint │ [2] Compute a partition within checkpointing. └─────┰──────┘ ┌─────┸──────┐ │    Wait    │ [3] Synchronize the copy stream with the current stream. └─────┰──────┘ ┠ ─ ─ ─ ┐ ┃ ┌─────┴─────┐ ┃ │ Recompute │ [4] Schedule the recomputation at backpropagation. ┃ └─────┬─────┘ ┠ ─ ─ ─ ┘ ┃ ─ ─ ─ ╂ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┌─────┸──────┐   (fence) │    Copy    │ └─────┰──────┘
 The copy stream synchronizes to copy the output. ([3] in the diagram)
 Finalize tasks. If checkpointing is enabled, here the recomputation is scheduled at backpropagation. ([4] in the diagram)
The pipeline parallelism of Pipe.
Generates schedules for each clock cycle.
The pipeline parallelism for Pipe.
Runs pipeline parallelism.          It modifies the given batches in place.
Copies micro-batches after computation for the previous         micro-batches.
Runs tasks with synchronization to copy streams.","['This source code is licensed under the BSD license found in the LICENSE file in the root directory of this source tree.', 'Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'Copyright 2019 Kakao Brain', 'SPDX-License-Identifier: GPL-2.0-only']",BSD,,0.0,0.0
121,121,131,pytorch-main/README.md,BSD-possibility,10,[],"![PyTorch Logo](https://github.com/pytorch/pytorch/blob/main/docs/source/_static/img/pytorch-logo-dark.png)

--------------------------------------------------------------------------------

PyTorch is a Python package that provides two high-level features:
- Tensor computation (like NumPy) with strong GPU acceleration
- Deep neural networks built on a tape-based autograd system

You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.

Our trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).

<!-- toc -->

- [More About PyTorch](#more-about-pytorch)
  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)
  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)
  - [Python First](#python-first)
  - [Imperative Experiences](#imperative-experiences)
  - [Fast and Lean](#fast-and-lean)
  - [Extensions Without Pain](#extensions-without-pain)
- [Installation](#installation)
  - [Binaries](#binaries)
    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)
  - [From Source](#from-source)
    - [Prerequisites](#prerequisites)
    - [Install Dependencies](#install-dependencies)
    - [Get the PyTorch Source](#get-the-pytorch-source)
    - [Install PyTorch](#install-pytorch)
      - [Adjust Build Options (Optional)](#adjust-build-options-optional)
  - [Docker Image](#docker-image)
    - [Using pre-built images](#using-pre-built-images)
    - [Building the image yourself](#building-the-image-yourself)
  - [Building the Documentation](#building-the-documentation)
  - [Previous Versions](#previous-versions)
- [Getting Started](#getting-started)
- [Resources](#resources)
- [Communication](#communication)
- [Releases and Contributing](#releases-and-contributing)
- [The Team](#the-team)
- [License](#license)

<!-- tocstop -->

## More About PyTorch

[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)

At a granular level, PyTorch is a library that consists of the following components:

| Component | Description |
| ---- | --- |
| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |
| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |
| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |
| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |
| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |
| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |

Usually, PyTorch is used either as:

- A replacement for NumPy to use the power of GPUs.
- A deep learning research platform that provides maximum flexibility and speed.

Elaborating Further:

### A GPU-Ready Tensor Library

If you use NumPy, then you have used Tensors (a.k.a. ndarray).

![Tensor illustration](./docs/source/_static/img/tensor_illustration.png)

PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the
computation by a huge amount.

We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs
such as slicing, indexing, mathematical operations, linear algebra, reductions.
And they are fast!

### Dynamic Neural Networks: Tape-Based Autograd

PyTorch has a unique way of building neural networks: using and replaying a tape recorder.

Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.
One has to build a neural network and reuse the same structure again and again.
Changing the way the network behaves means that one has to start from scratch.

With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to
change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes
from several research papers on this topic, as well as current and past work such as
[torch-autograd](https://github.com/twitter/torch-autograd),
[autograd](https://github.com/HIPS/autograd),
[Chainer](https://chainer.org), etc.

While this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.
You get the best of speed and flexibility for your crazy research.

![Dynamic graph](https://github.com/pytorch/pytorch/blob/main/docs/source/_static/img/dynamic_graph.gif)

### Python First

PyTorch is not a Python binding into a monolithic C++ framework.
It is built to be deeply integrated into Python.
You can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.
You can write your new neural network layers in Python itself, using your favorite libraries
and use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).
Our goal is to not reinvent the wheel where appropriate.

### Imperative Experiences

PyTorch is designed to be intuitive, linear in thought, and easy to use.
When you execute a line of code, it gets executed. There isn't an asynchronous view of the world.
When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.
The stack trace points to exactly where your code was defined.
We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.

### Fast and Lean

PyTorch has minimal framework overhead. We integrate acceleration libraries
such as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.
At the core, its CPU and GPU Tensor and neural network backends
are mature and have been tested for years.

Hence, PyTorch is quite fast — whether you run small or large neural networks.

The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.
We've written custom memory allocators for the GPU to make sure that
your deep learning models are maximally memory efficient.
This enables you to train bigger deep learning models than before.

### Extensions Without Pain

Writing new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward
and with minimal abstractions.

You can write new neural network layers in Python using the torch API
[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).

If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.
No wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).


## Installation

### Binaries
Commands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)


#### NVIDIA Jetson Platforms

Python wheels for NVIDIA's Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)

They require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.


### From Source

#### Prerequisites
If you are installing from source, you will need:
- Python 3.8 or later (for Linux, Python 3.8.1+ is needed)
- A compiler that fully supports C++17, such as clang or gcc (especially for aarch64, gcc 9.4.0 or newer is required)

We highly recommend installing an [Anaconda](https://www.anaconda.com/download) environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.

If you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:
- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)
- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v7 or above
- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA

Note: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/pdf/cuDNN-Support-Matrix.pdf) for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardware

If you want to disable CUDA support, export the environment variable `USE_CUDA=0`.
Other potentially useful environment variables may be found in `setup.py`.

If you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)

If you want to compile with ROCm support, install
- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation
- ROCm is currently supported only for Linux systems.

If you want to disable ROCm support, export the environment variable `USE_ROCM=0`.
Other potentially useful environment variables may be found in `setup.py`.

#### Install Dependencies

**Common**

```bash
conda install cmake ninja
# Run this command from the PyTorch directory after cloning the source code using the “Get the PyTorch Source“ section below
pip install -r requirements.txt
```

**On Linux**

```bash
conda install mkl mkl-include
# CUDA only: Add LAPACK support for the GPU if needed
conda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo

# (optional) If using torch.compile with inductor/triton, install the matching version of triton
# Run from the pytorch directory after cloning
make triton
```

**On MacOS**

```bash
# Add this package on intel x86 processor machines only
conda install mkl mkl-include
# Add these packages if torch.distributed is needed
conda install pkg-config libuv
```

**On Windows**

```bash
conda install mkl mkl-include
# Add these packages if torch.distributed is needed.
# Distributed package support on Windows is a prototype feature and is subject to changes.
conda install -c conda-forge libuv=1.39
```

#### Get the PyTorch Source
```bash
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
# if you are updating an existing checkout
git submodule sync
git submodule update --init --recursive
```

#### Install PyTorch
**On Linux**

If you would like to compile PyTorch with [new C++ ABI](https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html) enabled, then first run this command:
```bash
export _GLIBCXX_USE_CXX11_ABI=1
```

If you're compiling for AMD ROCm then first run this command:
```bash
# Only run this if you're compiling for ROCm
python tools/amd_build/build_amd.py
```

Install PyTorch
```bash
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
python setup.py develop
```

> _Aside:_ If you are using [Anaconda](https://www.anaconda.com/distribution/#download-section), you may experience an error caused by the linker:
>
> ```plaintext
> build/temp.linux-x86_64-3.7/torch/csrc/stub.o: file not recognized: file format not recognized
> collect2: error: ld returned 1 exit status
> error: command 'g++' failed with exit status 1
> ```
>
> This is caused by `ld` from the Conda environment shadowing the system `ld`. You should use a newer version of Python that fixes this issue. The recommended Python version is 3.8.1+.

**On macOS**

```bash
python3 setup.py develop
```

**On Windows**

Choose Correct Visual Studio Version.

PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,
Professional, or Community Editions. You can also install the build tools from
https://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools *do not*
come with Visual Studio Code by default.

If you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)

**CPU-only builds**

In this mode PyTorch computations will run on your CPU, not your GPU

```cmd
conda activate
python setup.py develop
```

Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you'll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.

**CUDA based build**

In this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching

[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.
NVTX is a part of CUDA distributive, where it is called ""Nsight Compute"". To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.
Make sure that CUDA with Nsight Compute is installed after Visual Studio.

Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.
<br/> If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.

Additional libraries such as
[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.

You can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations


```cmd
cmd

:: Set the environment variables after you have downloaded and unzipped the mkl package,
:: else CMake would throw an error as `Could NOT find OpenMP`.
set CMAKE_INCLUDE_PATH={Your directory}\mkl\include
set LIB={Your directory}\mkl\lib;%LIB%

:: Read the content in the previous section carefully before you proceed.
:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.
:: ""Visual Studio 2019 Developer Command Prompt"" will be run automatically.
:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f ""usebackq tokens=*"" %i in (`""%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe"" -version [15^,17^) -products * -latest -property installationPath`) do call ""%i\VC\Auxiliary\Build\vcvarsall.bat"" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%

:: [Optional] If you want to override the CUDA host compiler
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe

python setup.py develop

```

##### Adjust Build Options (Optional)

You can adjust the configuration of cmake variables optionally (without building first), by doing
the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done
with such a step.

On Linux
```bash
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
python setup.py build --cmake-only
ccmake build  # or cmake-gui build
```

On macOS
```bash
export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-""$(dirname $(which conda))/../""}
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # or cmake-gui build
```

### Docker Image

#### Using pre-built images

You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+

```bash
docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest
```

Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.
for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you
should increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.

#### Building the image yourself

**NOTE:** Must be built with a docker version > 18.06

The `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.
You can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it
unset to use the default.

```bash
make -f docker.Makefile
# images are tagged as docker.io/${your_docker_username}/pytorch
```

You can also pass the `CMAKE_VARS=""...""` environment variable to specify additional CMake variables to be passed to CMake during the build.
See [setup.py](./setup.py) for the list of available variables.

```bash
CMAKE_VARS=""BUILD_CAFFE2=ON BUILD_CAFFE2_OPS=ON"" make -f docker.Makefile
```

### Building the Documentation

To build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org) and the
readthedocs theme.

```bash
cd docs/
pip install -r requirements.txt
```
You can then build the documentation by running `make <format>` from the
`docs/` folder. Run `make` to get a list of all available output formats.

If you get a katex error run `npm install katex`.  If it persists, try
`npm install -g katex`

> Note: if you installed `nodejs` with a different package manager (e.g.,
`conda`) then `npm` will probably install a version of `katex` that is not
compatible with your version of `nodejs` and doc builds will fail.
A combination of versions that is known to work is `node@6.13.1` and
`katex@0.13.18`. To install the latter with `npm` you can run
```npm install -g katex@0.13.18```

### Previous Versions

Installation instructions and binaries for previous PyTorch versions may be found
on [our website](https://pytorch.org/previous-versions).


## Getting Started

Three-pointers to get you started:
- [Tutorials: get you started with understanding and using PyTorch](https://pytorch.org/tutorials/)
- [Examples: easy to understand PyTorch code across all domains](https://github.com/pytorch/examples)
- [The API Reference](https://pytorch.org/docs/)
- [Glossary](https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md)

## Resources

* [PyTorch.org](https://pytorch.org/)
* [PyTorch Tutorials](https://pytorch.org/tutorials/)
* [PyTorch Examples](https://github.com/pytorch/examples)
* [PyTorch Models](https://pytorch.org/hub/)
* [Intro to Deep Learning with PyTorch from Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)
* [Intro to Machine Learning with PyTorch from Udacity](https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229)
* [Deep Neural Networks with PyTorch from Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)
* [PyTorch Twitter](https://twitter.com/PyTorch)
* [PyTorch Blog](https://pytorch.org/blog/)
* [PyTorch YouTube](https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw)

## Communication
* Forums: Discuss implementations, research, etc. https://discuss.pytorch.org
* GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.
* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org). If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1
* Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv
* Facebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorch
* For brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)

## Releases and Contributing

Typically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).

We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.

If you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us.
Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.

To learn more about making a contribution to Pytorch, please see our [Contribution page](CONTRIBUTING.md). For more information about PyTorch releases, see [Release page](RELEASE.md).

## The Team

PyTorch is a community-driven project with several skillful engineers and researchers contributing to it.

PyTorch is currently maintained by [Soumith Chintala](http://soumith.ch), [Gregory Chanan](https://github.com/gchanan), [Dmytro Dzhulgakov](https://github.com/dzhulgakov), [Edward Yang](https://github.com/ezyang), and [Nikita Shulga](https://github.com/malfet) with major contributions coming from hundreds of talented individuals in various forms and means.
A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito.

Note: This project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch) with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.

## License

PyTorch has a BSD-style license, as found in the [LICENSE](LICENSE) file.
","[""`\ndef filter_license_lines(lines):\n    license_relevant_lines = []\n    for line in lines:\n        text = line[3]\n        if 'Copyright' in text or 'License' in text or 'SPDX-License-Identifier' in text:\n            license_relevant_lines.append(text)\n    return license_relevant_lines\n`""]",BSD-possibility,,0.0,0.0
122,122,132,pytorch-main/NOTICE,Apache-2.0 BSD-3-Clause BSL-1.0,15,[],"=======================================================================
Software under third_party
=======================================================================
Software libraries under third_party are provided as github submodule
links, and their content is not part of the Caffe2 codebase. Their
licences can be found under the respective software repositories.

=======================================================================
Earlier BSD License
=======================================================================
Early development of Caffe2 in 2015 and early 2016 is licensed under the
BSD license. The license is attached below:

All contributions by Facebook:
Copyright (c) 2016 Facebook Inc.

All contributions by Google:
Copyright (c) 2015 Google Inc.
All rights reserved.

All contributions by Yangqing Jia:
Copyright (c) 2015 Yangqing Jia
All rights reserved.

All contributions by Kakao Brain:
Copyright 2019-2020 Kakao Brain

All other contributions:
Copyright(c) 2015, 2016 the respective contributors
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


=======================================================================
Caffe's BSD License
=======================================================================
Some parts of the caffe2 code is derived from the original Caffe code, which is
created by Yangqing Jia and is now a BSD-licensed open-source project. The Caffe
license is as follows:

COPYRIGHT

All contributions by the University of California:
Copyright (c) 2014, The Regents of the University of California (Regents)
All rights reserved.

All other contributions:
Copyright (c) 2014, the respective contributors
All rights reserved.

Caffe uses a shared copyright model: each contributor holds copyright over
their contributions to Caffe. The project versioning records all such
contribution and copyright details. If a contributor wants to further mark
their specific copyright on a particular contribution, they should indicate
their copyright solely in the commit message of the change when it is
committed.

LICENSE

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

CONTRIBUTION AGREEMENT

By contributing to the BVLC/caffe repository through pull-request, comment,
or otherwise, the contributor releases their content to the
license and copyright terms herein.

=======================================================================
Caffe2's Apache License
=======================================================================

This repo contains Caffe2 code, which was previously licensed under
Apache License Version 2.0:

                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      ""License"" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      ""Licensor"" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      ""Legal Entity"" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      ""control"" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      ""You"" (or ""Your"") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      ""Source"" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      ""Object"" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      ""Work"" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      ""Derivative Works"" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      ""Contribution"" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, ""submitted""
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as ""Not a Contribution.""

      ""Contributor"" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a ""NOTICE"" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an ""AS IS"" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

=======================================================================
Cephes's 3-Clause BSD License
=======================================================================

Code derived from implementations in the Cephes Math Library should mention
its derivation and reference the following license:

   3-Clause BSD License for the Cephes Math Library
   Copyright (c) 2018, Steven Moshier
   All rights reserved.

   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions are met:

   * Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

   * Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

   * Neither the name of the nor the
   names of its contributors may be used to endorse or promote products
   derived from this software without specific prior written permission.

   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND
   ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
   WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
   DISCLAIMED. IN NO EVENT SHALL Steven Moshier BE LIABLE FOR ANY
   DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
   (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
   LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
   ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


=======================================================================
SciPy's 3-Clause BSD License
=======================================================================

Code derived from implementations in SciPy should mention its derivation
and reference the following license:

   Copyright (c) 2001-2002 Enthought, Inc.  2003-2019, SciPy Developers.
   All rights reserved.

   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions
   are met:

   1. Redistributions of source code must retain the above copyright
     notice, this list of conditions and the following disclaimer.

   2. Redistributions in binary form must reproduce the above
     copyright notice, this list of conditions and the following
     disclaimer in the documentation and/or other materials provided
     with the distribution.

   3. Neither the name of the copyright holder nor the names of its
     contributors may be used to endorse or promote products derived
     from this software without specific prior written permission.

   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
   ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
   OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

=======================================================================
Boost's 1.0 Software License
=======================================================================

Code derived from implementations in Boost 1.0 should mention its
derivation and reference the following license:

   Boost Software License - Version 1.0 - August 17th, 2003

   Permission is hereby granted, free of charge, to any person or organization
   obtaining a copy of the software and accompanying documentation covered by
   this license (the ""Software"") to use, reproduce, display, distribute,
   execute, and transmit the Software, and to prepare derivative works of the
   Software, and to permit third-parties to whom the Software is furnished to
   do so, all subject to the following:

   The copyright notices in the Software and this entire statement, including
   the above license grant, this restriction and the following disclaimer,
   must be included in all copies of the Software, in whole or in part, and
   all derivative works of the Software, unless such copies or derivative
   works are solely in the form of machine-executable object code generated by
   a source language processor.

   THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
   FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
   SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
   FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
   ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
   DEALINGS IN THE SOFTWARE.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets ""[]""
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same ""printed page"" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the ""License"");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an ""AS IS"" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.

=======================================================================
PILLOW-SIMD Software License
=======================================================================

Code derived from implementations in PILLOW-SIMD should mention its derivation
and reference the following license:

    The Python Imaging Library (PIL) is

        Copyright © 1997-2011 by Secret Labs AB
        Copyright © 1995-2011 by Fredrik Lundh

    Pillow is the friendly PIL fork. It is

        Copyright © 2010-2022 by Alex Clark and contributors

    Like PIL, Pillow is licensed under the open source HPND License:

    By obtaining, using, and/or copying this software and/or its associated
    documentation, you agree that you have read, understood, and will comply
    with the following terms and conditions:

    Permission to use, copy, modify, and distribute this software and its
    associated documentation for any purpose and without fee is hereby granted,
    provided that the above copyright notice appears in all copies, and that
    both that copyright notice and this permission notice appear in supporting
    documentation, and that the name of Secret Labs AB or the author not be
    used in advertising or publicity pertaining to distribution of the software
    without specific, written prior permission.

    SECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS
    SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS.
    IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR BE LIABLE FOR ANY SPECIAL,
    INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM
    LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE
    OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
    PERFORMANCE OF THIS SOFTWARE.
","[""`\n['Copyright 2015, Anton Blanchard, IBM Corp."", ""SPDX-License-Identifier: GPL-2.0-only']\n`""]",Apache-2.0 BSD-3-Clause BSL-1.0,,0.0,0.0
123,123,133,pytorch-main/LICENSE,BSD-3-Clause,23,[],"From PyTorch:

Copyright (c) 2016-     Facebook, Inc            (Adam Paszke)
Copyright (c) 2014-     Facebook, Inc            (Soumith Chintala)
Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)
Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)
Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)
Copyright (c) 2011-2013 NYU                      (Clement Farabet)
Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)
Copyright (c) 2006      Idiap Research Institute (Samy Bengio)
Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)

From Caffe2:

Copyright (c) 2016-present, Facebook Inc. All rights reserved.

All contributions by Facebook:
Copyright (c) 2016 Facebook Inc.

All contributions by Google:
Copyright (c) 2015 Google Inc.
All rights reserved.

All contributions by Yangqing Jia:
Copyright (c) 2015 Yangqing Jia
All rights reserved.

All contributions by Kakao Brain:
Copyright 2019-2020 Kakao Brain

All contributions by Cruise LLC:
Copyright (c) 2022 Cruise LLC.
All rights reserved.

All contributions from Caffe:
Copyright(c) 2013, 2014, 2015, the respective contributors
All rights reserved.

All other contributions:
Copyright(c) 2015, 2016 the respective contributors
All rights reserved.

Caffe2 uses a copyright model similar to Caffe: each contributor holds
copyright over their contributions to Caffe2. The project versioning records
all such contribution and copyright details. If a contributor wants to further
mark their specific copyright on a particular contribution, they should
indicate their copyright solely in the commit message of the change when it is
committed.

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

3. Neither the names of Facebook, Deepmind Technologies, NYU, NEC Laboratories America
   and IDIAP Research Institute nor the names of its contributors may be
   used to endorse or promote products derived from this software without
   specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
","[""`\n['SPDX-License-Identifier: GPL-2.0-only']\n`""]",BSD-3-Clause,,0.0,0.0
124,124,134,pytorch-main/torch/fx/experimental/unification/LICENSE.txt,BSD-3-Clause,1222,[],"Copyright (c) 2014 Matthew RocklinAll rights reserved.Redistribution and use in source and binary forms, with or withoutmodification, are permitted provided that the following conditions are met:a. Redistributions of source code must retain the above copyright notice,this list of conditions and the following disclaimer.b. Redistributions in binary form must reproduce the above copyrightnotice, this list of conditions and the following disclaimer in thedocumentation and/or other materials provided with the distribution.c. Neither the name of Unification nor the names of its contributorsmay be used to endorse or promote products derived from this softwarewithout specific prior written permission.THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THEIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSEARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FORANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIALDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS ORSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVERCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICTLIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAYOUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCHDAMAGE.","['Copyright (c) 2014 Matthew RocklinAll rights reserved.Redistribution and use in source and binary forms, with or withoutmodification, are permitted provided that the following conditions are met:a. Redistributions of source code must retain the above copyright notice,this list of conditions and the following disclaimer.b. Redistributions in binary form must reproduce the above copyrightnotice, this list of conditions and the following disclaimer in thedocumentation and/or other materials provided with the distribution.c. Neither the name of Unification nor the names of its contributorsmay be used to endorse or promote products derived from this softwarewithout specific prior written permission.THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THEIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSEARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FORANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIALDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS ORSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVERCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICTLIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAYOUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCHDAMAGE.', 'SPDX-License-Identifier: GPL-2.0-only']",BSD-3-Clause,,0.0,0.0
125,125,135,pytorch-main/torch/distributed/pipeline/sync/LICENSE,BSD-3-Clause,1625,[],"Copyright 2019-2020 Kakao Brain

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.

3. Neither the name of the copyright holder nor the names of its
   contributors may be used to endorse or promote products derived from this
   software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.
",['SPDX-License-Identifier: GPL-2.0-only'],BSD-3-Clause,,0.0,0.0
126,126,136,pytorch-main/torch/overrides.py,Public-domain,160,[],"noqa: F404
Doesn't actually take or return tensor arguments
These are deprecated; don't test them
alias for torch.det  # type: ignore[attr-defined]
alias for torch.outer
type: ignore[attr-defined]  # noqa: B950
alias for torch.matmul
alias for torch.vstack
Generate methods like __add__ and add_ by default from add
Must have the same signature as func
This will make func dispatchable by __torch_function__
If torch function is not enabled, there are no overloaded types
Runtime is O(num_arguments * num_unique_types)
Check for __torch_function__ methods.
overloaded_args already have unique types.
Check for __torch_function__ mode.
Call overrides
ignore private functions or functions that are deleted in torch.__init__
ignore re-exported modules
ignore __future__ imports
cannot be overriden by __torch_function__
Force metaclass to generate constructor at the base of the hierarchy
 Every function in the PyTorchAPI that can be overriden needs an entry in this dict.
 Optimally we would use inspect to get the function signature and define the lambda function procedurally but that is blocked by generating function signatures for native kernels that can be consumed by inspect. See Issue #28233.
 alias for torch.cat alias for torch.concatenate
 Default method Inplace variant Dunder method Inplace dunder method Reverse dunder method
 bitwise_<op> have dunder methods of the form __<op>__ And so on.
 We only collect arguments if they have a unique type, which ensures reasonable performance even with a long list of possibly overloaded arguments.
 NB: Important to exclude _disabled_torch_function_impl, otherwise https://github.com/pytorch/pytorch/issues/64687
 Create lists explicitly for the first type (usually the only one done) to avoid setting up the iterator for overloaded_args.
 By default, insert argument at the end, but if it is subclass of another argument, insert it before that argument. This ensures ""subclasses before superclasses"".
 if we're here, the mode must be set to a TorchFunctionStackMode this unsets it and calls directly into TorchFunctionStackMode's torch function
 This call needs to become a classmethod call in the future. See https://github.com/pytorch/pytorch/issues/63767
 Use `public_api` instead of `implementation` so __torch_function__ implementations can do equality/identity comparisons.
 NB: this can't simply be `enable_reentrant_dispatch = torch._C._RestorePythonTLSSnapshot` because: 1. torch._C._RestorePythonTLSSnapshot is unavailable when this file initially gets imported. Probably an import order thing. 2. enable_reentrant_dispatch is technically public API; assigning it the object would change the __module__ to look private.
Python implementation of ``__torch_function__``  While most of the torch API and handling for ``__torch_function__`` happens at the C++ level, some of the torch API is written in Python so we need python-level handling for ``__torch_function__`` overrides as well. The main developer-facing functionality in this file are handle_torch_function and has_torch_function. See torch/functional.py and test/test_overrides.py for usage examples.  Note ---- heavily inspired by NumPy's ``__array_function__`` (see: https://github.com/pytorch/pytorch/issues/24015 and https://www.numpy.org/neps/nep-0018-array-function-protocol.html )  If changing this file in a way that can affect ``__torch_function__`` overhead, please report the benchmarks in ``benchmarks/overrides_benchmark``. See the instructions in the ``README.md`` in that directory.
Decorator that temporarily disables ``UserWarning``s for the given ``module`` if the warning message matches the     given ``regex`` pattern.      Arguments     ---------     func : function         Function to disable the warnings for.     regex : str         A regex pattern compilable by ``re.compile``. This is used to match the ``UserWarning`` message.     module : str         The python module to which the filtering should be restricted.      Returns     -------     function         The wrapped function.
Return public functions that cannot be overridden by ``__torch_function__``.      Returns     -------     Set[Callable]         A tuple of functions that are publicly available in the torch API but cannot         be overridden with ``__torch_function__``. Mostly this is because none of the         arguments of these functions are tensors or tensor-likes.      Examples     --------     >>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()     True     >>> torch.add in torch.overrides.get_ignored_functions()     False
Return public functions that do not wrap in a subclass when invoked by     the default ``Tensor.__torch_function__`` that preserves subclasses.  Typically,     these functions represent field accesses (i.e., retrieving a Tensor that     is stored somewhere on the Tensor) as opposed to computation.  Users of     these functions expect object identity to be preserved over multiple accesses     (e.g., ``a.grad is a.grad``) which cannot be upheld if we're wrapping on     the fly every time (furthermore, the tensor stored here might already be     the subclass, in which case wrapping really ought not to happen).      Not ALL property accessors have this property; for example ``Tensor.T`` actually     just creates a new transposed tensor on the fly, and so we SHOULD interpose on     these calls (you need to check the implementation of the function to see if     this is the case or not).  Additionally, if a property accessor doesn't return a Tensor,     it doesn't have to be on this list (though it is harmless if it is).
Return a dict containing dummy overrides for all overridable functions      Returns     -------     Dict[Callable, Callable]         A dictionary that maps overridable functions in the PyTorch API to         lambda functions that have the same signature as the real function         and unconditionally return -1. These lambda functions are useful         for testing API coverage for a type that defines ``__torch_function__``.      Examples     --------     >>> import inspect     >>> my_add = torch.overrides.get_testing_overrides()[torch.add]     >>> inspect.signature(my_add)     <Signature (input, other, out=None)>
Wraps a given function with ``__torch_function__`` -related functionality.      Parameters     ----------     dispatcher: Callable         A callable that returns an iterable of Tensor-likes passed into the function.      Note     ----     This decorator may reduce the performance of your code. Generally, it's enough to express     your code as a series of functions that, themselves, support __torch_function__. If you     find yourself in the rare situation where this is not the case, e.g. if you're wrapping a     low-level library and you also need it to work for Tensor-likes, then this function is available.      Examples     --------     >>> def dispatcher(a): # Must have the same signature as func     ...     return (a,)     >>> @torch.overrides.wrap_torch_function(dispatcher)     >>> def func(a): # This will make func dispatchable by __torch_function__     ...     return a + 0
Returns a list of arguments on which to call __torch_function__.      Checks arguments in relevant_args for __torch_function__ implementations,     storing references to the arguments and their types in overloaded_args and     overloaded_types in order of calling precedence. Only distinct types are     considered. If a type is a subclass of another type it will have higher     precedence, otherwise the precedence order is the same as the order of     arguments in relevant_args, that is, from left-to-right in the argument list.      The precedence-determining algorithm implemented in this function is     described in `NEP-0018`_.      See torch::append_overloaded_arg for the equivalent function in the C++     implementation.      Parameters     ----------     relevant_args : iterable of array-like         Iterable of array-like arguments to check for __torch_function__         methods.      get_type_fn : callable, optional         Function to call on each argument in relevant_args to get its type.      Returns     -------     overloaded_args : list         Arguments from relevant_args on which to call __torch_function__         methods, in the order in which they should be called.      .. _NEP-0018:        https://numpy.org/neps/nep-0018-array-function-protocol.html
Implement a function with checks for ``__torch_function__`` overrides.      See torch::autograd::handle_torch_function for the equivalent of this     function in the C++ implementation.      Arguments     ---------     public_api : function         Function exposed by the public torch API originally called like         ``public_api(*args, **kwargs)`` on which arguments are now being         checked.     relevant_args : iterable         Iterable of arguments to check for __torch_function__ methods.     args : tuple         Arbitrary positional arguments originally passed into ``public_api``.     kwargs : tuple         Arbitrary keyword arguments originally passed into ``public_api``.      Returns     -------     object         Result from calling ``implementation`` or an ``__torch_function__``         method, as appropriate.      Raises     ------     TypeError : if no implementation is found.      Example     -------     >>> def func(a):     ...     if has_torch_function_unary(a):     ...         return handle_torch_function(func, (a,), a)     ...     return a + 0
r""""""Check for __torch_function__ implementations in the elements of an iterable     or if a __torch_function__ mode is enabled.  Considers exact ``Tensor`` s     and ``Parameter`` s non-dispatchable.  Use this to guard a call to     :func:`handle_torch_function`; don't use it to test if something     is Tensor-like, use :func:`is_tensor_like` instead.     Arguments     ---------     relevant_args : iterable         Iterable or arguments to check for __torch_function__ methods.     Returns     -------     bool         True if any of the elements of relevant_args have __torch_function__         implementations, False otherwise.     See Also     ________     torch.is_tensor_like         Checks if something is a Tensor-like, including an exact ``Tensor``.
r""""""Special case of `has_torch_function` for single inputs.     Instead of:       `has_torch_function((t,))`     call:       `has_torch_function_unary(t)`     which skips unnecessary packing and unpacking work.
r""""""Special case of `has_torch_function` that skips tuple creation.      This uses the METH_FASTCALL protocol introduced in Python 3.7      Instead of:       `has_torch_function((a, b))`     call:       `has_torch_function_variadic(a, b)`     which skips unnecessary packing and unpacking work.
List functions that are overridable via __torch_function__      Returns     -------     Dict[Any, List[Callable]]         A dictionary that maps namespaces that contain overridable functions         to functions in that namespace that can be overridden.
Get a human readable string name for a function passed to     __torch_function__      Arguments     ---------     f : Callable         Function to resolve the name of.      Returns     -------     str         Name of the function; if eval'ed it should give back the input         function.
Returns a set of the overridable methods on ``torch.Tensor``
Returns True if the function passed in is a handler for a     method or property belonging to ``torch.Tensor``, as passed     into ``__torch_function__``.      .. note::        For properties, their ``__get__`` method must be passed in.      This may be needed, in particular, for the following reasons:      1. Methods/properties sometimes don't contain a `__module__` slot.     2. They require that the first passed-in argument is an instance        of ``torch.Tensor``.      Examples     --------     >>> is_tensor_method_or_property(torch.Tensor.add)     True     >>> is_tensor_method_or_property(torch.add)     False
Returns ``True`` if the passed-in input is a Tensor-like.      Currently, this occurs whenever there's a ``__torch_function__``     attribute on the type of the input.      Examples     --------     A subclass of tensor is generally a Tensor-like.      >>> class SubTensor(torch.Tensor): ...     >>> is_tensor_like(SubTensor([0]))     True      Built-in or user types aren't usually Tensor-like.      >>> is_tensor_like(6)     False     >>> is_tensor_like(None)     False     >>> class NotATensor: ...     >>> is_tensor_like(NotATensor())     False      But, they can be made Tensor-like by implementing __torch_function__.      >>> class TensorLike:     ...     @classmethod     ...     def __torch_function__(cls, func, types, args, kwargs):     ...         return -1     >>> is_tensor_like(TensorLike())     True
A ``TorchFunctionMode`` allows you to override the meaning of all     ``__torch_function__`` overrideable functions within a dynamic scope,     without having to actually create a tensor subclass or manually     monkey-patch functions in the PyTorch API.  Some common situations     where you should use a mode:          * You want to override the meaning of factory functions, or other           functions that do not otherwise take a tensor as an argument           (these cannot be overridden with tensor subclasses).          * You want to override the behavior of all functions without needing           to wrap your inputs in tensor subclasses; e.g., if you are just           interested in logging intermediate computations.          * You want to control the order of execution of various tensor           subclasses explicitly, rather than implicitly via the return of           ``NotImplemented``.      Independent subclasses of :class:`TorchFunctionMode` are compositional:     modes can be pushed onto a stack using ``with MyMode():``.     When you call functions in the PyTorch API inside your     ``__torch_function__`` implementation, by default, they will forward on to     the next mode on the mode stack.  If you want recursively call back into     your current ``__torch_function__`` implementation, either explicitly     invoke ``self.__torch_function__(...)``, or use the context manager     ``enable_torch_function_mode(self, replace=self.inner)`` to make PyTorch     API self-referential (beware of infinite loops, in this case!)",['SPDX-License-Identifier: GPL-2.0-only'],Public-domain,,0.0,0.0
127,127,137,pytorch-main/torch/distributions/lkj_cholesky.py,Apache-2.0,1446,[],"xdoctest: +IGNORE_WANT(""non-deterministic"")
l @ l.T is a sample of a correlation 3x3 matrix
This is used to draw vectorized samples from the beta distribution in Sec. 3.2 of [1].
Replace NaNs in first row
Fill diagonal elements; clamp for numerical stability
Compute normalization constant (page 1999 of [1])
 Copyright: Contributors to the Pyro project. SPDX-License-Identifier: Apache-2.0
 This uses the Onion method, but there are a few differences from [1] Sec. 3.2: - This vectorizes the for loop and also works for heterogeneous eta. - Same algorithm generalizes to n=1. - The procedure is simplified since we are sampling the cholesky factor of the correlation matrix instead of the correlation matrix itself. As such, we only need to generate `w`.
 See: https://mc-stan.org/docs/2_25/functions-reference/cholesky-lkj-correlation-distribution.html The probability of a correlation matrix is proportional to determinant ** (concentration - 1) = prod(L_ii ^ 2(concentration - 1)) Additionally, the Jacobian of the transformation from Cholesky factor to correlation matrix is: prod(L_ii ^ (D - i)) So the probability of a Cholesky factor is propotional to prod(L_ii ^ (2 * concentration - 2 + D - i)) = prod(L_ii ^ order_i) with order_i = 2 * concentration - 2 + D - i
 pi_constant in [1] is D * (D - 1) / 4 * log(pi) pi_constant in multigammaln is (D - 1) * (D - 2) / 4 * log(pi) hence, we need to add a pi_constant = (D - 1) * log(pi) / 2
This closely follows the implementation in NumPyro (https://github.com/pyro-ppl/numpyro).  Original copyright notice:  # Copyright: Contributors to the Pyro project. # SPDX-License-Identifier: Apache-2.0
r""""""     LKJ distribution for lower Cholesky factor of correlation matrices.     The distribution is controlled by ``concentration`` parameter :math:`\eta`     to make the probability of the correlation matrix :math:`M` generated from     a Cholesky factor proportional to :math:`\det(M)^{\eta - 1}`. Because of that,     when ``concentration == 1``, we have a uniform distribution over Cholesky     factors of correlation matrices::          L ~ LKJCholesky(dim, concentration)         X = L @ L' ~ LKJCorr(dim, concentration)      Note that this distribution samples the     Cholesky factor of correlation matrices and not the correlation matrices     themselves and thereby differs slightly from the derivations in [1] for     the `LKJCorr` distribution. For sampling, this uses the Onion method from     [1] Section 3.      Example::          >>> # xdoctest: +IGNORE_WANT(""non-deterministic"")         >>> l = LKJCholesky(3, 0.5)         >>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix         tensor([[ 1.0000,  0.0000,  0.0000],                 [ 0.3516,  0.9361,  0.0000],                 [-0.1899,  0.4748,  0.8593]])      Args:         dimension (dim): dimension of the matrices         concentration (float or Tensor): concentration/shape parameter of the             distribution (often referred to as eta)      **References**      [1] `Generating random correlation matrices based on vines and extended onion method` (2009),     Daniel Lewandowski, Dorota Kurowicka, Harry Joe.     Journal of Multivariate Analysis. 100. 10.1016/j.jmva.2009.04.008","['Copyright: Contributors to the Pyro project. SPDX-License-Identifier: Apache-2.0', 'SPDX-License-Identifier: Apache-2.0', 'SPDX-License-Identifier: Apache-2.0']",Apache-2.0,,0.0,1.4210854715202004e-14
128,128,138,pytorch-main/torch/utils/viz/_cycles.py,Apache-2.0,222,[],"provide a way to disarm the callback
http://www.apache.org/licenses/LICENSE-2.0
Object annotations.
For basic types, use the repr.
container {
main {
preContainer {
 when GC runs during exit, things like `sys` will already be unloaded so we have to disable the callback to avoid hitting errors.
 things in gc.garbage have survived a collection so to free them we have to collect a generation greater than them but that might _also_ free other stuff and we don't want to miss that stuff. So we have to now force gc at the highest level here, report all of what we found, _then_ we can free it up.
 we have to re-run GC to clean up the cycles we saved from before.
 Function to visualize cycles adapated from refcycle: Copyright 2013 Mark Dickinson
 Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at
 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
 if cell_contents is empty, accessing it raises ValueError in this case there is no object to annotate
 For subclasses of weakref, we can't reliably distinguish the callback (if any) from other attributes.
 Some badly-behaved code replaces the f_locals dict with something that doesn't support the full dict interface.  So we only continue with the annotation if f_locals is a Python dict.
Return known information about references held by the given object.      Returns a mapping from referents to lists of descriptions.  Note that there     may be more than one edge leading to any particular referent; hence the     need for a list.  Descriptions are currently strings.
Return a string to be used for Graphviz nodes.      The string should be short but as informative as possible.
_template = """""" <!DOCTYPE html> <html> <head>   <style>     body {       margin: 0;       padding: 0;       overflow: hidden;     }      #container {       display: flex;       flex-direction: column;       height: 100vh;     }      #main {       flex: 2;       overflow: auto;     }      #preContainer {       flex: 1;       overflow: auto;     }      svg {         overflow: scroll;     }      pre {       margin: 0;       padding: 10px;     }   </style> </head> <body>   <div id=""container"">     <div id=""main"">     </div>     <div id=""preContainer"">       <pre id=""stacktrace"">Mouse over tensor objects to see where they were allocated.</pre>     </div>   </div> <script src='https://cdnjs.cloudflare.com/ajax/libs/viz.js/1.8.0/viz-lite.js'></script> <script> let dot = $DOT let image = Viz(dot, {format: 'svg'}); document.getElementById('main').innerHTML = image $LISTENERS </script> </body> </html>
_listener_template = """""" document.getElementById('node{id}').addEventListener('mouseover', function(event) {{   document.getElementById(""stacktrace"").textContent = {stack} }})
Install a warning that reports whenever a cycle that is holding CUDA memory is observed.      The warning produces an .html file that visualizes the cycle,     and links it to the stack frame that allocted the CUDA tensor.      Reference cycles are freed by the cycle collector rather than being cleaned up     when the objects in the cycle first become unreachable. If a cycle points to a tensor,     the CUDA memory for that tensor will not be freed until garbage collection runs.     Accumulation of CUDA allocations can lead to out of memory errors (OOMs), as well as     non-deterministic allocation behavior which is harder to debug.","['Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0', 'SPDX-License-Identifier: GPL-2.0-only', 'Copyright 2013 Mark Dickinson']",Apache-2.0,,0.0,1.4210854715202004e-14
129,129,139,pytorch-main/torch/utils/model_dump/htm.mjs,Apache-possibility,247,[],"// HTM, Apache License
var n=function(t,s,r,e){var u;s[0]=0;for(var h=1;h<s.length;h++){var p=s[h++],a=s[h]?(s[0]|=p?1:2,r[s[h++]]):s[++h];3===p?e[0]=a:4===p?e[1]=Object.assign(e[1]||{},a):5===p?(e[1]=e[1]||{})[s[++h]]=a:6===p?e[1][s[++h]]+=a+"""":p?(u=t.apply(a,n(t,a,r,["""",null])),e.push(u),a[0]?s[0]|=2:(s[h-2]=0,s[h]=u)):e.push(a)}return e},t=new Map;export default function(s){var r=t.get(this);return r||(r=new Map,t.set(this,r)),(r=n(this,r.get(s)||(r.set(s,r=function(n){for(var t,s,r=1,e="""",u="""",h=[0],p=function(n){1===r&&(n||(e=e.replace(/^\s*\n\s*|\s*\n\s*$/g,"""")))?h.push(0,n,e):3===r&&(n||e)?(h.push(3,n,e),r=2):2===r&&""...""===e&&n?h.push(4,n,0):2===r&&e&&!n?h.push(5,0,!0,e):r>=5&&((e||!n&&5===r)&&(h.push(r,0,e,s),r=6),n&&(h.push(r,n,0,s),r=6)),e=""""},a=0;a<n.length;a++){a&&(1===r&&p(),p(a));for(var l=0;l<n[a].length;l++)t=n[a][l],1===r?""<""===t?(p(),h=[h],r=3):e+=t:4===r?""--""===e&&"">""===t?(r=1,e=""""):e=t+e[0]:u?t===u?u="""":e+=t:'""'===t||""'""===t?u=t:"">""===t?(p(),r=1):r&&(""=""===t?(r=5,s=e,e=""""):""/""===t&&(r<5||"">""===n[a][l+1])?(p(),3===r&&(h=h[0]),r=h,(h=h[0]).push(2,0,r),r=0):"" ""===t||""\t""===t||""\n""===t||""\r""===t?(p(),r=2):e+=t),3===r&&""!--""===e&&(r=4,h=h[0])}return p(),h}(s)),r),arguments,[])).length>1?r:r[0]}
","['// HTM, Apache License']",Apache-possibility,,0.0,0.0
130,130,140,pytorch-main/torch/csrc/serialization.cpp,See-URL,1871,[],"Call Python fildes.read(nbytes) and copy it to buf.
we read EOF
Slurp it into the buffer we actually want
Either does fildes.readinto(buf) or fildes.write(buf)
Call Python fildes.readinto(buf)
Call Python fildes.write(buf)
Requires that we read EXACTLY nbytes; fails if we don't.
Here we use a tensor.to() to impl D2H for all non-CPU device.
convert big endian cpu to little endian storage
fast track for bytes and little endian
NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays)
NOLINTNEXTLINE(bugprone-branch-clone)
convert little endian storage to big endian cpu
NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays)
NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays)
fast track for bytes and little endian
NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays)
NOLINTNEXTLINE(bugprone-branch-clone)
Here we use a tensor.copy_() to impl H2D for all non-CPU device.
 Try to use fildes.readinto() instead of fildes.read() because it is more memory efficient. TODO: Stop calling PyObject_HasAttrString() in a loop on our read loop
 If we request a large amount of data, f.read() will internally try to allocate a buffer of that size.  This is counterproductive, because it's not the buffer we ultimately want to write the data into.  Read less than that and avoid allocating too much extra memory. TODO: Maybe 260 KB is a bit small... 2^18 (~260 KB)
 fildes.readinto can return UnsupportedOperation so fall back to fildes.read.
 doPartialRead may not set errno we read in 1GB blocks to avoid bugs on Mac OS X Lion see https://github.com/pytorch/pytorch/issues/1031 for more details
 This is guaranteed by POSIX, but I just want to be double-sure to not underflow a signed integer.
 doPartialWrite may not set errno we write in 1GB blocks to avoid bugs on Mac OS X Lion see https://github.com/pytorch/pytorch/issues/1031 for more details
 save_save is necessary since the old eager format saved storages as [size + data], but the v1.5 eager format removes this since size is saved in the filesize.
 We are using a mutable pointer here because we're ultimately calling into a Python API that requires that, even though it won't mutate the data.
is_read
is_read
resizable=","['NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays)', 'NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays)', 'NOLINTNEXTLINE(cppcoreguidelines-avoid-c-arrays,modernize-avoid-c-arrays)']",See-URL,,0.0,1.4210854715202004e-14
131,131,141,pytorch-main/torch/testing/_comparison.py,See-URL,366,[],"Ensure that only mismatches are used for the max_abs_diff computation
Ensure that only mismatches are used for the max_rel_diff computation
noqa: B903
We are not using `self._raise_error_meta` here since we need the exception chaining
type: ignore[return-value]
These checks are needed, since Tensor.to_dense() fails on tensors that are already strided
Since the origination aborts after the first failure, we try to be deterministic
Hide this function from `pytest`'s traceback
Explicitly raising from None to hide the internal traceback
tensor to tensor comparison
scalar to scalar comparison
numpy array to numpy array comparison
sequence to sequence comparison
mapping to mapping comparison
By default, directly related instances can be compared
This check can be made more strict with allow_subclasses=False
If the inputs are not directly related, they are never considered close
NaN != NaN by default.
The default error message can be overwritten.
Hide this function from `pytest`'s traceback
TODO: compose all metas into one AssertionError
 Some analysis of tolerance by logging tests from test_torch.py can be found in https://github.com/pytorch/pytorch/pull/32538. {dtype: (rtol, atol)}
 The default tolerances of torch.float32 are used for quantized dtypes, because quantized tensors are compared in their dequantized and floating point representation. For more details see `TensorLikePair._compare_quantized_values`
 We require both tolerance to be omitted or specified, because specifying only one might lead to surprising results. Imagine setting atol=0.0 and the tensors still match because rtol>0.0.
 TODO: Instead of always upcasting to int64, it would be sufficient to cast to the next higher dtype to avoid overflow
 The comparison logic uses operators currently not supported by the MPS backends. See https://github.com/pytorch/pytorch/issues/77144 for details. TODO: Remove this conversion as soon as all operations are supported natively by the MPS backend type: ignore[attr-defined]
 Compressed and plain indices in the CSR / CSC / BSR / BSC sparse formates can be `torch.int32` _or_ `torch.int64`. While the same dtype is enforced for the compressed and plain indices of a single tensor, it can be different between two tensors. Thus, we need to convert them to the same dtype, or the comparison will fail.
 We explicitly exclude str's here since they are self-referential and would cause an infinite recursion loop: ""a"" == ""a""[0][0]...
 Raising an `UnsupportedInputs` during origination indicates that the pair type is not able to handle the inputs. Thus, we try the next pair type.
 Raising an `ErrorMeta` during origination is the orderly way to abort and so we simply re-raise it. This is only in a separate branch, because the one below would also except it.
 Raising any other exception during origination is unexpected and will give some extra information about what happened. If applicable, the exception should be expected in the future.
 Raising any exception besides `ErrorMeta` while comparing is unexpected and will give some extra information about what happened. If applicable, the exception should be expected in the future.
 [ErrorMeta Cycles] ErrorMeta objects in this list capture tracebacks that refer to the frame of this function. The local variable `error_metas` refers to the error meta objects, creating a reference cycle. Frames in the traceback would not get freed until cycle collection, leaking cuda memory in tests. We break the cycle by removing the reference to the error_meta objects from this frame as it returns.
 The types of the sequences do not have to match. They only have to have the same length and their elements have to match.
 The types and a possible ordering of mappings do not have to match. They only have to have the same set of keys and their elements have to match.
 Exceptions to these rules are Python scalars. They can be checked regardless of their type if check_dtype=False.
 If msg is a callable, it can be used to augment the generated message with extra information
Internal testing exception that makes that carries error metadata.
Returns the default absolute and relative testing tolerances for a set of inputs based on the dtype.      See :func:`assert_close` for a table of the default tolerance for each dtype.      Returns:         (Tuple[float, float]): Loosest tolerances of all input dtypes.
Gets absolute and relative to be used for numeric comparisons.      If both ``rtol`` and ``atol`` are specified, this is a no-op. If both are not specified, the return value of     :func:`default_tolerances` is used.      Raises:         ErrorMeta: With :class:`ValueError`, if only ``rtol`` or ``atol`` is specified.      Returns:         (Tuple[float, float]): Valid absolute and relative tolerances.
Makes a mismatch error message for numeric values.      Args:         default_identifier (str): Default description of the compared values, e.g. ""Tensor-likes"".         identifier (Optional[Union[str, Callable[[str], str]]]): Optional identifier that overrides             ``default_identifier``. Can be passed as callable in which case it will be called with             ``default_identifier`` to create the description at runtime.         extra (Optional[str]): Extra information to be placed after the message header and the mismatch statistics.         abs_diff (float): Absolute difference.         abs_diff_idx (Optional[Union[int, Tuple[int, ...]]]): Optional index of the absolute difference.         atol (float): Allowed absolute tolerance. Will only be added to mismatch statistics if it or ``rtol`` are             ``> 0``.         rel_diff (float): Relative difference.         rel_diff_idx (Optional[Union[int, Tuple[int, ...]]]): Optional index of the relative difference.         rtol (float): Allowed relative tolerance. Will only be added to mismatch statistics if it or ``atol`` are             ``> 0``.
Makes a mismatch error message for scalars.      Args:         actual (Union[bool, int, float, complex]): Actual scalar.         expected (Union[bool, int, float, complex]): Expected scalar.         rtol (float): Relative tolerance.         atol (float): Absolute tolerance.         identifier (Optional[Union[str, Callable[[str], str]]]): Optional description for the scalars. Can be passed             as callable in which case it will be called by the default value to create the description at runtime.             Defaults to ""Scalars"".
Makes a mismatch error message for tensors.      Args:         actual (torch.Tensor): Actual tensor.         expected (torch.Tensor): Expected tensor.         matches (torch.Tensor): Boolean mask of the same shape as ``actual`` and ``expected`` that indicates the             location of matches.         rtol (float): Relative tolerance.         atol (float): Absolute tolerance.         identifier (Optional[Union[str, Callable[[str], str]]]): Optional description for the tensors. Can be passed             as callable in which case it will be called by the default value to create the description at runtime.             Defaults to ""Tensor-likes"".
Exception to be raised during the construction of a :class:`Pair` in case it doesn't support the inputs.
ABC for all comparison pairs to be used in conjunction with :func:`assert_equal`.      Each subclass needs to overwrite :meth:`Pair.compare` that performs the actual comparison.      Each pair receives **all** options, so select the ones applicable for the subclass and forward the rest to the     super class. Raising an :class:`UnsupportedInputs` during constructions indicates that the pair is not able to     handle the inputs and the next pair type will be tried.      All other errors should be raised as :class:`ErrorMeta`. After the instantiation, :meth:`Pair._make_error_meta` can     be used to automatically handle overwriting the message with a user supplied one and id handling.
Checks if all inputs are instances of a given class and raise :class:`UnsupportedInputs` otherwise.
Raises an :class:`ErrorMeta` from a given exception type and message and the stored id.          .. warning::              If you use this before the ``super().__init__(...)`` call in the constructor, you have to pass the ``id``             explicitly.
Compares the inputs and raises an :class`ErrorMeta` in case they mismatch.
Returns extra information that will be included in the representation.          Should be overwritten by all subclasses that use additional options. The representation of the object will only         be surfaced in case we encounter an unexpected error and thus should help debug the issue. Can be a sequence of         key-value-pairs or attribute names.
Pair for any type of inputs that will be compared with the `==` operator.      .. note::          Since this will instantiate for any kind of inputs, it should only be used as fallback after all other pairs         couldn't handle the inputs.
Pair for ``None`` inputs.
Pair for :class:`bool` inputs.      .. note::          If ``numpy`` is available, also handles :class:`numpy.bool_` inputs.
Pair for Python number (:class:`int`, :class:`float`, and :class:`complex`) inputs.      .. note::          If ``numpy`` is available, also handles :class:`numpy.number` inputs.      Kwargs:         rtol (Optional[float]): Relative tolerance. If specified ``atol`` must also be specified. If omitted, default             values based on the type are selected with the below table.         atol (Optional[float]): Absolute tolerance. If specified ``rtol`` must also be specified. If omitted, default             values based on the type are selected with the below table.         equal_nan (bool): If ``True``, two ``NaN`` values are considered equal. Defaults to ``False``.         check_dtype (bool): If ``True``, the type of the inputs will be checked for equality. Defaults to ``False``.      The following table displays correspondence between Python number type and the ``torch.dtype``'s. See     :func:`assert_close` for the corresponding tolerances.      +------------------+-------------------------------+     | ``type``         | corresponding ``torch.dtype`` |     +==================+===============================+     | :class:`int`     | :attr:`~torch.int64`          |     +------------------+-------------------------------+     | :class:`float`   | :attr:`~torch.float64`        |     +------------------+-------------------------------+     | :class:`complex` | :attr:`~torch.complex64`      |     +------------------+-------------------------------+
Pair for :class:`torch.Tensor`-like inputs.      Kwargs:         allow_subclasses (bool):         rtol (Optional[float]): Relative tolerance. If specified ``atol`` must also be specified. If omitted, default             values based on the type are selected. See :func:assert_close: for details.         atol (Optional[float]): Absolute tolerance. If specified ``rtol`` must also be specified. If omitted, default             values based on the type are selected. See :func:assert_close: for details.         equal_nan (bool): If ``True``, two ``NaN`` values are considered equal. Defaults to ``False``.         check_device (bool): If ``True`` (default), asserts that corresponding tensors are on the same             :attr:`~torch.Tensor.device`. If this check is disabled, tensors on different             :attr:`~torch.Tensor.device`'s are moved to the CPU before being compared.         check_dtype (bool): If ``True`` (default), asserts that corresponding tensors have the same ``dtype``. If this             check is disabled, tensors with different ``dtype``'s are promoted  to a common ``dtype`` (according to             :func:`torch.promote_types`) before being compared.         check_layout (bool): If ``True`` (default), asserts that corresponding tensors have the same ``layout``. If this             check is disabled, tensors with different ``layout``'s are converted to strided tensors before being             compared.         check_stride (bool): If ``True`` and corresponding tensors are strided, asserts that they have the same stride.
Checks if the attributes of two tensors match.          Always checks          - the :attr:`~torch.Tensor.shape`,         - whether both inputs are quantized or not,         - and if they use the same quantization scheme.          Checks for          - :attr:`~torch.Tensor.layout`,         - :meth:`~torch.Tensor.stride`,         - :attr:`~torch.Tensor.device`, and         - :attr:`~torch.Tensor.dtype`          are optional and can be disabled through the corresponding ``check_*`` flag during construction of the pair.
Equalizes some attributes of two tensors for value comparison.          If ``actual`` and ``expected`` are ...          - ... not on the same :attr:`~torch.Tensor.device`, they are moved CPU memory.         - ... not of the same ``dtype``, they are promoted  to a common ``dtype`` (according to             :func:`torch.promote_types`).         - ... not of the same ``layout``, they are converted to strided tensors.          Args:             actual (Tensor): Actual tensor.             expected (Tensor): Expected tensor.          Returns:             (Tuple[Tensor, Tensor]): Equalized tensors.
Compares quantized tensors by comparing the :meth:`~torch.Tensor.dequantize`'d variants for closeness.          .. note::              A detailed discussion about why only the dequantized variant is checked for closeness rather than checking             the individual quantization parameters for closeness and the integer representation for equality can be             found in https://github.com/pytorch/pytorch/issues/68548.
Compares sparse COO tensors by comparing          - the number of sparse dimensions,         - the number of non-zero elements (nnz) for equality,         - the indices for equality, and         - the values for closeness.
Compares sparse compressed tensors by comparing          - the number of non-zero elements (nnz) for equality,         - the plain indices for equality,         - the compressed indices for equality, and         - the values for closeness.
Checks if the values of two tensors are equal.
Checks if the values of two tensors are close up to a desired tolerance.
Originates pairs from the individual inputs.      ``actual`` and ``expected`` can be possibly nested :class:`~collections.abc.Sequence`'s or     :class:`~collections.abc.Mapping`'s. In this case the pairs are originated by recursing through them.      Args:         actual (Any): Actual input.         expected (Any): Expected input.         pair_types (Sequence[Type[Pair]]): Sequence of pair types that will be tried to construct with the inputs.             First successful pair will be used.         sequence_types (Tuple[Type, ...]): Optional types treated as sequences that will be checked elementwise.         mapping_types (Tuple[Type, ...]): Optional types treated as mappings that will be checked elementwise.         id (Tuple[Any, ...]): Optional id of a pair that will be included in an error message.         **options (Any): Options passed to each pair during construction.      Raises:         ErrorMeta: With :class`AssertionError`, if the inputs are :class:`~collections.abc.Sequence`'s, but their             length does not match.         ErrorMeta: With :class`AssertionError`, if the inputs are :class:`~collections.abc.Mapping`'s, but their set of             keys do not match.         ErrorMeta: With :class`TypeError`, if no pair is able to handle the inputs.         ErrorMeta: With any expected exception that happens during the construction of a pair.      Returns:         (List[Pair]): Originated pairs.
Asserts that inputs are equal.      ``actual`` and ``expected`` can be possibly nested :class:`~collections.abc.Sequence`'s or     :class:`~collections.abc.Mapping`'s. In this case the comparison happens elementwise by recursing through them.      Args:         actual (Any): Actual input.         expected (Any): Expected input.         pair_types (Sequence[Type[Pair]]): Sequence of :class:`Pair` types that will be tried to construct with the             inputs. First successful pair will be used. Defaults to only using :class:`ObjectPair`.         sequence_types (Tuple[Type, ...]): Optional types treated as sequences that will be checked elementwise.         mapping_types (Tuple[Type, ...]): Optional types treated as mappings that will be checked elementwise.         **options (Any): Options passed to each pair during construction.
r""""""Asserts that ``actual`` and ``expected`` are close.      If ``actual`` and ``expected`` are strided, non-quantized, real-valued, and finite, they are considered close if      .. math::          \lvert \text{actual} - \text{expected} \rvert \le \texttt{atol} + \texttt{rtol} \cdot \lvert \text{expected} \rvert      Non-finite values (``-inf`` and ``inf``) are only considered close if and only if they are equal. ``NaN``'s are     only considered equal to each other if ``equal_nan`` is ``True``.      In addition, they are only considered close if they have the same      - :attr:`~torch.Tensor.device` (if ``check_device`` is ``True``),     - ``dtype`` (if ``check_dtype`` is ``True``),     - ``layout`` (if ``check_layout`` is ``True``), and     - stride (if ``check_stride`` is ``True``).      If either ``actual`` or ``expected`` is a meta tensor, only the attribute checks will be performed.      If ``actual`` and ``expected`` are sparse (either having COO, CSR, CSC, BSR, or BSC layout), their strided members are     checked individually. Indices, namely ``indices`` for COO, ``crow_indices`` and ``col_indices`` for CSR and BSR,     or ``ccol_indices``  and ``row_indices`` for CSC and BSC layouts, respectively,     are always checked for equality whereas the values are checked for closeness according to the definition above.      If ``actual`` and ``expected`` are quantized, they are considered close if they have the same     :meth:`~torch.Tensor.qscheme` and the result of :meth:`~torch.Tensor.dequantize` is close according to the     definition above.      ``actual`` and ``expected`` can be :class:`~torch.Tensor`'s or any tensor-or-scalar-likes from which     :class:`torch.Tensor`'s can be constructed with :func:`torch.as_tensor`. Except for Python scalars the input types     have to be directly related. In addition, ``actual`` and ``expected`` can be :class:`~collections.abc.Sequence`'s     or :class:`~collections.abc.Mapping`'s in which case they are considered close if their structure matches and all     their elements are considered close according to the above definition.      .. note::          Python scalars are an exception to the type relation requirement, because their :func:`type`, i.e.         :class:`int`, :class:`float`, and :class:`complex`, is equivalent to the ``dtype`` of a tensor-like. Thus,         Python scalars of different types can be checked, but require ``check_dtype=False``.      Args:         actual (Any): Actual input.         expected (Any): Expected input.         allow_subclasses (bool): If ``True`` (default) and except for Python scalars, inputs of directly related types             are allowed. Otherwise type equality is required.         rtol (Optional[float]): Relative tolerance. If specified ``atol`` must also be specified. If omitted, default             values based on the :attr:`~torch.Tensor.dtype` are selected with the below table.         atol (Optional[float]): Absolute tolerance. If specified ``rtol`` must also be specified. If omitted, default             values based on the :attr:`~torch.Tensor.dtype` are selected with the below table.         equal_nan (Union[bool, str]): If ``True``, two ``NaN`` values will be considered equal.         check_device (bool): If ``True`` (default), asserts that corresponding tensors are on the same             :attr:`~torch.Tensor.device`. If this check is disabled, tensors on different             :attr:`~torch.Tensor.device`'s are moved to the CPU before being compared.         check_dtype (bool): If ``True`` (default), asserts that corresponding tensors have the same ``dtype``. If this             check is disabled, tensors with different ``dtype``'s are promoted  to a common ``dtype`` (according to             :func:`torch.promote_types`) before being compared.         check_layout (bool): If ``True`` (default), asserts that corresponding tensors have the same ``layout``. If this             check is disabled, tensors with different ``layout``'s are converted to strided tensors before being             compared.         check_stride (bool): If ``True`` and corresponding tensors are strided, asserts that they have the same stride.         msg (Optional[Union[str, Callable[[str], str]]]): Optional error message to use in case a failure occurs during             the comparison. Can also passed as callable in which case it will be called with the generated message and             should return the new message.      Raises:         ValueError: If no :class:`torch.Tensor` can be constructed from an input.         ValueError: If only ``rtol`` or ``atol`` is specified.         AssertionError: If corresponding inputs are not Python scalars and are not directly related.         AssertionError: If ``allow_subclasses`` is ``False``, but corresponding inputs are not Python scalars and have             different types.         AssertionError: If the inputs are :class:`~collections.abc.Sequence`'s, but their length does not match.         AssertionError: If the inputs are :class:`~collections.abc.Mapping`'s, but their set of keys do not match.         AssertionError: If corresponding tensors do not have the same :attr:`~torch.Tensor.shape`.         AssertionError: If ``check_layout`` is ``True``, but corresponding tensors do not have the same             :attr:`~torch.Tensor.layout`.         AssertionError: If only one of corresponding tensors is quantized.         AssertionError: If corresponding tensors are quantized, but have different :meth:`~torch.Tensor.qscheme`'s.         AssertionError: If ``check_device`` is ``True``, but corresponding tensors are not on the same             :attr:`~torch.Tensor.device`.         AssertionError: If ``check_dtype`` is ``True``, but corresponding tensors do not have the same ``dtype``.         AssertionError: If ``check_stride`` is ``True``, but corresponding strided tensors do not have the same stride.         AssertionError: If the values of corresponding tensors are not close according to the definition above.      The following table displays the default ``rtol`` and ``atol`` for different ``dtype``'s. In case of mismatching     ``dtype``'s, the maximum of both tolerances is used.      +---------------------------+------------+----------+     | ``dtype``                 | ``rtol``   | ``atol`` |     +===========================+============+==========+     | :attr:`~torch.float16`    | ``1e-3``   | ``1e-5`` |     +---------------------------+------------+----------+     | :attr:`~torch.bfloat16`   | ``1.6e-2`` | ``1e-5`` |     +---------------------------+------------+----------+     | :attr:`~torch.float32`    | ``1.3e-6`` | ``1e-5`` |     +---------------------------+------------+----------+     | :attr:`~torch.float64`    | ``1e-7``   | ``1e-7`` |     +---------------------------+------------+----------+     | :attr:`~torch.complex32`  | ``1e-3``   | ``1e-5`` |     +---------------------------+------------+----------+     | :attr:`~torch.complex64`  | ``1.3e-6`` | ``1e-5`` |     +---------------------------+------------+----------+     | :attr:`~torch.complex128` | ``1e-7``   | ``1e-7`` |     +---------------------------+------------+----------+     | :attr:`~torch.quint8`     | ``1.3e-6`` | ``1e-5`` |     +---------------------------+------------+----------+     | :attr:`~torch.quint2x4`   | ``1.3e-6`` | ``1e-5`` |     +---------------------------+------------+----------+     | :attr:`~torch.quint4x2`   | ``1.3e-6`` | ``1e-5`` |     +---------------------------+------------+----------+     | :attr:`~torch.qint8`      | ``1.3e-6`` | ``1e-5`` |     +---------------------------+------------+----------+     | :attr:`~torch.qint32`     | ``1.3e-6`` | ``1e-5`` |     +---------------------------+------------+----------+     | other                     | ``0.0``    | ``0.0``  |     +---------------------------+------------+----------+      .. note::          :func:`~torch.testing.assert_close` is highly configurable with strict default settings. Users are encouraged         to :func:`~functools.partial` it to fit their use case. For example, if an equality check is needed, one might         define an ``assert_equal`` that uses zero tolerances for every ``dtype`` by default:          >>> import functools         >>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)         >>> assert_equal(1e-9, 1e-10)         Traceback (most recent call last):         ...         AssertionError: Scalars are not equal!         <BLANKLINE>         Expected 1e-10 but got 1e-09.         Absolute difference: 9.000000000000001e-10         Relative difference: 9.0      Examples:         >>> # tensor to tensor comparison         >>> expected = torch.tensor([1e0, 1e-1, 1e-2])         >>> actual = torch.acos(torch.cos(expected))         >>> torch.testing.assert_close(actual, expected)          >>> # scalar to scalar comparison         >>> import math         >>> expected = math.sqrt(2.0)         >>> actual = 2.0 / math.sqrt(2.0)         >>> torch.testing.assert_close(actual, expected)          >>> # numpy array to numpy array comparison         >>> import numpy as np         >>> expected = np.array([1e0, 1e-1, 1e-2])         >>> actual = np.arccos(np.cos(expected))         >>> torch.testing.assert_close(actual, expected)          >>> # sequence to sequence comparison         >>> import numpy as np         >>> # The types of the sequences do not have to match. They only have to have the same         >>> # length and their elements have to match.         >>> expected = [torch.tensor([1.0]), 2.0, np.array(3.0)]         >>> actual = tuple(expected)         >>> torch.testing.assert_close(actual, expected)          >>> # mapping to mapping comparison         >>> from collections import OrderedDict         >>> import numpy as np         >>> foo = torch.tensor(1.0)         >>> bar = 2.0         >>> baz = np.array(3.0)         >>> # The types and a possible ordering of mappings do not have to match. They only         >>> # have to have the same set of keys and their elements have to match.         >>> expected = OrderedDict([(""foo"", foo), (""bar"", bar), (""baz"", baz)])         >>> actual = {""baz"": baz, ""bar"": bar, ""foo"": foo}         >>> torch.testing.assert_close(actual, expected)          >>> expected = torch.tensor([1.0, 2.0, 3.0])         >>> actual = expected.clone()         >>> # By default, directly related instances can be compared         >>> torch.testing.assert_close(torch.nn.Parameter(actual), expected)         >>> # This check can be made more strict with allow_subclasses=False         >>> torch.testing.assert_close(         ...     torch.nn.Parameter(actual), expected, allow_subclasses=False         ... )         Traceback (most recent call last):         ...         TypeError: No comparison pair was able to handle inputs of type         <class 'torch.nn.parameter.Parameter'> and <class 'torch.Tensor'>.         >>> # If the inputs are not directly related, they are never considered close         >>> torch.testing.assert_close(actual.numpy(), expected)         Traceback (most recent call last):         ...         TypeError: No comparison pair was able to handle inputs of type <class 'numpy.ndarray'>         and <class 'torch.Tensor'>.         >>> # Exceptions to these rules are Python scalars. They can be checked regardless of         >>> # their type if check_dtype=False.         >>> torch.testing.assert_close(1.0, 1, check_dtype=False)          >>> # NaN != NaN by default.         >>> expected = torch.tensor(float(""Nan""))         >>> actual = expected.clone()         >>> torch.testing.assert_close(actual, expected)         Traceback (most recent call last):         ...         AssertionError: Scalars are not close!         <BLANKLINE>         Expected nan but got nan.         Absolute difference: nan (up to 1e-05 allowed)         Relative difference: nan (up to 1.3e-06 allowed)         >>> torch.testing.assert_close(actual, expected, equal_nan=True)          >>> expected = torch.tensor([1.0, 2.0, 3.0])         >>> actual = torch.tensor([1.0, 4.0, 5.0])         >>> # The default error message can be overwritten.         >>> torch.testing.assert_close(actual, expected, msg=""Argh, the tensors are not close!"")         Traceback (most recent call last):         ...         AssertionError: Argh, the tensors are not close!         >>> # If msg is a callable, it can be used to augment the generated message with         >>> # extra information         >>> torch.testing.assert_close(         ...     actual, expected, msg=lambda msg: f""Header\n\n{msg}\n\nFooter""         ... )         Traceback (most recent call last):         ...         AssertionError: Header         <BLANKLINE>         Tensor-likes are not close!         <BLANKLINE>         Mismatched elements: 2 / 3 (66.7%)         Greatest absolute difference: 2.0 at index (1,) (up to 1e-05 allowed)         Greatest relative difference: 1.0 at index (1,) (up to 1.3e-06 allowed)         <BLANKLINE>         Footer
.. warning::         :func:`torch.testing.assert_allclose` is deprecated since ``1.12`` and will be removed in a future release.        Please use :func:`torch.testing.assert_close` instead. You can find detailed upgrade instructions        `here <https://github.com/pytorch/pytorch/issues/61844>`_.",['SPDX-License-Identifier: GPL-2.0-only'],See-URL,,0.0,0.0
132,132,142,pytorch-main/torch/mps/__init__.py,See-URL,939,[],"type: ignore[assignment]
local helper function (not public or exported)
 the torch.mps.manual_seed() can be called from the global torch.manual_seed() in torch/random.py. So we need to make sure mps is available (otherwise we just return without erroring out)
r"""""" This package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python. Metal is Apple's API for programming metal GPU (graphics processor unit). Using MPS means that increased performance can be achieved, by running work on the metal GPU(s). See https://developer.apple.com/documentation/metalperformanceshaders for more details.
r""""""Waits for all kernels in all streams on a MPS device to complete.
r""""""Returns the random number generator state as a ByteTensor.
r""""""Sets the random number generator state.      Args:         new_state (torch.ByteTensor): The desired state
r""""""Sets the seed for generating random numbers.      Args:         seed (int): The desired seed.
r""""""Sets the seed for generating random numbers to a random number.
r""""""Releases all unoccupied cached memory currently held by the caching     allocator so that those can be used in other GPU applications.
r""""""Set memory fraction for limiting process's memory allocation on MPS device.     The allowed value equals the fraction multiplied by recommended maximum device memory     (obtained from Metal API device.recommendedMaxWorkingSetSize).     If trying to allocate more than the allowed value in a process, it will raise an out of     memory error in allocator.      Args:         fraction(float): Range: 0~2. Allowed memory equals total_memory * fraction.      .. note::        Passing 0 to fraction means unlimited allocations        (may cause system failure if out of memory).        Passing fraction greater than 1.0 allows limits beyond the value        returned from device.recommendedMaxWorkingSetSize.
r""""""Returns the current GPU memory occupied by tensors in bytes.      .. note::        The returned size does not include cached allocations in        memory pools of MPSAllocator.
r""""""Returns total GPU memory allocated by Metal driver for the process in bytes.      .. note::        The returned size includes cached allocations in MPSAllocator pools        as well as allocations from MPS/MPSGraph frameworks.",['SPDX-License-Identifier: GPL-2.0-only'],See-URL,,0.0,0.0
133,133,143,pytorch-main/torch/utils/data/datapipes/utils/common.py,See-URL,289,[],"xdoctest: +SKIP(""Failing on some CI machines"")
Signature cannot be inspected, likely it is a built-in fn or written in C
Functions or Methods
Callable Objects
Local function
Lambda function
empty mask matches any input name
print out an error message and raise the error out
Deprecated function names and its corresponding DataPipe type and kwargs for the `_deprecation_warning` function
Traverse only simple structures
 Extract function from partial object Nested partial function is automatically expanded as a single partial object
 Note that this is in-place modifying the internal list from `os.walk` This only works because `os.walk` doesn't shallow copy before turn https://github.com/python/cpython/blob/f4c03484da59049eb62a9bf7777b963e2267d187/Lib/os.py#L407
Check that function used in a callable datapipe works with the input column.      This simply ensures that the number of positional arguments matches the size     of the input column. The function must not contain any non-default     keyword-only arguments.      Examples:         >>> # xdoctest: +SKIP(""Failing on some CI machines"")         >>> def f(a, b, *, c=1):         >>>     return a + b + c         >>> def f_def(a, b=1, *, c=1):         >>>     return a + b + c         >>> assert validate_input_col(f, [1, 2])         >>> assert validate_input_col(f_def, 1)         >>> assert validate_input_col(f_def, [1, 2])      Notes:         If the function contains variable positional (`inspect.VAR_POSITIONAL`) arguments,         for example, f(a, *args), the validator will accept any size of input column         greater than or equal to the number of positional arguments.         (in this case, 1).      Args:         fn: The function to check.         input_col: The input column to check.      Raises:         ValueError: If the function is not compatible with the input column.
Check function is pickable or not.      If it is a lambda or local function, a UserWarning will be raised. If it's not a callable function, a TypeError will be raised.
StreamWrapper is introduced to wrap file handler generated by DataPipe operation like `FileOpener`.      StreamWrapper would guarantee the wrapped file handler is closed when it's out of scope.
Traverse structure and attempts to close all found StreamWrappers on best effort basis.
Automatically close stream when all child streams are closed or if there are none.",['SPDX-License-Identifier: GPL-2.0-only'],See-URL,,0.0,0.0
134,134,144,pytorch-main/torch/csrc/utils/tensor_new.cpp,See-URL,1921,[],"This line uses seq so we must NOT override obj before this line
match NumPy semantics, except use default tensor type instead of double.
NOLINTNEXTLINE(bugprone-branch-clone)
TODO: use MaybeOwned
NOTE: no sparse XLA or Lazy
TODO: Make this accept options instead of dispatch key
NOLINTNEXTLINE(performance-no-int-to-ptr)
NB: device_idx here is NOT a DeviceIndex, but index into PythonArgs
TODO: This line doesn't seem to be exercised at all in tests
namespace
NOLINTNEXTLINE(performance-no-int-to-ptr)
Handles tensor.new(...)
Warning: a wrong attribute error may be suppressed here
torch.sparse_coo_tensor(([0, 1],), self.empty(2, 0).cuda(), (4, 0))
if no dtype provided, infer type based on value type.
See Note [Ensuring sparse values and indices match devices]
See Note [Ensuring sparse values and indices match devices]
See Note [Ensuring sparse values and indices match devices]
See Note [Ensuring sparse values and indices match devices]
See Note [Ensuring sparse values and indices match devices]
ensure new_tensor a leaf node
TODO: add requires_grad once we decide on semantics for sharing data.
ensure new_tensor a leaf node
Make sure this capsule will never be used again.
Check whether 'obj' is a 'Tensor'
Check whether 'obj' is a NumPy Array or Scalar.
No need to clone again, later.
Check whether 'obj' is a 'DLPack' capsule
Check whether 'obj' implements the buffer protocol
Given an aliasable tensor, should we copy it?
 NB: It appears there is some consistency invariant between options and device, where if device is non-empty, its type must be consistent with the device type in options. TODO: Refactor this so we just pass everything in via options
 Note that after the first iteration, obj is the only thing that keeps the seq raw pointer alive.
 this is always guaranteed to be a floating-point type, and makes it more convenient to write e.g. torch.tensor(0.) than torch.tensor(0., dtype=torch.Tensor.dtype).
 this won't change (unless we hit undefined, but that will fail later).
 infer the scalar type and device type; it's not expected to infer the layout since these constructors are defined per-layout-type (e.g. tensor vs sparse_coo_tensor).
 This exists to prevent us from tracing the call to empty().  The actual autograd code doesn't really matter, because requires_grad is always false here. What are the semantics of tensor_new()? We manually construct a tensor and place on it on the correct device with empty() and to(). We then have to ""lift"" the newly constructed tensor in some cases, like when we're performing a functorch transform or running functionalization. The exclude guards are all to ensure that extra logic doesn't run when we're constructing the raw tensor.
 functorch uses FuncTorchDynamicLayerBackMode as a mode key to wrap all tensors returned from operators in special TensorWrapper tensor extension
 We disable Fake and DeferredInit handlers for similar reasons as functorch.
 Note [Functionalization <> torch.Tensor constructor] Functionalization ""lifts"" the newly constructed tensor into a wrapper using aten::lift().
 Tracing should probably also use the ""lift"" operator to add the tensor to a trace, but it's technically BC-breaking to do that, since we currently trace .to() calls.
 If the device is Meta, take the shortcut. We don't want to allocate an empty CPU tensor which would break our contract for meta tensors.
 However, it is VERY important that we trace the to() call here (even though the reason this is important is a hack).  Without *some* factory function call that is traced at construction time, we will consider a tensor constant as originating from ""outside"" the trace, and if you try to return it directly we will fail with the error saying no ""no observable data dependence"".  In an ideal world, we wouldn't trace a to() call but I need to think harder about what exactly we should trace in this case.
 torch.jit.trace will continue to trace out `.to()` instead of `.lift()`, since changing it is BC-breaking.
 lift has no autograd implementation, so we need to make sure we don't try to dispatch to it. TODO: arguably it should have an autograd implementation that noops
 ""base"" here refers to the Tensor type on which the function was invoked, e.g.: in x.new(y), 'x' is the base. TODO: Rewrite this using dispatchKeyToTensorOptions
 Note: this signature doesn't have a dtype, even though it has a device; it probably shouldn't have a device (we should infer it).
 Note: this signature doesn't have a dtype, even though it has a device; it probably shouldn't have a device (we should infer it).
 new(sequence) binds to this signature but should be treated differently unless the sequences is a torch.Size
 This constructor is no longer legacy, it will also be usable for subclass initialization
 prevent Tensor matching with IntArrayRef, PyObject*
 BASE_CTOR (aka torch.Tensor) is now relaxed to accept any dtype; previously it was ""float"" biased
 new(sequence) binds to this signature but should be treated differently unless the sequences is a torch.Size
 Handles ONLY torch.Tensor Unlike the legacy dtype/device specialized constructors, this one is relaxed to accept any device/dtype input tensor (even if it doesn't match the default)
 Handles calls like torch.DoubleTensor, torch.cuda.FloatTensor, torch.sparse.FloatTensor, etc.
 Specific to tensor indexing, converts an indexing list to an indexing tensor (type Byte or Long)
 Clear error indicator if attribute does not exists. Otherwise subsequent Python C API calls might return bogus values. See https://github.com/pytorch/pytorch/issues/58520 for more details
 the global state of invariants check flag will be restored via CheckSparseTensorInvariantsContext destructor
 the global state of invariants check flag will be restored via CheckSparseTensorInvariantsContext destructor
 Note [Ensuring sparse values and indices match devices] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ In all places where we construct indices, we read out options from values (rather than use inferred_options).  Why?  This handles the case when values is a CUDA tensor, but indices is a non-Tensor value (and the device argument is not set).  Example:
 Sparse tensors require both indices and values to live on the same device. If values lives on CUDA, we can infer where the indices should live, and should accept even ordinary index sequences (and just make sure we write them into the correct device).  values is the ONLY way we know that the index tensor should go to CUDA, so we have to get the information in somehow.
 This code is kind of jank.  For one, the dtype in options is silently ignored by internal_new_from_data.  Also, in classic janky code style, it used to not work quite right: if values lives on ""cuda:1"", before all we said was ""this needs to be CUDA"" and indices would be allocated on the wrong tensor. Options is more right and gets this correct.
 atensor steals the ownership of the underlying storage. It also passes a destructor function that will be called when the underlying storage goes out of scope. When the destructor is called, the dlMTensor is destructed too. HACK: Ensure that we hold the GIL here just in case the managed tensor originating from a buggy NumPy build.
 It is possible that the call to at::fromDLPack is the very first call to create a Tensor in PyTorch. If so, then _lazy_init has not been called, and the attempt to call createPyObject will fail because cuda ATen types have not been registered in Python yet. so if we have a cuda tensor, then we need to make sure we have called _lazy_init here
 Used when: 1. 'obj' implements the buffer protocol and no type is given. 2. creating a new tensor from a Python sequence.
 PyArray_CheckScalar is true for both scalars and 0-dim arrays, per https://numpy.org/devdocs/reference/c-api/array.html#c.PyArray_CheckScalar But for 0-dim arrays no `PyArray_FromScalar` call is needed
 Uses a newly cloned storage, instead of the shared one. The THPObjectPtr will delete the previous storage in the end of the previous scope.
 Given a defined tensor, we copy it if either we have to (copy=True) or if we need to (copy=None) because of mismatched device or dtype.
 If we are not copying, we have to check whther we have the tensor in the right device, with the right dtype.
 If tensor is a NumPy Array view, we warn the user about non-writeable arrays if this is the case.
 Setting 'requires_grad' when the tensor is not a leaf does not work. Whenever that happens, we have to use 'detach'.
 Undefined tensor means it does not implement neither DLPack nor the buffer protocol. Last case is a sequence, in which case we must copy (copy can't be false).
 Make tensor from sequence, inferring its type, and then convert it to the desired type. Type inference is activated only if the dtype has not been specified. Otherwise, we force the unwrapped dtype.
 namespace utils namespace torch
non_blocking=
copy=
non_blocking=
copy=
warn_if_not_writeable=
non_blocking=
copy=
non_blocking=*/false, /*copy=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
copy_variables=
copy_numpy=
type_inference=
validate_names=
copy_variables=
copy_numpy=
type_inference=
warn_if_not_writeable=
non_blocking=
copy=
copy_variables =
copy_numpy =
type_inference =","['type_inference=', 'type_inference=', 'type_inference=', 'type_inference=', 'type_inference=', 'type_inference=', 'type_inference =', 'type_inference=', 'type_inference=']",See-URL,,0.0,-1.7763568394002505e-14
135,135,145,pytorch-main/torch/testing/_internal/composite_compliance.py,See-URL,380,[],"CCT: CompositeCompliantTensor class which is generated using generate_cct
Introspection please save us
CCT: CompositeCompliantTensor class which is generated using generate_cct_and_mode
CCT: CompositeCompliantTensor class which is generated using generate_cct_and_mode
CCT: CompositeCompliantTensor class which is generated using generate_cct_and_mode
NOTE: [What errors are Composite Compliance trying to catch?]
see NOTE: [What errors are Composite Compliance trying to catch?]
see NOTE: [What errors are Composite Compliance trying to catch?]
NB: ones, not ones_like, so we get a regular Tensor here
see NOTE: [What errors are Composite Compliance trying to catch?]
Permutations of arg and kwargs in CCT.
Permutations tangent arg and tangent kwargs in CCT.
see NOTE: [What errors are Composite Compliance trying to catch?]
 manually populated from native_functions that have inplace_view: True. In the future we will probably be able to grab that list directly
 This function returns a new class CompositeCompliantTensor The two arguments control the behaviour described below.
 autograd_view_consistency: If True, alias result using `set_` if func returns a view (See Note [Alias Result]). Since Forward AD doesn't work with `set_` we disable it by setting alias to False.
 The storage of CompositeCompliantTensor should never be used directly by a Composite operation; if the Composite operator attempts to read from the storage without dispatching then it'll raise a RuntimeError due to it being a meta storage. type: ignore[attr-defined]
 CompositeCompliantTensor steals the ""requires_grad""-ness. Why a new copy of `elem`? Because sometimes OpInfo shares inputs between tests...
 Propagate conjugate bits to the wrapper tensor Ref: https://github.com/albanD/subclass_zoo/issues/24 Ref: https://github.com/albanD/subclass_zoo/issues/21
 NB: We are making an assumption that if the function is in-place, then the first argument is being written to. Introspection please save us!
 Note [Alias Result] Autograd asserts that for B = A.view_fn(...), B and A's storages are the same. Here we try to make B alias A to avoid those asserts. See https://github.com/pytorch/pytorch/issues/65339 for more information about the issue.
 Idea: this is a weird way of getting a storage that aliases the input. This is a workaround for #65339. 1. under no_dispatch, all of the wrapper tensors look like regular tensors with special storage (the storage is nullptr and advertises CPU/CUDA device. 2. we run func, which ends up running the view operation 3. All view operations reuse the input's storage and return result Tensor(s) with new sizes/strides/offset that alias the input. 4. we set the storage (and sizes/strides/offset) of the wrapper tensor results to be that of the tensors that alias the input
 Some operations are allowed to in-place modify the metadata of the inputs. The only ones are the ""inplace view functions""; when we run into these, we manually modify the metadata of the input.
 For each CompositeCompliantTensor t, we check that t and t.elem have consistent metadata. If they don't have consistent metadata, that means the operator did something fishy.
 Given a list of flat arguments, some of which may be Tensors, return all possible ways some of the arguments could be CompositeCompliantTensors (CCT). For example, given Tensors A, B, C and flat_args = [A, 1, B], We would return the following 4 options: [CCT(A), 1, CCT(B)] [CCT(A), 1, B] [A, 1, CCT(B)] [A, 1, B] NB: Yes, this is exponential. No, we don't care too much because PyTorch ops don't accept that many input Tensors.
 For an operation f(*args, **kwargs), each Tensor argument may either be a regular Tensor or a Tensor Subclass. This iterator iterates through all of those options.
 This test checks ALL possible permutations of calling `op` with arguments that are individually either a regular Tensor or a Tensor subclass.
 The general strategy is to wrap some Tensor args and kwargs in CompositeCompliantTensor wrappers and call the operation.
 If some composite operation does any non-compliant behavior, CompositeCompliantTensor will raise an error.
 There's two things we want to catch: - errors that would raise within the torch_dispatch impl - data_ptr accesses The first is easy to filter for (we could make the error a different error class), the second is always going to be a RuntimeError due to how it is implemented (if you try to access the data_ptr of thex wrapper Tensor, it raises you some internal RuntimeError).
 So the most general thing to catch here was RuntimeError. If you are here and debugging why your test failed, it's plausible that the operator itself is broken and that there are other tests failing.
 Checks via the usage of torch dispatch mode certain anti-patterns that are not composite compliant.
 In particular, the anti-pattern we are trying to prevent is a user creating an empty tensor and then resize_-ing it. Torch Dispatch Mode helps here because all factory functions will create tensors that are CompositeCompliantTensor.
 The general strategy is to wrap all Tensor args and kwargs in CompositeCompliantTensor wrappers. If an operator that is Composite does any non-compliant behavior, CompositeCompliantTensor will raise an error.
 Checks if the backward formula is composite compliant by testing all possible permutations of {inputs, grad_outputs} being CompositeCompliantTensor or regular Tensors.
 NB: it is important that op is accepted as a Callable and not an OpInfo, this means we can apply check_backward_formula to things that aren't OpInfos while debugging.
 Checks if the forward AD formula is composite compliant by testing all possible permutations of {primals, tangents} being CompositeCompliantTensor or regular Tensors.
 NB: it is important that op is accepted as a Callable and not an OpInfo, this means we can apply check_forward_ad_formula to things that aren't OpInfos while debugging.
 Generate `tangent` tensor if given object is a Tensor and requires grad is set.
 Returns dual tensor if primal is a tensor/tensor subclass with requires_grad set.",['SPDX-License-Identifier: GPL-2.0-only'],See-URL,,0.0,0.0
136,136,146,pytorch-main/torch/multiprocessing/__init__.py,See-URL,934,[],"noqa: F403
noqa: PLE0605 type: ignore[attr-defined]
 This call adds a Linux specific prctl(2) wrapper function to this module. See https://github.com/pytorch/pytorch/pull/14391 for more information.
torch.multiprocessing is a wrapper around the native :mod:`multiprocessing` module.  It registers custom reducers, that use shared memory to provide shared views on the same data in different processes. Once the tensor/storage is moved to shared_memory (see :func:`~torch.Tensor.share_memory_`), it will be possible to send it to other processes without making any copies.  The API is 100% compatible with the original module - it's enough to change ``import multiprocessing`` to ``import torch.multiprocessing`` to have all the tensors sent through the queues or shared via other mechanisms, moved to shared memory.  Because of the similarity of APIs we do not document most of this package contents, and we recommend referring to very good docs of the original module.
Add helper function to spawn N processes and wait for completion of any of
Set the strategy for sharing CPU tensors.      Args:         new_strategy (str): Name of the selected strategy. Should be one of             the values returned by :func:`get_all_sharing_strategies()`.
Return the current strategy for sharing CPU tensors.
Return a set of sharing strategies supported on a current system.",['SPDX-License-Identifier: GPL-2.0-only'],See-URL,,0.0,0.0
137,137,147,pytorch-main/torch/testing/_internal/opinfo/definitions/_masked.py,See-URL,417,[],"Used for log_softmax, softmax, softmin
broadcast last mask dimension:
broadcast middle mask dimension:
broadcast first mask dimension:
mask.ndim < input.ndim
mask.ndim == 1
Translate unbiased or correction arguments into ddof
masked.{std, var} doesn't support `.var(unbiased)`
Skip samples that lead to nans in var computation
dimension is required
FIXME: sum reduces all dimensions when dim=[]
RuntimeError: undefined value tensor
https://github.com/pytorch/pytorch/issues/80411
FIXME: ""cuda_scatter_gather_base_kernel_func"" not implemented for ... (used for sparse_coo inputs)
Runs very slowly on slow gradcheck - alternatively reduce input sizes
NotSupportedError: Compiled functions can't ... use keyword-only arguments with defaults
Can reuse the same inputs; dim is required in both
Runs very slowly on slow gradcheck - alternatively reduce input sizes
NotSupportedError: Compiled functions can't ... use keyword-only arguments with defaults
NotSupportedError: Compiled functions can't ... use keyword-only arguments with defaults
Can reuse the same inputs; dim is required in both
FIXME: amax reduces all dimensions when dim=[]
RuntimeError: Unknown builtin op: aten::iinfo
FIXME: amax reduces all dimensions when dim=[]
RuntimeError: Unknown builtin op: aten::iinfo
initial is not a keyword for argmax
NotSupportedError: Compiled functions can't ... use keyword-only arguments with defaults
initial is not a keyword for argmin
NotSupportedError: Compiled functions can't ... use keyword-only arguments with defaults
FIXME: sum reduces all dimensions when dim=[]
RuntimeError: undefined value tensor
FIXME: ""_segment_reduce_lengths_cpu/cuda"" not implemented for ... (used for sparse_csr inputs)
NotSupportedError: Compiled functions can't ... use keyword-only arguments with defaults
FIXME: sum reduces all dimensions when dim=[]
See https://github.com/pytorch/pytorch/pull/78358
Issue with conj and torch dispatch, see https://github.com/pytorch/pytorch/issues/82479
FIXME: sum reduces all dimensions when dim=[]
RuntimeError: undefined value tensor
Runs very slowly on slow gradcheck - alternatively reduce input sizes
See https://github.com/pytorch/pytorch/pull/78358
Issue with conj and torch dispatch, see https://github.com/pytorch/pytorch/issues/82479
FIXME: sum reduces all dimensions when dim=[]
RuntimeError: undefined value tensor
Runs very slowly on slow gradcheck - alternatively reduce input sizes
NotSupportedError: Compiled functions can't ... use keyword-only arguments with defaults
FIXME: reduces all dimensions when dim=[]
Identity can't be -torch.inf without overflow
NotSupportedError: Compiled functions can't ... use keyword-only arguments with defaults
all the values are the same except for -inf vs nan
 PyTorch on XLA throws an error when passed with dim argument for 0d tensor. See https://github.com/pytorch/xla/issues/3061 for more details.
 masks that require broadcasting of inputs (mask.ndim > input.ndim) will not be supported, however, we may reconsider this if there will be demand on this kind of degenerate cases.
 FIXME: for now reductions with non-zero reduction identity and unspecified mask are not supported for sparse COO tensors, see torch.masked.prod implementation for details.
 - sparse CSR tensors are always 2-D tensors - masked reduction on CSR tensors are defined only if keepdim is True.
 reductions with non-zero reduction identity and unspecified mask is not supported for sparse CSR tensors, see torch.masked.prod implementation for details.
 Reductions of CSR tensors use different implementations for inner and/or outer dimensions. So, as a minimum of testing CSR implementations the following kwargs must be generated: dict(dim=0, keepdim=True) dict(dim=1, keepdim=True) dict(dim=(0, 1), keepdim=True) Here we generate the dim=1 case from the dim=0 case.
 FIXME: ""cuda_scatter_gather_base_kernel_func"" not implemented for ... (used for sparse_coo inputs) FIXME: ""_segment_reduce_lengths_cpu/cuda"" not implemented for ... (used for sparse_csr inputs)
 FIXME: ""cuda_scatter_gather_base_kernel_func"" not implemented for ... (used for sparse_coo inputs) FIXME: ""_segment_reduce_lengths_cpu/cuda"" not implemented for ... (used for sparse_csr inputs)
 torch.jit.frontend.NotSupportedError: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults
Sample inputs for masked reduction operators.      Masked reduction operator is a reduction operator with trailing     mask optional argument. A mask is a bool tensor with the same     shape as input or a shape that is broadcastable to input shape.
Sample inputs for masked reduction operators that support inputs     with sparse coo layouts.
Sample inputs for masked reduction operators that support inputs     with sparse csr layouts.
Sample inputs for masked norm.
Sample inputs for masked std/var.
Sample inputs for masked softmax, log_softmax, and softmin.      Masked normalization operator is a reduction operator with     trailing mask optional argument. A mask is a bool tensor with the     same shape as input or a shape that is broadcastable to input     shape.
Sample inputs for masked cumsum and cumprod.
Sample inputs for masked logaddexp.
Sample inputs for masked normalize.",['SPDX-License-Identifier: GPL-2.0-only'],See-URL,,0.0,0.0
138,138,148,pytorch-main/torch/masked/_ops.py,See-URL,944,[],"A workaround to support both TorchScript and MyPy:
The JIT doesn't understand Union, nor torch.dtype here
Expose function as public symbol
Default example data:
add function name to operation names dictionaries
one-line representation of a tensor:
multi-line representation of a tensor with indent
add function name to operation names dictionaries
Apply function name info to docstring templates:
Apply docstring templates to function doctring:
lstrip module name when present
Flatted N-D indices to 1-D indices
TODO: eliminate this restriction
the set of mask flat indices that define masked-in elements:
the set of input flat indices of specified and masked-in elements:
the indices and values of masked-in elements
apply mask to the dense part of the input values:
the set of flat indices of unspecified input and masked-in elements:
the indices of masked-in zero elements
construct result
appending zero elements leads to uncoalesced sparse tensor
promote dtype if specified
Reduce dense dimensions
FIXME: Implement reductions for dense dimensions for ops with non-zero reduction identities
Reduce sparse dimensions
remove reduced sparse dimensions if keepdim = False
Use scatter_reduce to reduce items in the new_values tensor that correspond to the same indices in new_indices
lexsort indices and get index tensor for scatter reduction
FIXME: temporary workaround for issue with bfloat16/float16 remove when acctype is implemented for scatter_reduce
promote dtype if specified
type: ignore[attr-defined]
amax and amin do not support dtype kwarg
TODO: implement sparse CSR specific where operator for efficiency
default mask
mask shape must match with input shape
mask layout must match with input layout
sparse mask must be coalesced
mask is a boolean tensor
lstrip ord argument
type: ignore[union-attr]
__doc__ is generated by _apply_docstring_templates decorator
promote integer types to int64 when output dtype is not specified
__doc__ is generated by _apply_docstring_templates decorator
promote integer types to int64 when output dtype is not specified
Workaround https://github.com/pytorch/pytorch/issues/56586
See comment in the sparse_csr branch, the same issue arises for sparse_coo tensors
masked_prod(csr, ..., mask=None) == torch._sparse_csr_prod(csr, ...) * all(csr.nonzero(), ...)
TODO: compute count analytically
Cannot use _apply_docstring_templates as it is only set up for reductions and normalizations
TODO: compute count analytically
TODO: eliminate mask_input as unnecessary when using masked divide.
TODO: replace torch.maximum with masked maximum when available.
TODO: replace torch.divide with masked divide when available.
 All masked reduction/normalization operations have the same signatures. Here we introduce docstring templates that are applied to docstrings of reduction/normalization functions via _apply_docstring_templates decorator.
 argument name sufficies separated by double underscore will be removed in the final documentation string.
 Strictly speaking, the identity value of the mean operation is the mean of the input. Since the mean value depends on the dim argument and it may be a non-scalar tensor, we consider the identity value of the mean operation ambiguous. Moreover, the mean value of empty input is undefined.
 We use NaN for now because the implementation is currently using torch.nanmedian and NaN is the identity for that function since it gets ignored
 Currently, `dim=()` in reductions operations means ""reduce over all dimensions"" while in future, it will read ""no reduce"". See https://github.com/pytorch/pytorch/issues/29137 When gh-29137 is resolved, this if-block must be deleted.
 Support torch.any with tuple dim argument. Workaround of https://github.com/pytorch/pytorch/issues/56586
 For set operations on sparse tensor indices, we'll convert multi-dimensional indices to 1-D indices for efficiency.
 the input is coalesced, hence input_flat_indices are ordered and the result is guaranteed to be coalesced:
 IndexError: amax(): Expected reduction dim 0 to have non-zero size. sum()/prod() return the reduction identity when dim has size 0 but amax()/amin() do not See https://github.com/pytorch/pytorch/issues/61901
 zero out reduced sparse dimensions if keepdim = True ensures that the call to torch.unique folds duplicated indices together while preserving the dimension
 Currently, while sparse CSR is always 2D with no dense dimensions keepdim must be True FIXME: when dense dimensions are implemented for CSR tensors
 all intervals new_crow_indices[i] - new_crow_indices[i-1] are 1 except for where crow_indices[i] == crow_indices[i-1] where the interval remains as 0
 Broadcasting of CSR tensors is not implemented. Working around by using COO layout.
 csr.to(dtype=torch.int64) is not implemented, so using coo.to on input to ensure the promoted dtype
 csr.to(dtype=torch.int64) is not implemented, so using coo.to on input to ensure the promoted dtype
 mask is None corresponds to all-True mask. The unspecified elements in the CSR tensor correspond to zero values. Hence, the prod reduction result is automatically zero unless all elements are specified. A semi-optimal way to take this into account is to use:
 but that requires implementing `all` and `nonzero` support for sparse csr tensors.
 See comment in the sparse_csr branch of prod, a similar issue arises here where unspecified elements along a dimension may need to be reduced with the result
 See comment in the sparse_csr branch of prod, a similar issue arises here where unspecified elements along a dimension may need to be reduced with the result
 TODO: replace torch.subtract/divide/square/maximum with masked subtract/divide/square/maximum when these will be available.
Decorator that applies docstring templates to function docstring     and returns the function instance.
A utility function called from tools/update_masked_docs.py     script to update the module torch.masked._docs.py
reduction_signature=""""""\
reduction_descr=""""""\ Returns {operation name} of all the elements in the :attr:`input` tensor along the given dimension(s) :attr:`dim` while the :attr:`input` elements are masked out according to the boolean tensor
reduction_args=""""""\ If :attr:`keepdim` is ``True``, the output tensor is of the same size as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1. Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the output tensor having 1 (or ``len(dim)``) fewer dimension(s).  The boolean tensor :attr:`mask` defines the ""validity"" of :attr:`input` tensor elements: if :attr:`mask` element is True then the corresponding element in :attr:`input` tensor will be included in {operation name} computation, otherwise the element is ignored.  When all elements of :attr:`input` along the given dimension :attr:`dim` are ignored (fully masked-out), the corresponding element of the output tensor will have undefined value: it may or may not correspond to the identity value of {operation name} operation; the choice may correspond to the value that leads to the most efficient storage of :attr:`output` tensor.  The mask of the output tensor can be computed as ``torch.any(torch.broadcast_to(mask, input.shape), dim, keepdim=keepdim, dtype=torch.bool)``.  The shapes of the :attr:`mask` tensor and the :attr:`input` tensor don't need to match, but they must be :ref:`broadcastable <broadcasting-semantics>` and the dimensionality of the :attr:`mask` tensor must not be greater than of the :attr:`input` tensor.  Args:     input (Tensor): the input tensor     {args_declarations}  Keyword args:
reduction_example=""""""\ Example::      >>> input = {example_input}     >>> input     {indent_example_input}     >>> mask = {example_mask}     >>> mask     {indent_example_mask}     >>> {full_function_name}(input, {example_args}, mask=mask)     {indent_example_output}
reduction_identity=""""""\
reduction_identity_dtype=""""""\ The identity value of {operation name} operation, which is used to start the reduction, depends on input dtype. For instance, for float32, uint8,
normalization_signature=""""""\
normalization_descr=""""""\ Returns {operation name} of all the slices in the :attr:`input` tensor along :attr:`dim` while the :attr:`input` elements are masked out according to the boolean tensor :attr:`mask`.
normalization_args=""""""\ The boolean tensor :attr:`mask` defines the ""validity"" of :attr:`input` tensor elements: if :attr:`mask` element is True then the corresponding element in :attr:`input` tensor will be included in {operation name} computation, otherwise the element is ignored.  The values of masked-out elements of the output tensor have undefined value: it may or may not be set to zero or nan; the choice may correspond to the value that leads to the most efficient storage of :attr:`output` tensor.  The mask of the {operation name} output tensor can be computed as ``torch.broadcast_to(mask, input.shape)``.  The shapes of the :attr:`mask` tensor and the :attr:`input` tensor don't need to match, but they must be :ref:`broadcastable <broadcasting-semantics>` and the dimensionality of the :attr:`mask` tensor must not be greater than of the :attr:`input` tensor.  Args:     input (Tensor): the input tensor     {args_declarations}  Keyword args:
normalization_example=""""""\ Example::      >>> input = {example_input}     >>> input     {indent_example_input}     >>> mask = {example_mask}     >>> mask     {indent_example_mask}     >>> {full_function_name}(input, {example_args}, mask=mask)     {indent_example_output}
dim=""""""\ dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
dim__as_int=""""""\
ord=""""""\ ord (int, float, optional): the order of vector norm. Default: 2.
ord__required=""""""\ ord (int, float): the order of vector norm. Default: 2.
unbiased=""""""\ unbiased (bool): when True, use Bessel’s correction, otherwise, compute
eps=""""""\
keepdim=""""""\ keepdim (bool, optional): whether the output tensor has
dtype=""""""\ dtype (:class:`torch.dtype`, optional): the desired data type   of returned tensor.  If specified, the input tensor is   casted to :attr:`dtype` before the operation is
mask=""""""\ mask (:class:`torch.Tensor`, optional): the boolean tensor   containing the binary mask of validity of input tensor   elements.
softmax=""""""\ Let ``x`` be a sequence of unmasked elements of one-dimensional slice of the :attr:`input` tensor. Softmax of i-th element in ``x`` is
log_softmax=""""""\ Let ``x`` be a sequence of unmasked elements of one-dimensional slice of the :attr:`input` tensor. LogSoftmax of i-th element in ``x`` is
softmin=""""""\ Let ``x`` be a sequence of unmasked elements of one-dimensional slice of the :attr:`input` tensor. Softmin of i-th element in ``x`` is
normalize=""""""\ Let ``x`` be a sequence of unmasked elements of one-dimensional slice of the :attr:`input` tensor. Normalize of i-th element in ``x`` is
cumsum=""""""\ Let ``x`` be a sequence of unmasked elements of one-dimensional slice of the :attr:`input` tensor. Cumsum of i-th element in ``x`` is
cumprod=""""""\ Let ``x`` be a sequence of unmasked elements of one-dimensional slice of the :attr:`input` tensor. Cumsum of i-th element in ``x`` is
Return identity value as scalar tensor of a reduction operation on     given input, or None, if the identity value cannot be uniquely     defined for the given input.      The identity value of the operation is defined as the initial     value to reduction operation that has a property ``op(op_identity,     value) == value`` for any value in the domain of the operation.     Or put it another way, including or excluding the identity value in     a list of operands will not change the reduction result.      See https://github.com/pytorch/rfcs/pull/27 for more information.
Return dim argument as a tuple of sorted dim values.
Sparse variant of torch.where. Supports sparse COO and hybrid sparse COO tensors.      _sparse_coo_where implements the following invariant:        _sparse_coo_where(mask, input, fill_value).to_dense(fill_value) ==         torch.where(mask.to_dense(), input.to_dense(), torch.full(input.shape, fill_value))      where `a == b` means `assertEqual(a, b)`, mask is boolean sparse     tensor, and `to_dense(fill_value)` is like `to_dense()` except     that the unspecified elements are mapped to `fill_value` rather     than to `0`.      Returns a sparse COO tensor with the following features:      - all specified elements correspond to masked-in elements that       have the values of the input tensor. If there exists a masked-in       element (as specified by mask) that is not specified in the       input, in the result tensor, the corresponding element has value       0. In the dense part of the sparse tensor, the masked-out       elements are replaced with fill_value.      - all unspecified elements correspond to masked-out elements.
Sparse variant of torch.where. Supports sparse CSR tensors.
torch.where with sparse inputs support.      _where implements the following invariant:        _where(mask, input, fill_value).to_dense(fill_value) ==         torch.where(mask.to_dense(), input.to_dense(), torch.full(input.shape, fill_value))      where `a == b` means `assertEqual(a, b)`, mask is boolean sparse     tensor, and `to_dense(fill_value)` is like `to_dense()` except     that the unspecified elements are mapped to `fill_value` rather     than to `0`.      Returns a sparse tensor with the following features:      - all specified elements correspond to masked-in elements that       have the values of the input tensor. If there exists a masked-in       element (as specified by mask) that is not specified in the       input, in the result tensor, the corresponding element has value       0. In the dense part of the sparse tensor, the masked-out       elements are replaced with fill_value.      - all unspecified elements correspond to masked-out elements.
Return canonical input mask.      A canonical input mask is defined as a boolean mask tensor that     shape and layout matches with the shape and the layout of the     input.      The canonical input mask is computed from the :attr:`mask` tensor     content to meet the following criteria:      1. The shape of the canonical input mask is the same as the shape        of :attr:`input` tensor. If the mask tensor has a smaller shape        than the shape of the :attr:`input`, broadcasting rules will be        applied. Downcasting of mask is not supported.      2. The layout of the canonical input mask is the same as the        layout of the :attr:`input` tensor. If the mask has different        layout, it will be converted to the expected layout.  In the        case of sparse COO layout, the canonical input mask will be        coalesced.      3. The dtype of the canonical input mask is torch.bool. If the        mask dtype is not bool then it will be converted to bool dtype        using `.to(dtype=bool)` method call.      4. The elements of the canonical input mask have boolean values        copied from the content of the :attr:`mask` tensor (after        possible broadcasting and dtype conversion transforms).  In        general, the sparsity pattern of the sparse canonical input        mask need not to be the same as the sparsity pattern of the        sparse :attr:`input` tensor.
Return output mask of masked operation applied to given arguments.
Return input with masked-out elements eliminated for the given operations.
\ {reduction_signature}  {reduction_descr}  {reduction_identity_dtype}  {reduction_args}
\ {reduction_signature}  {reduction_descr}  {reduction_identity_dtype}  {reduction_args}
\ {reduction_signature} {reduction_descr} {reduction_identity_dtype} {reduction_args}
\ {reduction_signature} {reduction_descr} {reduction_identity_dtype} {reduction_args}
\ {reduction_signature}  {reduction_descr}  By definition, the identity value of a mean operation is the mean value of the tensor. If all elements of the input tensor along given dimension(s) :attr:`dim` are masked-out, the identity value of the mean is undefined.  Due to this ambiguity, the elements of output tensor with strided layout, that correspond to fully masked-out elements, have ``nan`` values.  {reduction_args}
\ {reduction_signature} {reduction_descr} By definition, the identity value of a median operation is the median value of the tensor. If all elements of the input tensor along given dimension(s) :attr:`dim` are masked-out, the identity value of the median is undefined.  Due to this ambiguity, the elements of output tensor with strided layout, that correspond to fully masked-out elements, have ``nan`` values. {reduction_args}
logaddexp(input, other, *, dtype=None, input_mask=None, other_mask=None) -> Tensor  Returns logaddexp of all the elements in the :attr:`input` and the :attr:`other` tensor. The :attr:`input` elements are masked out according to the boolean tensor :attr:`input_mask` and the attr:`other` elements are masked out according to the boolean tensor :attr:`other_mask`.  The shapes of a mask tensor and the tensor to be masked don't need to match, but they must be :ref:`broadcastable <broadcasting-semantics>` and the dimensionality of the mask tensor must not be greater than of the tensor to be masked.  Args:     input (Tensor): the input tensor     other (Tensor): the second input tensor  Keyword args:     dtype (:class:`torch.dtype`, optional): the desired data type       of returned tensor.  If specified, the output tensor is       casted to :attr:`dtype` after the operation is       performed. Default: None.     input_mask (:class:`torch.Tensor`, optional): the boolean tensor       containing the binary mask of validity of :attr:`input` tensor elements.       Default: None that is equivalent to ``torch.ones(input.shape, dtype=torch.bool)``.     other_mask (:class:`torch.Tensor`, optional): the boolean tensor       containing the binary mask of validity of :attr:`other` tensor elements.       Default: None that is equivalent to ``torch.ones(other.shape, dtype=torch.bool)``.  Example::      >>> input = torch.tensor([-100.0, -200, -300])     >>> input     tensor([-100., -200., -300.])     >>> other = torch.tensor([-1.0, -2, -3])     >>> other     tensor([-1., -2., -3.])     >>> mask = torch.tensor([True, False, True])     >>> mask     tensor([ True, False,  True])     >>> torch.masked._ops.logaddexp(input, other, input_mask=mask, other_mask=mask)     tensor([-1., -inf, -3.])
\ {reduction_signature}  {reduction_descr}  The identity value of norm operation, which is used to start the reduction, is ``{identity_float32}``, except for ``ord=-inf`` it is ``{identity_ord_ninf}``.  {reduction_args}
\ {reduction_signature} {reduction_descr} The identity value of sample variance operation is undefined. The elements of output tensor with strided layout, that correspond to fully masked-out elements, have ``nan`` values. {reduction_args}
\ {reduction_signature} {reduction_descr} The identity value of sample standard deviation operation is undefined. The elements of output tensor with strided layout, that correspond to fully masked-out elements, have ``nan`` values. {reduction_args}",['SPDX-License-Identifier: GPL-2.0-only'],See-URL,,0.0,0.0
139,139,149,pytorch-main/torch/profiler/profiler.py,See-file,542,[],"Default to device 0, if unset. Fallback on cpu.
Construct the memory timeline plot data
Use nanosecond here to avoid naming clash when exporting the trace
prof.export_chrome_trace(""/tmp/test_trace_"" + str(prof.step_num) + "".json"")
send a signal to the profiler that the next iteration has started
deprecated:
add step markers into the trace and table view
key is (prev_action, current_action), value is action list corresponding to the state pair.
used for exit action
"" + str(self.step_num)
"" + str(cur_step))
 FIXME: CUDA Graph does not work well with CUPTI teardown. 1) crashes on 1st lazy CUPTI re-init after teardown (CUDA 11) 2) crashes on 2nd non-lazy CUPTI re-init after teardown (CUDA 12) Workaround: turn off CUPTI teardown when using CUDA Graphs.
 Depending on the file suffix, save the data as json.gz or json. For html, we can embed the image into an HTML file.
 Non-default profiler schedule allows user to turn profiler on and off on different iterations of the training loop; trace_handler is called every time a new trace becomes available
 In this example with wait=1, warmup=1, active=2, repeat=1, profiler will skip the first step/iteration, start warming up on the second, record the third and the forth iterations, after which the trace will become available and on_trace_ready (when set) is called; the cycle repeats starting with the next step
 on_trace_ready=torch.profiler.tensorboard_trace_handler('./log') used when outputting for tensorboard
 Start tracking increments to profiler step, this will be used by Kineto
Returns a set of supported profiler tracing activities.      Note: profiler uses CUPTI library to trace on-device CUDA kernels.     In case when CUDA is enabled but CUPTI is not available, passing     ``ProfilerActivity.CUDA`` to profiler results in using the legacy CUDA     profiling code (same as in the legacy ``torch.autograd.profiler``).     This, in turn, results in including CUDA time in the profiler table output,     but not in the JSON trace.
Low-level profiler wrap the autograd profile      Args:         activities (iterable): list of activity groups (CPU, CUDA) to use in profiling, supported values:             ``torch.profiler.ProfilerActivity.CPU``, ``torch.profiler.ProfilerActivity.CUDA``.             Default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA.         record_shapes (bool): save information about operator's input shapes.         profile_memory (bool): track tensor memory allocation/deallocation (see ``export_memory_timeline``             for more details).         with_stack (bool): record source information (file and line number) for the ops.         with_flops (bool): use formula to estimate the FLOPS of specific operators             (matrix multiplication and 2D convolution).         with_modules (bool): record module hierarchy (including function names)             corresponding to the callstack of the op. e.g. If module A's forward call's             module B's forward which contains an aten::add op,             then aten::add's module hierarchy is A.B             Note that this support exist, at the moment, only for TorchScript models             and not eager mode models.          experimental_config (_ExperimentalConfig) : A set of experimental options             used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.      .. note::         This API is experimental and subject to change in the future.          Enabling shape and stack tracing results in additional overhead.         When record_shapes=True is specified, profiler will temporarily hold references to the tensors;         that may further prevent certain optimizations that depend on the reference count and introduce         extra tensor copies.
Exports the collected trace in Chrome JSON format.
Save stack traces in a file in a format suitable for visualization.          Args:             path (str): save stacks file to this location;             metric (str): metric to use: ""self_cpu_time_total"" or ""self_cuda_time_total""          .. note::             Example of using FlameGraph tool:              - git clone https://github.com/brendangregg/FlameGraph             - cd FlameGraph             - ./flamegraph.pl --title ""CPU time"" --countname ""us."" profiler.stacks > perf_viz.svg
Averages events, grouping them by operator name and (optionally) input shapes and         stack.          .. note::             To use shape/stack functionality make sure to set record_shapes/with_stack             when creating profiler context manager.
Returns the list of unaggregated profiler events,         to be used in the trace callback or after the profiling is finished
Adds a user defined metadata with a string key and a string value         into the trace file
Adds a user defined metadata with a string key and a valid json value         into the trace file
Export memory event information from the profiler collected         tree for a given device, and export a timeline plot. There are 3         exportable files using ``export_memory_timeline``, each controlled by the         ``path``'s suffix.          - For an HTML compatible plot, use the suffix ``.html``, and a memory timeline           plot will be embedded as a PNG file in the HTML file.          - For plot points consisting of ``[times, [sizes by category]]``, where           ``times`` are timestamps and ``sizes`` are memory usage for each category.           The memory timeline plot will be saved a JSON (``.json``) or gzipped JSON           (``.json.gz``) depending on the suffix.          - For raw memory points, use the suffix ``.raw.json.gz``. Each raw memory           event will consist of ``(timestamp, action, numbytes, category)``, where           ``action`` is one of ``[PREEXISTING, CREATE, INCREMENT_VERSION, DESTROY]``,           and ``category`` is one of the enums from           ``torch.profiler._memory_profiler.Category``.          Output: Memory timeline written as gzipped JSON, JSON, or HTML.
Profiler actions that can be taken at the specified intervals
Returns a callable that can be used as profiler ``schedule`` argument. The profiler will skip     the first ``skip_first`` steps, then wait for ``wait`` steps, then do the warmup for the next ``warmup`` steps,     then do the active recording for the next ``active`` steps and then repeat the cycle starting with ``wait`` steps.     The optional number of cycles is specified with the ``repeat`` parameter, the zero value means that     the cycles will continue until the profiling is finished.
Default profiler behavior - immediately starts recording the events,     keeps doing it on every profiler step.
Outputs tracing files to directory of ``dir_name``, then that directory can be     directly delivered to tensorboard as logdir.     ``worker_name`` should be unique for each worker in distributed scenario,     it will be set to '[hostname]_[pid]' by default.
Profiler context manager.      Args:         activities (iterable): list of activity groups (CPU, CUDA) to use in profiling, supported values:             ``torch.profiler.ProfilerActivity.CPU``, ``torch.profiler.ProfilerActivity.CUDA``.             Default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA.         schedule (Callable): callable that takes step (int) as a single parameter and returns             ``ProfilerAction`` value that specifies the profiler action to perform at each step.         on_trace_ready (Callable): callable that is called at each step when ``schedule``             returns ``ProfilerAction.RECORD_AND_SAVE`` during the profiling.         record_shapes (bool): save information about operator's input shapes.         profile_memory (bool): track tensor memory allocation/deallocation.         with_stack (bool): record source information (file and line number) for the ops.         with_flops (bool): use formula to estimate the FLOPs (floating point operations) of specific operators             (matrix multiplication and 2D convolution).         with_modules (bool): record module hierarchy (including function names)             corresponding to the callstack of the op. e.g. If module A's forward call's             module B's forward which contains an aten::add op,             then aten::add's module hierarchy is A.B             Note that this support exist, at the moment, only for TorchScript models             and not eager mode models.         experimental_config (_ExperimentalConfig) : A set of experimental options             used for Kineto library features. Note, backward compatibility is not guaranteed.          use_cuda (bool):             .. deprecated:: 1.8.1                 use ``activities`` instead.      .. note::         Use :func:`~torch.profiler.schedule` to generate the callable schedule.         Non-default schedules are useful when profiling long training jobs         and allow the user to obtain multiple traces at the different iterations         of the training process.         The default schedule simply records all the events continuously for the         duration of the context manager.      .. note::         Use :func:`~torch.profiler.tensorboard_trace_handler` to generate result files for TensorBoard:          ``on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)``          After profiling, result files can be found in the specified directory. Use the command:          ``tensorboard --logdir dir_name``          to see the results in TensorBoard.         For more information, see         `PyTorch Profiler TensorBoard Plugin <https://github.com/pytorch/kineto/tree/master/tb_plugin>`__      .. note::         Enabling shape and stack tracing results in additional overhead.         When record_shapes=True is specified, profiler will temporarily hold references to the tensors;         that may further prevent certain optimizations that depend on the reference count and introduce         extra tensor copies.      Examples:      .. code-block:: python          with torch.profiler.profile(             activities=[                 torch.profiler.ProfilerActivity.CPU,                 torch.profiler.ProfilerActivity.CUDA,             ]         ) as p:             code_to_profile()         print(p.key_averages().table(             sort_by=""self_cuda_time_total"", row_limit=-1))      Using the profiler's ``schedule``, ``on_trace_ready`` and ``step`` functions:      .. code-block:: python          # Non-default profiler schedule allows user to turn profiler on and off         # on different iterations of the training loop;         # trace_handler is called every time a new trace becomes available         def trace_handler(prof):             print(prof.key_averages().table(                 sort_by=""self_cuda_time_total"", row_limit=-1))             # prof.export_chrome_trace(""/tmp/test_trace_"" + str(prof.step_num) + "".json"")          with torch.profiler.profile(             activities=[                 torch.profiler.ProfilerActivity.CPU,                 torch.profiler.ProfilerActivity.CUDA,             ],              # In this example with wait=1, warmup=1, active=2, repeat=1,             # profiler will skip the first step/iteration,             # start warming up on the second, record             # the third and the forth iterations,             # after which the trace will become available             # and on_trace_ready (when set) is called;             # the cycle repeats starting with the next step              schedule=torch.profiler.schedule(                 wait=1,                 warmup=1,                 active=2,                 repeat=1),             on_trace_ready=trace_handler             # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')             # used when outputting for tensorboard             ) as p:                 for iter in range(N):                     code_iteration_to_profile(iter)                     # send a signal to the profiler that the next iteration has started                     p.step()
Signals the profiler that the next profiling step has started.
Execution Trace Observer      Each process can have a single ExecutionTraceObserver instance. The observer     can be added to record function callbacks via calling register_callback()     explicitly. Without calling unregister_callback(), repeated calls to     register_callback() will not add additional observers to record function     callbacks. Once an ExecutionTraceObserver is created, the start() and stop()     methods control when the event data is recorded.      Deleting or calling unregister_callback() will remove the observer from the     record function callbacks, finalize the output file, and will stop     incurring any overheads.
Initializes the default states.
Calls unregister_callback() to make sure to finalize outputs.
Adds ET observer to record function callbacks. The data will be         written to output_file_path.
Removes ET observer from record function callbacks.
Returns True if the execution trace observer is registered, otherwise False.
Returns True if the observer is running, otherwise False.
Starts to capture.
Stops to capture.
Returns the output file name.",['SPDX-License-Identifier: GPL-2.0-only'],See-file,,0.0,0.0
140,140,150,pytorch-main/torch/nn/utils/_named_member_accessor.py,BSD-style,791,[],"type: ignore[assignment]
type: ignore[assignment]
type: ignore[assignment]
type: ignore[assignment]
type: ignore[assignment]
Nested attribute access
noqa: TRY200
type: ignore[return-value]
Batched operations
Swap back if any exception occurs
Swap back if any key is missing when allow_missing is False
Shortcut methods
 This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.
A class that provides a way to access the submodules and parameters/buffers of a module.      It provides caching mechanism to speed up submodule lookups.     This is useful for functional programming to manipulate the module state.
Return the submodule specified by the given path.          For example, to get the submodule mod.layer1.conv1,         use accessor.get_submodule(""layer1.conv1"")          Compare to mod.get_submodule(""layer1.conv1""), this method will cache the         intermediate submodule access to speed up future lookups.
Swap the submodule specified by the given ``path`` to ``value``.          For example, to swap the attribute mod.layer1.conv1 use         ``accessor.swap_submodule(""layer1.conv1"", conv2)``.
Get the tensor specified by the given path to value.          For example, to get the attribute mod.layer1.conv1.weight,         use accessor.get_tensor('layer1.conv1.weight')          Compare to mod.get_parameter(""layer1.conv1.weight""), this method will         cache the intermediate submodule access to speed up future lookups.
Set the attribute specified by the given path to value.          For example, to set the attribute mod.layer1.conv1.weight,         use accessor.set_tensor(""layer1.conv1.weight"", value)
Delete the attribute specified by the given path.          For example, to delete the attribute mod.layer1.conv1.weight,         use accessor.del_tensor(""layer1.conv1.weight"")
Swap the attribute specified by the given path to value.          For example, to swap the attribute mod.layer1.conv1.weight,         use accessor.swap_tensor(""layer1.conv1.weight"", value)
Get the tensors specified by the given paths.          For example, to get the attributes mod.layer1.conv1.weight and         mod.layer1.conv1.bias, use accessor.get_tensors([""layer1.conv1.weight"",         ""layer1.conv1.bias""])
Set the attributes specified by the given paths to values.          For example, to set the attributes mod.layer1.conv1.weight and         mod.layer1.conv1.bias, use accessor.set_tensors([""layer1.conv1.weight"",         ""layer1.conv1.bias""], [weight, bias])
Set the attributes specified by the given paths to values.          For example, to set the attributes mod.layer1.conv1.weight and         mod.layer1.conv1.bias, use accessor.set_tensors_dict({             ""layer1.conv1.weight"": weight,             ""layer1.conv1.bias"": bias,         })
Delete the attributes specified by the given paths.          For example, to delete the attributes mod.layer1.conv1.weight and         mod.layer1.conv1.bias, use accessor.del_tensors([""layer1.conv1.weight"",         ""layer1.conv1.bias""])
Swap the attributes specified by the given paths to values.          For example, to swap the attributes mod.layer1.conv1.weight and         mod.layer1.conv1.bias, use accessor.swap_tensors([""layer1.conv1.weight"",         ""layer1.conv1.bias""], [weight, bias])
Swap the attributes specified by the given paths to values.          For example, to swap the attributes mod.layer1.conv1.weight and         mod.layer1.conv1.bias, use accessor.swap_tensors_dict({             ""layer1.conv1.weight"": weight,             ""layer1.conv1.bias"": bias,         })
Check that the given keys are valid.
Iterate over all the parameters in the module.
Iterate over all the buffers in the module.
Iterate over all the tensors in the module.
Iterate over all the modules in the module.","['This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.', 'SPDX-License-Identifier: GPL-2.0-only']",BSD-style,,0.0,0.0
141,141,151,pytorch-main/torch/_functorch/make_functional.py,BSD-style,1254,[],"Remove all the members in the model
type: ignore[assignment]
TODO: We don't need to copy the model to create a stateless copy
Temporarily load the state back onto self.stateless_model
Remove the loaded state on self.stateless_model
TODO: We don't need to copy the model to create a stateless copy
Temporarily load the state back onto self.stateless_model
Remove the loaded state on self.stateless_model
type: ignore[misc]
NB: Not very efficient, more of a POC
type: ignore[misc]
NB: Not very efficient, more of a POC
 Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.
 Utilities to make nn.Module ""functional"" In particular the goal is to be able to provide a function that takes as input the parameters and evaluate the nn.Module using fixed inputs.
named_params is a dictionary of tensors: {'A': A, 'B': B}     tied_named_params is another dictionary of tensors {'A': A, 'B': B, 'B_tied': B}     with potentially tied (or 'duplicated') tensors      This function creates a mapping from the names in named_params to the     names in tied_named_params: {'A': ['A'], 'B': ['B', 'B_tied']}.
This function removes all the Parameters from the model and     return them as a tuple as well as their original attribute names.     The weights must be re-loaded with `load_weights` before the model     can be used again.     Note that this function modifies the model in place and after this     call, mod.parameters() will be empty.
Reload a set of weights so that `mod` can be used again to perform a forward pass.     Note that the `params` are regular Tensors (that can have history) and so are left     as Tensors. This means that mod.parameters() will still be empty after this call.
load_state(model, weights, weight_names, buffers=(), buffer_names=()) -> model      load_state takes `weights` and `buffers` and assigns them to the model.     This is the inverse operation of `make_functional_deprecated_v1`.
make_functional_deprecated_v1(model) -> weights, func, weight_names      Given an nn.Module, make_functional_deprecated_v1 extracts the state (weights)     and returns a functional version of the model, `func`. This makes     it so that it is possible use transforms over the parameters of     `model`.      `func` can be invoked as follows:     ```     x = torch.randn(4, 3)     model = nn.Linear(3, 3)     weights, func, _ = make_functional_deprecated_v1(model)     func(weights, (x,))     ```      And here is an example of applying the grad transform:     ```     x = torch.randn(4, 3)     model = nn.Linear(3, 3)     weights, _, func = make_functional_deprecated_v1(model)     grad_weights = grad(func)(weights, (x,))     ```      To put the state back into a model, use `load_state`.
make_functional_with_buffers_deprecated_v1(model) -> weights, buffers, func, weight_names, buffer_names      Given an nn.Module, make_functional_with_buffers_deprecated_v1 extracts the state (weights and buffers)     and returns a functional version of the model, `func`.      `func` can be invoked as follows:     ```     x = torch.randn(4, 3)     model = nn.Linear(3, 3)     weights, buffers, func, _, _ = make_functional_with_buffers_deprecated_v1(model)     func(weights, buffers, (x,))     ```      And here is an example of applying the grad transform:     ```     x = torch.randn(4, 3)     model = nn.Linear(3, 3)     weights, buffers, func, _, _ = make_functional_with_buffers_deprecated_v1(model)     func(weights, buffers, (x,))     grad_weights = grad(func)(weights, buffers, (x,))     ```      To put the state back into a model, use `load_state`.
This is the callable object returned by :func:`make_functional_with_buffers`.
This is the callable object returned by :func:`make_functional`.
make_functional(model, disable_autograd_tracking=False) -> func, params      Given a ``torch.nn.Module``, :func:`make_functional` extracts the state     (params) and returns a functional version of the model, ``func``. This     makes it so that it is possible use transforms over the parameters of     ``model``.      ``func`` can be invoked as follows:      .. code-block:: python          import torch         import torch.nn as nn         from functorch import make_functional          x = torch.randn(4, 3)         model = nn.Linear(3, 3)         func, params = make_functional(model)         func(params, x)      And here is an example of applying the grad transform over the parameters     of a model.      .. code-block:: python          import torch         import torch.nn as nn         from functorch import make_functional, grad          x = torch.randn(4, 3)         t = torch.randn(4, 3)         model = nn.Linear(3, 3)         func, params = make_functional(model)          def compute_loss(params, x, t):             y = func(params, x)             return nn.functional.mse_loss(y, t)          grad_weights = grad(compute_loss)(params, x, t)      If the model has any buffers, please use :func:`make_functional_with_buffers` instead.      Args:         model (torch.nn.Module): Input model.         disable_autograd_tracking (bool): Flag to disable gradients tracking for output parameters.             The returned params are unrelated to the set of params from the original model. If False (default),             the params will have ``requires_grad=True`` on them (aka they will be trackable with regular             PyTorch autograd), matching the requires_grad-ness of the params from the original model.             Otherwise, the returned params will have ``requires_grad=False``. Default, False.             If you plan on using regular PyTorch autograd (e.g., if you want to call ``.backward()`` or             ``torch.autograd.grad()``, then set ``disable_autograd_tracking=False``.             Otherwise, if you're only planning on using functorch's gradient transforms,             then please set ``disable_autograd_tracking=True`` to avoid unnecessarily tracking             history with PyTorch autograd.
make_functional_with_buffers(model, disable_autograd_tracking=False) -> func, params, buffers      Given a ``torch.nn.Module``, make_functional_with_buffers extracts the     state (params and buffers) and returns a functional version of the model     ``func`` that can be invoked like a function.      ``func`` can be invoked as follows:      .. code-block:: python          import torch         import torch.nn as nn         from functorch import make_functional_with_buffers          x = torch.randn(4, 3)         model = nn.Linear(3, 3)         func, params, buffers = make_functional_with_buffers(model)         func(params, buffers, x)      And here is an example of applying the grad transform over the parameters     of a model:      .. code-block:: python          import torch         import torch.nn as nn         from functorch import make_functional_with_buffers, grad          x = torch.randn(4, 3)         t = torch.randn(4, 3)         model = nn.Linear(3, 3)         func, params, buffers = make_functional_with_buffers(model)          def compute_loss(params, buffers, x, t):             y = func(params, buffers, x)             return nn.functional.mse_loss(y, t)          grad_weights = grad(compute_loss)(params, buffers, x, t)      Args:         model (torch.nn.Module): Input model.         disable_autograd_tracking (bool): Flag to disable gradients tracking for output parameters.             The returned params are unrelated to the set of params from the original model. If False (default),             the params will have ``requires_grad=True`` on them (aka they will be trackable with regular             PyTorch autograd), matching the requires_grad-ness of the params from the original model.             Otherwise, the returned params will have ``requires_grad=False``. Default, False.             If you plan on using regular PyTorch autograd (e.g., if you want to call ``.backward()`` or             ``torch.autograd.grad()``, then set ``disable_autograd_tracking=False``.             Otherwise, if you're only planning on using functorch's gradient transforms,             then please set ``disable_autograd_tracking=True`` to avoid unnecessarily tracking             history with PyTorch autograd.
combine_state_for_ensemble(models) -> func, params, buffers      Prepares a list of torch.nn.Modules for ensembling with :func:`vmap`.      Given a list of ``M`` ``nn.Modules`` of the same class, stacks all of their     parameters and buffers together to make ``params`` and ``buffers``.     Each parameter and buffer in the result will have an additional dimension     of size ``M``.      :func:`combine_state_for_ensemble` also returns ``func``, a functional     version of one of the models in :attr:`models`. One cannot directly run     ``func(params, buffers, *args, **kwargs)`` directly, you probably want to     use ``vmap(func, ...)(params, buffers, *args, **kwargs)``      Here's an example of how to ensemble over a very simple model:      .. code-block:: python          num_models = 5         batch_size = 64         in_features, out_features = 3, 3         models = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]         data = torch.randn(batch_size, 3)          fmodel, params, buffers = combine_state_for_ensemble(models)         output = vmap(fmodel, (0, 0, None))(params, buffers, data)          assert output.shape == (num_models, batch_size, out_features)      .. warning::         All of the modules being stacked together must be the same (except for         the values of their parameters/buffers). For example, they should be in the         same mode (training vs eval).          This API is subject to change -- we're investigating better ways to         create ensembles and would love your feedback how to improve this.","['SPDX-License-Identifier: GPL-2.0-only', 'Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.']",BSD-style,,0.0,0.0
142,142,152,pytorch-main/torch/distributed/elastic/rendezvous/registry.py,BSD-style,1720,[]," Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.
This method is used to obtain a reference to a :py:class`RendezvousHandler`.     Custom rendezvous handlers can be registered by      ::        from torch.distributed.elastic.rendezvous import rendezvous_handler_registry       from torch.distributed.elastic.rendezvous.registry import get_rendezvous_handler        def create_my_rdzv(params: RendezvousParameters):         return MyCustomRdzv(params)        rendezvous_handler_registry.register(""my_rdzv_backend_name"", create_my_rdzv)        my_rdzv_handler = get_rendezvous_handler(""my_rdzv_backend_name"", RendezvousParameters)","['Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.', 'SPDX-License-Identifier: GPL-2.0-only']",BSD-style,,0.0,1.4210854715202004e-14
143,143,153,pytorch-main/torch/distributed/argparse_util.py,BSD-style,1489,[],"!/usr/bin/env python3
 Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.
 ``required`` means that it NEEDS to be present  in the command-line args rather than ""this option requires a value (either set explicitly or default"" so if we found default then we don't ""require"" it to be in the command-line so set it to False
Get argument values from ``PET_{dest}`` before defaultingto the given ``default`` value.      For flags (e.g. ``--standalone``)     use ``check_env`` instead.      .. note:: when multiple option strings are specified, ``dest`` is               the longest option string (e.g. for ``""-f"", ""--foo""``               the env var to set is ``PET_FOO`` not ``PET_F``)      Example:     ::       parser.add_argument(""-f"", ""--foo"", action=env, default=""bar"")       ./program                                      -> args.foo=""bar""      ./program -f baz                               -> args.foo=""baz""      ./program --foo baz                            -> args.foo=""baz""      PET_FOO=""env_bar"" ./program -f baz    -> args.foo=""baz""      PET_FOO=""env_bar"" ./program --foo baz -> args.foo=""baz""      PET_FOO=""env_bar"" ./program           -> args.foo=""env_bar""       parser.add_argument(""-f"", ""--foo"", action=env, required=True)       ./program                                      -> fails      ./program -f baz                               -> args.foo=""baz""      PET_FOO=""env_bar"" ./program           -> args.foo=""env_bar""      PET_FOO=""env_bar"" ./program -f baz    -> args.foo=""baz
Check whether the env var ``PET_{dest}`` exists before defaulting to the given ``default`` value.      Equivalent to     ``store_true`` argparse built-in action except that the argument can     be omitted from the commandline if the env var is present and has a     non-zero value.      .. note:: it is redundant to pass ``default=True`` for arguments               that use this action because a flag should be ``True``               when present and ``False`` otherwise.      Example:     ::       parser.add_argument(""--verbose"", action=check_env)       ./program                                  -> args.verbose=False      ./program --verbose                        -> args.verbose=True      PET_VERBOSE=1 ./program           -> args.verbose=True      PET_VERBOSE=0 ./program           -> args.verbose=False      PET_VERBOSE=0 ./program --verbose -> args.verbose=True      Anti-pattern (don't do this):      ::       parser.add_argument(""--verbose"", action=check_env, default=True)       ./program                                  -> args.verbose=True      ./program --verbose                        -> args.verbose=True      PET_VERBOSE=1 ./program           -> args.verbose=True      PET_VERBOSE=0 ./program           -> args.verbose=False","['SPDX-License-Identifier: GPL-2.0-only', ' This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.']",BSD-style,,0.0,0.0
144,144,154,pytorch-main/torch/distributed/elastic/multiprocessing/redirects.py,BSD-style,1731,[],"!/usr/bin/env python3
syntactic-sugar for redirect(""stdout"", ""tmp/stdout.log"")
 Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.
 Taken and modified from original source: https://eli.thegreenplace.net/2015/redirecting-all-kinds-of-stdout-in-python/
Redirects ``std`` (one of ``""stdout""`` or ``""stderr""``) to a file     in the path specified by ``to_file``. This method redirects the     underlying std file descriptor (not just python's ``sys.stdout|stderr``).     See usage for details.      Directory of ``dst_filename`` is assumed to exist and the destination file     is overwritten if it already exists.      .. note:: Due to buffering cross source writes are not guaranteed to               appear in wall-clock order. For instance in the example below               it is possible for the C-outputs to appear before the python               outputs in the log file.      Usage:      ::       # syntactic-sugar for redirect(""stdout"", ""tmp/stdout.log"")      with redirect_stdout(""/tmp/stdout.log""):         print(""python stdouts are redirected"")         libc = ctypes.CDLL(""libc.so.6"")         libc.printf(b""c stdouts are also redirected""         os.system(""echo system stdouts are also redirected"")       print(""stdout restored"")","['Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'SPDX-License-Identifier: GPL-2.0-only', 'This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.']",BSD-style,,0.0,1.4210854715202004e-14
145,145,155,pytorch-main/torch/distributed/elastic/events/__init__.py,BSD-style,1742,[],"!/usr/bin/env/python3
noqa: F401
Do not propagate message to the root logger
Add the logger to the global dictionary
We don't want to perform an extra computation if not needed.
Set up parameters.
Determines which file called this function.
Set up error trace if this is an exception
Initialize event object
Finally, record the event.
 Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.
 Delete the callstack variable. If kept, this can mess with python's garbage collector as we are holding on to stack frame information in the inspect module.
Module contains events processing mechanisms that are integrated with the standard python logging.  Example of usage:  ::    from torch.distributed.elastic import events   event = events.Event(name=""test_event"", source=events.EventSource.WORKER, metadata={...})   events.get_logging_handler(destination=""console"").info(event)
Constructs python logger based on the destination type or extends if provided.     Available destination could be found in ``handlers.py`` file.     The constructed logger does not propagate messages to the upper level loggers,     e.g. root logger. This makes sure that a single event can be processed once.      Args:         destination: The string representation of the event handler.             Available handlers found in ``handlers`` module","['This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.', ' Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'SPDX-License-Identifier: GPL-2.0-only']",BSD-style,,0.0,1.4210854715202004e-14
146,146,156,pytorch-main/torch/distributed/elastic/rendezvous/etcd_server.py,BSD-style,1723,[],"!/usr/bin/env python3
type: ignore[import]
use client
etcd server process finished
 Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.
Finds a free port and binds a temporary socket to it so that     the port can be ""reserved"" until used.      .. note:: the returned socket must be closed before using the port,               otherwise a ``address already in use`` error will happen.               The socket should be held and closed as close to the               consumer of the port as possible since otherwise, there               is a greater chance of race-condition where a different               process may see the port as being free and take it.      Returns: a socket binded to the reserved free port      Usage::      sock = find_free_port()     port = sock.getsockname()[1]     sock.close()     use_port(port)
.. note:: tested on etcd server v3.4.3      Starts and stops a local standalone etcd server on a random free     port. Useful for single node, multi-worker launches or testing,     where a sidecar etcd server is more convenient than having to     separately setup an etcd server.      This class registers a termination handler to shutdown the etcd     subprocess on exit. This termination handler is NOT a substitute for     calling the ``stop()`` method.      The following fallback mechanism is used to find the etcd binary:      1. Uses env var TORCHELASTIC_ETCD_BINARY_PATH     2. Uses ``<this file root>/bin/etcd`` if one exists     3. Uses ``etcd`` from ``PATH``      Usage     ::       server = EtcdServer(""/usr/bin/etcd"", 2379, ""/tmp/default.etcd"")      server.start()      client = server.get_client()      # use client      server.stop()      Args:         etcd_binary_path: path of etcd server binary (see above for fallback path)
Returns:             the port the server is running on.
Returns:             the host the server is running on.
Returns:             the etcd server endpoint (host:port)
Starts the server, and waits for it to be ready. When this function         returns the sever is ready to take requests.          Args:             timeout: time (in seconds) to wait for the server to be ready                 before giving up.             num_retries: number of retries to start the server. Each retry                 will wait for max ``timeout`` before considering it as failed.             stderr: the standard error file handle. Valid values are                 `subprocess.PIPE`, `subprocess.DEVNULL`, an existing file                 descriptor (a positive integer), an existing file object, and                 `None`.          Raises:             TimeoutError: if the server is not ready within the specified timeout
Returns:            An etcd client object that can be used to make requests to            this server.
Stops the server and cleans up auto generated resources (e.g. data dir)","['This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.', 'Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'SPDX-License-Identifier: GPL-2.0-only']",BSD-style,,0.0,1.4210854715202004e-14
147,147,157,pytorch-main/torch/distributed/elastic/utils/data/__init__.py,BSD-style,1708,[],"!/usr/bin/env python3
 Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.
 noqa: F401 noqa: F401","[""`\n[' This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree."", "" SPDX-License-Identifier: GPL-2.0-only']\n`""]",BSD-style,,0.0,0.0
148,148,158,pytorch-main/torch/distributed/elastic/multiprocessing/api.py,BSD-style,1733,[],"!/usr/bin/env python3
type: ignore[attr-defined] # noqa: F821
type: ignore[attr-defined] # noqa: F821
type: ignore[return]
return None -> should NEVER reach here since we regex check input
vm is a number (e.g. 0)
vm is a mapping (e.g. 0:1,1:2)
get the per-rank params up front so we fail fast if no mapping is found
each ret_val queue will always contain a single element.
see comments in ``join()`` for what this is
assertion for mypy type checker
save the return values temporarily into a member var
we should ALWAYS have ALL the return values when all the processes are done
assertion for mypy type checking
inherit parent environment vars
state vector; _vdone[local_rank] -> is local_rank finished or not
type: ignore[arg-type] # entrypoint is always a str
failed or signaled
else: --> succeeded; nothing to do
if ALL procs are finished or ANY have failed
terminate all running procs
Populate return with dummy values. This provides consistency with MultiprocessingHandler
there are no failures and procs still running
 Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.
 returns: {0: Std.OUT, 1: Std.OUT} returns: {0: Std.NONE, 1: Std.OUT} returns: {0: Std.OUT, 1: Std.OUT}
 validate that all mappings have the same number of keys and all local ranks are accounted for
 redirect file for stdout (to console if None) redirect file for stderr (to console if None)
 Note: set method should ONLY be invoked for the use case when all processes finished successfully. If any process died on event.wait() calling set() method will deadlock.
 torch.mp.ProcessContext Throws an Exception if some/all of worker processes failed timeout < 0 checks worker status and return immediately Join will never return success since we use synchronize.Event to wait for all processes to finish.
 IMPORTANT: we use multiprocessing.Queue to carry worker return values back to the parent, the worker process will wait before terminating until all the buffered items are fed by the feeder thread to the underlying pipe. Hence to prevent deadlocks on large return values, we opportunistically try queue.get on each join call See: https://docs.python.org/2/library/multiprocessing.html#all-platforms
 Wait untill all processes are finished. At this point workers finished executing user function
 entrypoint for MultiprocessContext will always be a Callable type: ignore[union-attr]
 If the process exited because of some reason, `ProcessLookupError` will be raised, it is safe to ignore it.
 If the process exited because of some reason, `ProcessLookupError` will be raised, it is safe to ignore it.
 pyre-fixme[6]: Expected `Union[typing.Sequence[Union[_PathLike[bytes], _PathLike[str], bytes, str]], bytes, str]` for 1st param but got `Tuple[str, *Tuple[Any, ...]]`.
 Ignore the timeout expired exception, since the child process will be forcefully terminated via SIGKILL
Exception is raised inside the torchelastic agent process by the termination handler     if the death signal got received by the process.
Termination handler that raises exceptions on the main process.      When the process receives death signal(SIGTERM, SIGINT), this termination handler will     be invoked. It raises the ``SignalException`` exception that should be processed by the     user code. Python does not terminate process after the termination handler is finished,     so the exception should not be silently ignored, otherwise the process will never     be terminated.
Get the kill signal. SIGKILL for unix, CTRL_C_EVENT for windows.
Get the default termination signal. SIGTERM for unix, CTRL_C_EVENT for windows.
Example:          ::           from_str(""0"") -> Std.NONE          from_str(""1"") -> Std.OUT          from_str(""0:3,1:0,2:1,3:2"") -> {0: Std.ALL, 1: Std.NONE, 2: Std.OUT, 3: Std.ERR}          Any other input raises an exception
Certain APIs take redirect settings either as a single value (e.g. apply to all     local ranks) or as an explicit user-provided mapping. This method is a convenience     method that converts a value or mapping into a mapping.      Example:      ::       to_map(Std.OUT, local_world_size=2) # returns: {0: Std.OUT, 1: Std.OUT}      to_map({1: Std.OUT}, local_world_size=2) # returns: {0: Std.NONE, 1: Std.OUT}      to_map({0: Std.OUT, 1: Std.OUT}, local_world_size=2) # returns: {0: Std.OUT, 1: Std.OUT}
Results of a completed run of processes started with ``start_processes()``.     Returned by ``PContext``.      Note the following:      1. All fields are mapped by local rank     2. ``return_values`` - only populated for functions (not the binaries).     3. ``stdouts`` - path to stdout.log (empty string if no redirect)     4. ``stderrs`` - path to stderr.log (empty string if no redirect)
The base class that standardizes operations over a set of processes     that are launched via different mechanisms. The name ``PContext``     is intentional to disambiguate with ``torch.multiprocessing.ProcessContext``.      .. warning:: stdouts and stderrs should ALWAYS be a superset of                  tee_stdouts and tee_stderrs (respectively) this is b/c                  tee is implemented as a redirect + tail -f <stdout/stderr.log>
Start processes using parameters defined in the constructor.
Start processes using strategy defined in a particular context.
Polls the run status of the processes running under this context.         This method follows an ""all-or-nothing"" policy and returns         a ``RunProcessResults`` object if either all processes complete         successfully or any process fails. Returns ``None`` if         all processes are still running.
Waits for the specified ``timeout`` seconds, polling every ``period`` seconds         for the processes to be done. Returns ``None`` if the processes are still running         on timeout expiry. Negative timeout values are interpreted as ""wait-forever"".         A timeout value of zero simply queries the status of the processes (e.g. equivalent         to a poll).          ..note: Multiprocessing library registers SIGTERM and SIGINT signal handlers that raise                 ``SignalException`` when the signals received. It is up to the consumer of the code                 to properly handle the exception. It is important not to swallow the exception otherwise                 the process would not terminate. Example of the typical workflow can be:          .. code-block:: python             pc = start_processes(...)             try:                 pc.wait(1)                 .. do some other work             except SignalException as e:                 pc.shutdown(e.sigval, timeout=30)          If SIGTERM or SIGINT occurs, the code above will try to shutdown child processes by propagating         received signal. If child processes will not terminate in the timeout time, the process will send         the SIGKILL.
Returns pids of processes mapped by their respective local_ranks
r""""""         Terminates all processes managed by this context and cleans up any         meta resources (e.g. redirect, error_file files).
r""""""         Terminates all processes managed by this context and cleans up any         meta resources (e.g. redirect, error_file files).          Args:             death_sig: Death signal to terminate processes.             timeout: Time to wait for processes to finish, if process is                 still alive after this time, it will be terminated via SIGKILL.
``PContext`` holding worker processes invoked as a function.
Convenience wrapper around python's ``subprocess.Popen``. Keeps track of     meta-objects associated to the process (e.g. stdout and stderr redirect fds).
``PContext`` holding worker processes invoked as a binary.","['This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.', 'Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.', 'SPDX-License-Identifier: GPL-2.0-only']",BSD-style,,0.0,1.4210854715202004e-14
149,149,159,pytorch-main/torch/distributed/c10d_logger.py,BSD-style,1488,[],"!/usr/bin/env python3
type: ignore[arg-type]
 Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.
 This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree.","[""`\n[' This source code is licensed under the BSD-style license found in the LICENSE file in the root directory of this source tree."", ' SPDX-License-Identifier: GPL-2.0-only', "" Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.']\n`""]",BSD-style,,0.0,0.0
150,150,160,pytorch-main/torch/csrc/utils/pythoncapi_compat.h,0BSD BSD,1951,[],"Header file providing new C API functions to old Python versions.
SPDX-License-Identifier: 0BSD
PyFrameObject, PyFrame_GetBack()
Cast argument to PyObject* type.
bpo-42262 added Py_NewRef() to Python 3.10.0a3
bpo-42262 added Py_XNewRef() to Python 3.10.0a3
bpo-39573 added Py_SET_REFCNT() to Python 3.9.0a4
bpo-39573 added Py_SET_TYPE() to Python 3.9.0a4
bpo-39573 added Py_SET_SIZE() to Python 3.9.0a4
bpo-40421 added PyFrame_GetCode() to Python 3.9.0b1
bpo-40421 added PyFrame_GetBack() to Python 3.9.0b1
bpo-40421 added PyFrame_GetLocals() to Python 3.11.0a7
bpo-40421 added PyFrame_GetGlobals() to Python 3.11.0a7
bpo-40421 added PyFrame_GetBuiltins() to Python 3.11.0a7
bpo-40421 added PyFrame_GetLasti() to Python 3.11.0b1
gh-91248 added PyFrame_GetVar() to Python 3.12.0a2
gh-91248 added PyFrame_GetVarString() to Python 3.12.0a2
bpo-39947 added PyThreadState_GetInterpreter() to Python 3.9.0a5
bpo-40429 added PyThreadState_GetFrame() to Python 3.9.0b1
bpo-39947 added PyInterpreterState_Get() to Python 3.9.0a5
bpo-39947 added PyInterpreterState_Get() to Python 3.9.0a6
bpo-43760 added PyThreadState_EnterTracing() to Python 3.11.0a2
bpo-43760 added PyThreadState_LeaveTracing() to Python 3.11.0a2
bpo-1635741 added PyModule_AddObjectRef() to Python 3.10.0a3
bpo-40024 added PyModule_AddType() to Python 3.9.0a5
inline _PyType_Name()
bpo-39573 added Py_IS_TYPE() to Python 3.9.0a4
gh-92154 added PyCode_GetCode() to Python 3.11.0b1
gh-95008 added PyCode_GetVarnames() to Python 3.11.0rc1
gh-95008 added PyCode_GetFreevars() to Python 3.11.0rc1
gh-95008 added PyCode_GetCellvars() to Python 3.11.0rc1
Py_UNUSED() was added to Python 3.4.0b2.
gh-105922 added PyImport_AddModuleRef() to Python 3.13.0a1
gh-105927 added PyWeakref_GetRef() to Python 3.13.0a1
SystemError if ref is NULL
bpo-36974 added PY_VECTORCALL_ARGUMENTS_OFFSET to Python 3.8b1
bpo-36974 added PyVectorcall_NARGS() to Python 3.8b1
gh-105922 added PyObject_Vectorcall() to Python 3.9.0a4
bpo-36974 added _PyObject_Vectorcall() to Python 3.8.0b1
PYTHONCAPI_COMPAT
 File distributed under the Zero Clause BSD (0BSD) license. Copyright Contributors to the pythoncapi_compat project.
 Homepage: https://github.com/python/pythoncapi_compat
 Latest version: https://raw.githubusercontent.com/python/pythoncapi_compat/master/pythoncapi_compat.h
 Compatibility with Visual Studio 2013 and older which don't support the inline keyword in C (only in C++): use __inline instead.
 On C++11 and newer, _Py_NULL is defined as nullptr on C++11, otherwise it is defined as NULL.
 Py_SETREF() and Py_XSETREF() were added to Python 3.5.2. It is excluded from the limited C API.
 bpo-43753 added Py_Is(), Py_IsNone(), Py_IsTrue() and Py_IsFalse() to Python 3.10.0b1.
 bpo-27129: Since Python 3.10.0a7, f_lasti is an instruction offset, not a bytes offset anymore. Python uses 16-bit ""wordcode"" (2 bytes) instructions.
 bpo-37194 added PyObject_CallNoArgs() to Python 3.9.0a1 PyObject_CallNoArgs() added to PyPy 3.9.16-v7.3.11
 bpo-39245 made PyObject_CallOneArg() public (previously called _PyObject_CallOneArg) in Python 3.9.0a4 PyObject_CallOneArg() added to PyPy 3.9.16-v7.3.11
 bpo-40241 added PyObject_GC_IsTracked() to Python 3.9.0a6. bpo-4688 added _PyObject_GC_IS_TRACKED() to Python 2.7.0a2.
 bpo-40241 added PyObject_GC_IsFinalized() to Python 3.9.0a6. bpo-18112 added _PyGCHead_FINALIZED() to Python 3.4.0 final.
 bpo-46906 added PyFloat_Pack2() and PyFloat_Unpack2() to Python 3.11a7. bpo-11734 added _PyFloat_Pack2() and _PyFloat_Unpack2() to Python 3.6.0b1. Python 3.11a2 moved _PyFloat_Pack2() and _PyFloat_Unpack2() to the internal C API: Python 3.11a2-3.11a6 versions are not supported.
 bpo-46906 added PyFloat_Pack4(), PyFloat_Pack8(), PyFloat_Unpack4() and PyFloat_Unpack8() to Python 3.11a7. Python 3.11a2 moved _PyFloat_Pack4(), _PyFloat_Pack8(), _PyFloat_Unpack4() and _PyFloat_Unpack8() to the internal C API: Python 3.11a2-3.11a6 versions are not supported.","['File distributed under the Zero Clause BSD (0BSD) license. Copyright Contributors to the pythoncapi_compat project.', 'SPDX-License-Identifier: 0BSD', 'SPDX-License-Identifier: GPL-2.0-only']",0BSD BSD,,0.0,1.4210854715202004e-14
151,151,161,pytorch-main/torch/csrc/jit/testing/file_check.cpp,NCSA,2173,[],"==-- llvm/Support/FileCheck.h ---------------------------*- C++ -*-==//
The LLVM Compiler Infrastructure
===----------------------------------------------------------------------===//
API modified from llvm::FileCheck
namespace
consecutive CHECK_DAGs & CHECK_NOTs need to be evaluated as a group
NOLINTNEXTLINE(cppcoreguidelines-non-private-member-variables-in-classes)
needs special parsing
add ':' and the space
Checks that source token is highlighted, does not advance search range.
Guaranteed to fail to generate error message.
already checked the group after
 This file is distributed under the University of Illinois Open Source License. See LICENSE.TXT for details.
 inclusive exclusive
 namespace testing namespace jit namespace torch","['This file is distributed under the University of Illinois Open Source License. See LICENSE.TXT for details.', 'SPDX-License-Identifier: GPL-2.0-only']",NCSA,,0.0,0.0
